{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import itertools\n",
    "import math\n",
    "from torch.jit import script, trace\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\n",
      "\n",
      "한마디도 안해? 쳐다보지도 않고?\n",
      "\n",
      "응. 새벽에도 일찍 나갔어. 온다간다 말도없이.\n",
      "\n",
      "왜 그러지? 나이들어서 자기 새끼 가졌다 하면 다들 좋아한다는데.\n",
      "\n",
      "울오빤 다르다니까. 괜히 언니말 들었다가 이게 뭐야?\n",
      "\n",
      "내말 듣고 애가졌니? 이미 가져놓고.\n",
      "\n",
      "나 정말 사고쳤나봐. 그래도 난 오빠가 좋아할줄 알았는데.\n",
      "\n",
      "$\n",
      "\n",
      " 왜 또? 일없으면 전화하지 말랬지? 숙제 다했어? 배고픈데 어쩌라고. 엄마 바쁜거 몰라? 좀만 기다려.  엄마, 나도 참 못된 엄마야. 새끼들 먹여 살릴라고 돈번다면서 배고프다는 새끼한테 소리나 지르고.\n",
      "\n",
      "손님 뜸한 시간이니께 니가 설거지 혀. 내가 가볼테니. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_name = 'chatData'\n",
    "corpus = os.path.join(r'C:\\Users\\admin\\jupyter\\pytorch\\chatbot', corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'r', encoding='utf-8') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "        \n",
    "printLines(os.path.join(corpus, 'test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한마디도 안해? 쳐다보지도 않고?\\t응. 새벽에도 일찍 나갔어. 온다간다 말도없이.', '응. 새벽에도 일찍 나갔어. 온다간다 말도없이.\\t왜 그러지? 나이들어서 자기 새끼 가졌다 하면 다들 좋아한다는데.', '왜 그러지? 나이들어서 자기 새끼 가졌다 하면 다들 좋아한다는데.\\t울오빤 다르다니까. 괜히 언니말 들었다가 이게 뭐야?', '울오빤 다르다니까. 괜히 언니말 들었다가 이게 뭐야?\\t내말 듣고 애가졌니? 이미 가져놓고.', '내말 듣고 애가졌니? 이미 가져놓고.\\t나 정말 사고쳤나봐. 그래도 난 오빠가 좋아할줄 알았는데.']\n"
     ]
    }
   ],
   "source": [
    "def loadLines(fileName):\n",
    "    lines = []\n",
    "    count = 0\n",
    "    count_list = []\n",
    "    result = []\n",
    "    with open(fileName, 'r', encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if line == '$':\n",
    "                count_list.append(count)\n",
    "                count = 0\n",
    "                pass\n",
    "            else:\n",
    "                lines.append(line)\n",
    "                count+= 1\n",
    "        count_list.append(count)\n",
    "        count = 0\n",
    "        for i in count_list: # 6, 4, 2, 7\n",
    "            for j in range(count+1, count+i):\n",
    "                result.append(lines[j-1] + '\\t' + lines[j])\n",
    "            count += i\n",
    "    return result\n",
    "            \n",
    "lines = loadLines(os.path.join(corpus, 'test.txt'))\n",
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한마디', '도', '안해', '?', '쳐다보', '지', '도', '않', '고', '?', '응', '.', '새벽', '에', '도', '일찍', '나가', '었', '어', '.', '올', 'ㄴ다', '갈', 'ㄴ다', '말', '도', '없이', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "#print(lines[0])\n",
    "\n",
    "result = kkma.morphs(lines[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한마디', '도', '안해', '?', '쳐다보지도', '않고', '?', '\\t', '응', '.', '새벽', '에도', '일찍', '나갔어', '.', '온다', '간다', '말', '도', '없이', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "#print(lines[0])\n",
    "\n",
    "result = okt.morphs(lines[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한마디', '도', '안', '해', '?', '쳐다보', '지', '도', '않', '고', '?', '응', '.', '새벽', '에', '도', '일찍', '나갔', '어', '.', '온다간다', '말', '도', '없이', '.']\n"
     ]
    }
   ],
   "source": [
    "from eunjeon import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "#print(lines[0])\n",
    "\n",
    "result = mecab.morphs(lines[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한마디', '도', '안해', '?', '쳐다보지도', '않고', '?', '\\t', '응', '.', '새벽', '에도', '일찍', '나갔어', '.', '온다', '간다', '말', '도', '없이', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abc\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "\n",
    "#print(lines[0])\n",
    "\n",
    "result = twitter.morphs(lines[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing newly formatted file...\n",
      "\n",
      "Sample lines from file:\n",
      "\"잘 마실게.\t오후에 학부모 대표 모임에 사과하러 가는 거 교감선생님만 가시면 안돼요? 전 약속이 있어서..\"\n",
      "\n",
      "\"오후에 학부모 대표 모임에 사과하러 가는 거 교감선생님만 가시면 안돼요? 전 약속이 있어서..\t무슨 소리야? 누군 약속 없어서 거기 가? 사람 증말..무조건 같이 가. 사과 하루이틀해? 진짜 어이없어.이거 안 마실래.\"\n",
      "\n",
      "\"무슨 소리야? 누군 약속 없어서 거기 가? 사람 증말..무조건 같이 가. 사과 하루이틀해? 진짜 어이없어.이거 안 마실래.\t하..\"\n",
      "\n",
      "\"언니 과외 마쳤어요? 오늘은 늦게까지 했네요.\t기말고사라..내가 하나 들어줘요?\"\n",
      "\n",
      "\"기말고사라..내가 하나 들어줘요?\t괜찮아요.\"\n",
      "\n",
      "\"성적가지고 해리한테 너무 그러지 마. 애가 엄청 스트레스 받는 모양이던데. 아직 초등학생인데 점수 좀 못 받으면 뭐 어떻다고.\t스트레스 받는 애 성적이 저 모양이야?\"\n",
      "\n",
      "\"스트레스 받는 애 성적이 저 모양이야?\t지도 잘하고 싶겠지. 지라고 못하고 싶겠어?\"\n",
      "\n",
      "\"지도 잘하고 싶겠지. 지라고 못하고 싶겠어?\t속상하잖아. 어떤 앤 산에서 살다 연필 잡은 지도 몇달 안됐는데 90점 100점 척척 받아오고 누군 해달라는 거 다 해주고 키웠는데 맨날 바닥이고.\"\n",
      "\n",
      "\"속상하잖아. 어떤 앤 산에서 살다 연필 잡은 지도 몇달 안됐는데 90점 100점 척척 받아오고 누군 해달라는 거 다 해주고 키웠는데 맨날 바닥이고.\t또 비교한다. 당신 그렇게 비교하는 게 애한테 얼마나 안좋은.\"\n",
      "\n",
      "\"또 비교한다. 당신 그렇게 비교하는 게 애한테 얼마나 안좋은.\t아 몰라! 비교가 안되야 비굘 안하지. 하여튼 맘에 안 들어. 나같음 자존심이 상해서라도 열심히 하겠는데..아우. 진짜 보고 있음 열불터져서..\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datafile = os.path.join(corpus, 'formatted_ko_conversations2.txt')\n",
    "\n",
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter, 'unicode_escape'))\n",
    "\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in lines:\n",
    "        writer.writerow([pair])\n",
    "        \n",
    "print(\"\\nSample lines from file:\")\n",
    "printLines(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token:'PAD', SOS_token:'SOS', EOS_token:'EOS'}\n",
    "        self.num_words = 3\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items(): # k : 키값, v : 빈도수\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "                \n",
    "        print('keep_words {} / {} = {:.4f}'.format(len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)))\n",
    "        \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token:'PAD', SOS_token:'SOS', EOS_token:'EOS'}\n",
    "        self.num_words = 3\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 70560 sentence pairs\n",
      "Trimmed to 41945 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 64789\n",
      "['\"언니 과외 마쳤어요? 오늘은 늦게까지 했네요.', '기말고사라..내가 하나 들어줘요?\"']\n",
      "['\"기말고사라..내가 하나 들어줘요?', '괜찮아요.\"']\n",
      "['\"스트레스 받는 애 성적이 저 모양이야?', '지도 잘하고 싶겠지. 지라고 못하고 싶겠어?\"']\n",
      "['\"내가 보기엔', '나가서 냉수 한잔만 떠다줘. 아우. 괜히 또 열받네.\"']\n",
      "['\"나가서 냉수 한잔만 떠다줘. 아우. 괜히 또 열받네.', '내 생각엔\"']\n",
      "['\"내 생각엔', '빨리 냉수 좀 떠줘.\"']\n",
      "['\"빨리 냉수 좀 떠줘.', '알았어.\"']\n",
      "['\"이순팔 맞는데..', '아깐 자기가 이순댄가 뭐그래놓구는..\"']\n",
      "['\"아깐 자기가 이순댄가 뭐그래놓구는..', '이분 진짜 이름이 뭐예요?\"']\n",
      "['\"이분 진짜 이름이 뭐예요?', '본인이 이순팔이라면 이순팔이겠죠 뭐. 전 가도 돼죠?\"']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10  \n",
    "\n",
    "def readVocs(datafile, corpus_name): # corpus_name : chatData / datafile : formatted_ko_conversations.txt\n",
    "    print('Reading lines...')\n",
    "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [l.split('\\t') for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs # voc : 문서 단어집합 / pairs : 문장 쌍 집합\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name) # voc : 단어집합, pairs : 질문 쌍\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 64786 / 64786 = 1.0000\n",
      "Trimmed from 41945 pairs to 41945, 1.0000 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 1\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    \n",
    "    voc.trim(MIN_COUNT)\n",
    "    \n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        target_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        \n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "                \n",
    "        for word in target_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "                \n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[ 1641,   211,  1036,   211,   152],\n",
      "        [12094, 57585, 61486,  2029,     2],\n",
      "        [12095,  4022, 27713,     2,     0],\n",
      "        [12096, 28878, 61488,     0,     0],\n",
      "        [ 5900, 57586,     2,     0,     0],\n",
      "        [12097,     2,     0,     0,     0],\n",
      "        [12098,     0,     0,     0,     0],\n",
      "        [12100,     0,     0,     0,     0],\n",
      "        [    2,     0,     0,     0,     0]])\n",
      "lengths: tensor([9, 6, 5, 3, 2])\n",
      "target_variable: tensor([[  334, 28878, 10312, 61858, 10928],\n",
      "        [12101, 57587, 61205, 27698,  1825],\n",
      "        [ 7594,   813, 36722, 32549,     2],\n",
      "        [   98,  1286,     2, 12974,     0],\n",
      "        [ 7592,  2655,     0,  3251,     0],\n",
      "        [    2, 57588,     0, 61859,     0],\n",
      "        [    0,   208,     0,     2,     0],\n",
      "        [    0, 35211,     0,     0,     0],\n",
      "        [    0,   424,     0,     0,     0],\n",
      "        [    0,     2,     0,     0,     0]])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0]], dtype=torch.uint8)\n",
      "max_target_len: 10\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    # print([voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]) : [8052, 20188, 9959, 20189, 16252, 2]\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    # print(list(itertools.zip_longest(*l, fillvalue=fillvalue))) : (1957, 2, 2, 2, 0)\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l] \n",
    "    # print(indexes_batch) : [[5027, 1239, 9433, 2], [5951, 4686, 1476, 2], [1116, 5309, 2], [319, 2], [186, 2]]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) \n",
    "    # print(lengths) : tensor([4, 4, 3, 2, 2])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    # print(padList) : [[5027, 1239, 9433, 2], [5951, 4686, 1476, 2], [1116, 5309, 2, 0], [319, 2, 0, 0], [186, 2, 0, 0]]\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch]) \n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList) \n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, \n",
    "                          dropout = (0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq) # input_seq : shape=(max_length, batch_size)\n",
    "        # print(embedded.shape) : torch.Size([10, 64, 500]) [max_length, batch_size, hidden_size(은닉상태 크기)]\n",
    "        \n",
    "        # nn.utils.rnn.pack_padded_sequence : 패딩연산처리 쉽게하기 위해 중간에 빈공간 제거(형태 : tensor)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # input_lengths : shape=(batch_size)\n",
    "        # print(packed.batch_sizes) : tensor([64, 64, 64, 58, 52, 45, 38, 17,  8,  2])\n",
    "        \n",
    "        outputs, hidden = self.gru(packed, hidden) # 입력hidden : shape=(n_layers * num_directions, batch_size, hidden_size)\n",
    "        # print(outputs.batch_sizes) : tensor([64, 64, 63, 52, 47, 34, 24, 18, 12,  6])\n",
    "        # print(hidden.shape) : torch.Size([4, 64, 500]) [층 * 양방향이면2 아니면1, batch_size, hidden_size]\n",
    "        \n",
    "        # nn.utils.rnn.pad_packed_sequence : 패딩연산이 끝난 것을 다시 원래대로 (형태 : torch)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # print(outputs.shape)# : torch.Size([10, 64, 1000]) # [max_length, batch_size, hidden_size(양방향으로 진행했으면 *2)]\n",
    "        \n",
    "        # 양방향 GRU의 출력을 합산합니다\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # print(outputs.shape) : torch.Size([10, 64, 500])\n",
    "        \n",
    "        # hidden : GRU의 최종 은닉 상태\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, 'is not an appropriate attention method.')\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "            \n",
    "    # 가중치 계산을 dot-product로 계산\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        # print(torch.sum(hidden * encoder_output, dim=2).shape) : torch.Size([10, 64]) 10개 생성[max_length, batch_size]\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "    \n",
    "    # \n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        # print(energy.shape) : torch.Size([10, 64, 500]) 10개 생성[max_length, batch_size, hidden_size]\n",
    "        \n",
    "        # print(torch.sum(hidden * energy, dim=2).shape) : torch.Size([10, 64]) 10개 생성[max_length, batch_size]\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "    \n",
    "    \n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        # cat : 합칠 때 차원은 2차원으로 / expand : 확장\n",
    "        # Tanh 함수는 함수값을 [-1, 1]로 제한시킴\n",
    "        # print((hidden.expand(encoder_output.size(0), -1, -1).shape)) : torch.Size([10, 64, 500])\n",
    "        # print(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2).shape) : torch.Size([10, 64, 1000])\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        # print(energy.shape) : torch.Size([10, 64, 500]) 10개 생성[max_length, batch_size, hidden_size]\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "            \n",
    "        attn_energies = attn_energies.t() # t() : 행과 열을 바꿔서 저장[1, 2, 3], [4, 5, 6] -> [1, 4, 7], [2, 5, 8]\n",
    "        \n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # 참조를 보존해 둡니다\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 레이어를 정의합니다\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # 주의: 한 단위 시간에 대해 한 단계(단어)만을 수행합니다\n",
    "        # 현재의 입력 단어에 대한 임베딩을 구합니다   \n",
    "        embedded = self.embedding(input_step) # input_step : 입력 시퀀스 배치에 대한 한 단위 시간(한 단어). shape=(1, batch_size)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # print(embedded.shape) : torch.Size([1, 64, 500])\n",
    "        \n",
    "        # 양방향x\n",
    "        # last_hidden : GRU의 마지막 은닉 레이어. shape=(n_layers * num_directions, batch_size, hidden_size)\n",
    "        # print(last_hidden.shape) : torch.Size([2, 64, 500]) \n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden) \n",
    "        # print(rnn_output.shape) : torch.Size([1, 64, 500])\n",
    "        # print(hidden.shape) : torch.Size([2, 64, 500])\n",
    "\n",
    "        # attention 가중치\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs) # encoder_outputs : 인코더 모델 출력 shape=(max_length, batch_size, hidden_size)\n",
    "        # print(attn_weights.shape) : torch.Size([64, 1, 10]) \n",
    "\n",
    "        # 인코더 출력에 어텐션을 곱하여 새로운 context vector생성\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # print(context.shape) : torch.Size([64, 1, 500])\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0) # print(rnn_output.shape) : torch.Size([64, 500])\n",
    "        context = context.squeeze(1) # print(context.shape) : torch.Size([64, 500])\n",
    "        concat_input = torch.cat((rnn_output, context), 1) # print(concat_input.shape) : torch.Size([64, 1000])\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # print(concat_output.shape) : torch.Size([64, 500])\n",
    "\n",
    "        # output : 각 단어가 디코딩된 시퀀스에서 다음 단어로 사용되었을 때 적합할 확률을 나타내는 정규화된 softmax 텐서. \n",
    "        # shape=(batch_size, voc.num_words)\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer,\n",
    "         batch_size, clip, max_length = MAX_LENGTH):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    \n",
    "    # EncoderRNN의 forward부분 실행\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "    \n",
    "    # 초기 디코더 입력을 생성(각 문장을 SOS 토큰으로 시작)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    # 디코더의 초기 은닉 상태를 인코더의 마지막 은닉 상태로\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    \n",
    "    # teacher_forcing : Decoder부분에서 앞 단어가 잘못 추측되었을 경우 뒤에도 달라지니 정답을 입력해 주는 것\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            # LuongAttnDecoderRNN의 forward로 실행\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "             # Teacher forcing 사용: 다음 입력을 현재의 목표로 둡니다\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # Teacher forcing 미사용: 다음 입력을 디코더의 출력으로 둡니다\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "            \n",
    "    loss.backward()\n",
    "    \n",
    "    # clip_grad_norm_: 그라디언트를 제자리에서 수정합니다\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # 각 단계에 대한 배치 설정\n",
    "    # batch2TrainData : return inp, lengths, output, mask, max_target_len\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        \n",
    "        print_loss += loss\n",
    "\n",
    "\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Checkpoint를 저장\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탐욕적 디코딩(Greedy decoding) : 각 단계에 대해 단순히 decoder_output 에서 가장 높은 softmax값을 갖는 단어를 선택하는 방식\n",
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "\n",
    "        # EncoderRNN의 forward부분 실행\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "\n",
    "        # encoder의 마지막 hidden이 decoder의 처음 hidden\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        # decoder의 처음입력을 SOS로 초기화\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "\n",
    "        # 디코더가 단어를 덧붙여 나갈 텐서를 초기화\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # LuongAttnDecoderRNN의 forward로 실행\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            # 가장 가능성 높은 단어 토큰과 그 softmax 점수를 구합니다\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "\n",
    "            # 토큰, 점수 기록\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "\n",
    "            # 현재의 토큰을 디코더의 다음 입력으로 준비시킵니다(차원을 증가시켜서)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    # indexes_batch : 문장을 단어집합에 저장된 수로 바꾼후 마지막에 EOS추가하는 함수\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    \n",
    "    # lengths 텐서를 만듭니다\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    # 배치의 차원을 뒤집어서 모델이 사용하는 형태로 만듭니다\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    # searcher를 이용하여 문장을 디코딩합니다\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    # 인덱스 -> 단어\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # 입력 문장을 받아옵니다\n",
    "            input_sentence = input('> ')\n",
    "            # 종료 조건인지 검사합니다\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # 문장을 평가합니다\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # 응답 문장을 형식에 맞춰 출력합니다\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# loadFilename이 제공되는 경우에는 모델을 불러옵니다\n",
    "if loadFilename:\n",
    "    # 모델을 학습할 때와 같은 기기에서 불러오는 경우\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # GPU에서 학습한 모델을 CPU로 불러오는 경우\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-f319a975feb5>:4: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:20.)\n",
      "  loss = crossEntropy.masked_select(mask).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1; Percent complete: 0.1%; Average loss: 11.0817\n",
      "Iteration: 2; Percent complete: 0.1%; Average loss: 11.0312\n",
      "Iteration: 3; Percent complete: 0.1%; Average loss: 10.9225\n",
      "Iteration: 4; Percent complete: 0.2%; Average loss: 10.7839\n",
      "Iteration: 5; Percent complete: 0.2%; Average loss: 10.3723\n",
      "Iteration: 6; Percent complete: 0.3%; Average loss: 9.9982\n",
      "Iteration: 7; Percent complete: 0.4%; Average loss: 9.8479\n",
      "Iteration: 8; Percent complete: 0.4%; Average loss: 9.2571\n",
      "Iteration: 9; Percent complete: 0.4%; Average loss: 9.0635\n",
      "Iteration: 10; Percent complete: 0.5%; Average loss: 9.7727\n",
      "Iteration: 11; Percent complete: 0.5%; Average loss: 9.9494\n",
      "Iteration: 12; Percent complete: 0.6%; Average loss: 10.0562\n",
      "Iteration: 13; Percent complete: 0.7%; Average loss: 10.1544\n",
      "Iteration: 14; Percent complete: 0.7%; Average loss: 10.2311\n",
      "Iteration: 15; Percent complete: 0.8%; Average loss: 10.1231\n",
      "Iteration: 16; Percent complete: 0.8%; Average loss: 9.8580\n",
      "Iteration: 17; Percent complete: 0.9%; Average loss: 9.5194\n",
      "Iteration: 18; Percent complete: 0.9%; Average loss: 9.2657\n",
      "Iteration: 19; Percent complete: 0.9%; Average loss: 9.0244\n",
      "Iteration: 20; Percent complete: 1.0%; Average loss: 8.6403\n",
      "Iteration: 21; Percent complete: 1.1%; Average loss: 8.9510\n",
      "Iteration: 22; Percent complete: 1.1%; Average loss: 8.8625\n",
      "Iteration: 23; Percent complete: 1.1%; Average loss: 9.0393\n",
      "Iteration: 24; Percent complete: 1.2%; Average loss: 8.7613\n",
      "Iteration: 25; Percent complete: 1.2%; Average loss: 8.6136\n",
      "Iteration: 26; Percent complete: 1.3%; Average loss: 8.8655\n",
      "Iteration: 27; Percent complete: 1.4%; Average loss: 8.3111\n",
      "Iteration: 28; Percent complete: 1.4%; Average loss: 8.8278\n",
      "Iteration: 29; Percent complete: 1.5%; Average loss: 8.8031\n",
      "Iteration: 30; Percent complete: 1.5%; Average loss: 8.5879\n",
      "Iteration: 31; Percent complete: 1.6%; Average loss: 8.8529\n",
      "Iteration: 32; Percent complete: 1.6%; Average loss: 9.2248\n",
      "Iteration: 33; Percent complete: 1.7%; Average loss: 8.7344\n",
      "Iteration: 34; Percent complete: 1.7%; Average loss: 8.8443\n",
      "Iteration: 35; Percent complete: 1.8%; Average loss: 8.5084\n",
      "Iteration: 36; Percent complete: 1.8%; Average loss: 8.7324\n",
      "Iteration: 37; Percent complete: 1.8%; Average loss: 8.6561\n",
      "Iteration: 38; Percent complete: 1.9%; Average loss: 8.5221\n",
      "Iteration: 39; Percent complete: 1.9%; Average loss: 8.6944\n",
      "Iteration: 40; Percent complete: 2.0%; Average loss: 8.3804\n",
      "Iteration: 41; Percent complete: 2.1%; Average loss: 8.5565\n",
      "Iteration: 42; Percent complete: 2.1%; Average loss: 8.6934\n",
      "Iteration: 43; Percent complete: 2.1%; Average loss: 8.5936\n",
      "Iteration: 44; Percent complete: 2.2%; Average loss: 8.7374\n",
      "Iteration: 45; Percent complete: 2.2%; Average loss: 8.2039\n",
      "Iteration: 46; Percent complete: 2.3%; Average loss: 8.8911\n",
      "Iteration: 47; Percent complete: 2.4%; Average loss: 8.7330\n",
      "Iteration: 48; Percent complete: 2.4%; Average loss: 8.6698\n",
      "Iteration: 49; Percent complete: 2.5%; Average loss: 8.3761\n",
      "Iteration: 50; Percent complete: 2.5%; Average loss: 8.6491\n",
      "Iteration: 51; Percent complete: 2.5%; Average loss: 8.3322\n",
      "Iteration: 52; Percent complete: 2.6%; Average loss: 8.6161\n",
      "Iteration: 53; Percent complete: 2.6%; Average loss: 8.4451\n",
      "Iteration: 54; Percent complete: 2.7%; Average loss: 8.3851\n",
      "Iteration: 55; Percent complete: 2.8%; Average loss: 8.3687\n",
      "Iteration: 56; Percent complete: 2.8%; Average loss: 8.5008\n",
      "Iteration: 57; Percent complete: 2.9%; Average loss: 8.3609\n",
      "Iteration: 58; Percent complete: 2.9%; Average loss: 8.6350\n",
      "Iteration: 59; Percent complete: 2.9%; Average loss: 8.4064\n",
      "Iteration: 60; Percent complete: 3.0%; Average loss: 8.3867\n",
      "Iteration: 61; Percent complete: 3.0%; Average loss: 8.5423\n",
      "Iteration: 62; Percent complete: 3.1%; Average loss: 8.6690\n",
      "Iteration: 63; Percent complete: 3.1%; Average loss: 8.6429\n",
      "Iteration: 64; Percent complete: 3.2%; Average loss: 8.4340\n",
      "Iteration: 65; Percent complete: 3.2%; Average loss: 8.3940\n",
      "Iteration: 66; Percent complete: 3.3%; Average loss: 8.1758\n",
      "Iteration: 67; Percent complete: 3.4%; Average loss: 8.4548\n",
      "Iteration: 68; Percent complete: 3.4%; Average loss: 8.4693\n",
      "Iteration: 69; Percent complete: 3.5%; Average loss: 8.5495\n",
      "Iteration: 70; Percent complete: 3.5%; Average loss: 8.1892\n",
      "Iteration: 71; Percent complete: 3.5%; Average loss: 8.2769\n",
      "Iteration: 72; Percent complete: 3.6%; Average loss: 8.5715\n",
      "Iteration: 73; Percent complete: 3.6%; Average loss: 8.6239\n",
      "Iteration: 74; Percent complete: 3.7%; Average loss: 8.6373\n",
      "Iteration: 75; Percent complete: 3.8%; Average loss: 8.6407\n",
      "Iteration: 76; Percent complete: 3.8%; Average loss: 8.2732\n",
      "Iteration: 77; Percent complete: 3.9%; Average loss: 8.5092\n",
      "Iteration: 78; Percent complete: 3.9%; Average loss: 8.5569\n",
      "Iteration: 79; Percent complete: 4.0%; Average loss: 8.5482\n",
      "Iteration: 80; Percent complete: 4.0%; Average loss: 8.4131\n",
      "Iteration: 81; Percent complete: 4.0%; Average loss: 8.4823\n",
      "Iteration: 82; Percent complete: 4.1%; Average loss: 8.4889\n",
      "Iteration: 83; Percent complete: 4.2%; Average loss: 8.5020\n",
      "Iteration: 84; Percent complete: 4.2%; Average loss: 8.4881\n",
      "Iteration: 85; Percent complete: 4.2%; Average loss: 8.6356\n",
      "Iteration: 86; Percent complete: 4.3%; Average loss: 8.4913\n",
      "Iteration: 87; Percent complete: 4.3%; Average loss: 8.6027\n",
      "Iteration: 88; Percent complete: 4.4%; Average loss: 8.6945\n",
      "Iteration: 89; Percent complete: 4.5%; Average loss: 8.3381\n",
      "Iteration: 90; Percent complete: 4.5%; Average loss: 8.3011\n",
      "Iteration: 91; Percent complete: 4.5%; Average loss: 8.4566\n",
      "Iteration: 92; Percent complete: 4.6%; Average loss: 8.5837\n",
      "Iteration: 93; Percent complete: 4.7%; Average loss: 8.6838\n",
      "Iteration: 94; Percent complete: 4.7%; Average loss: 8.5297\n",
      "Iteration: 95; Percent complete: 4.8%; Average loss: 8.5371\n",
      "Iteration: 96; Percent complete: 4.8%; Average loss: 8.3976\n",
      "Iteration: 97; Percent complete: 4.9%; Average loss: 8.4762\n",
      "Iteration: 98; Percent complete: 4.9%; Average loss: 8.5874\n",
      "Iteration: 99; Percent complete: 5.0%; Average loss: 8.2197\n",
      "Iteration: 100; Percent complete: 5.0%; Average loss: 8.3180\n",
      "Iteration: 101; Percent complete: 5.1%; Average loss: 8.6993\n",
      "Iteration: 102; Percent complete: 5.1%; Average loss: 8.4173\n",
      "Iteration: 103; Percent complete: 5.1%; Average loss: 8.3495\n",
      "Iteration: 104; Percent complete: 5.2%; Average loss: 8.4049\n",
      "Iteration: 105; Percent complete: 5.2%; Average loss: 8.6959\n",
      "Iteration: 106; Percent complete: 5.3%; Average loss: 8.1895\n",
      "Iteration: 107; Percent complete: 5.3%; Average loss: 8.6210\n",
      "Iteration: 108; Percent complete: 5.4%; Average loss: 8.3003\n",
      "Iteration: 109; Percent complete: 5.5%; Average loss: 8.6471\n",
      "Iteration: 110; Percent complete: 5.5%; Average loss: 8.7773\n",
      "Iteration: 111; Percent complete: 5.5%; Average loss: 8.5113\n",
      "Iteration: 112; Percent complete: 5.6%; Average loss: 8.7032\n",
      "Iteration: 113; Percent complete: 5.7%; Average loss: 8.3352\n",
      "Iteration: 114; Percent complete: 5.7%; Average loss: 8.4750\n",
      "Iteration: 115; Percent complete: 5.8%; Average loss: 8.4175\n",
      "Iteration: 116; Percent complete: 5.8%; Average loss: 8.4620\n",
      "Iteration: 117; Percent complete: 5.9%; Average loss: 8.2869\n",
      "Iteration: 118; Percent complete: 5.9%; Average loss: 8.3707\n",
      "Iteration: 119; Percent complete: 5.9%; Average loss: 8.4267\n",
      "Iteration: 120; Percent complete: 6.0%; Average loss: 8.5463\n",
      "Iteration: 121; Percent complete: 6.0%; Average loss: 8.9427\n",
      "Iteration: 122; Percent complete: 6.1%; Average loss: 8.6662\n",
      "Iteration: 123; Percent complete: 6.2%; Average loss: 8.5870\n",
      "Iteration: 124; Percent complete: 6.2%; Average loss: 8.2716\n",
      "Iteration: 125; Percent complete: 6.2%; Average loss: 8.5003\n",
      "Iteration: 126; Percent complete: 6.3%; Average loss: 8.5145\n",
      "Iteration: 127; Percent complete: 6.3%; Average loss: 8.6296\n",
      "Iteration: 128; Percent complete: 6.4%; Average loss: 8.3941\n",
      "Iteration: 129; Percent complete: 6.5%; Average loss: 8.7346\n",
      "Iteration: 130; Percent complete: 6.5%; Average loss: 8.3973\n",
      "Iteration: 131; Percent complete: 6.6%; Average loss: 8.4340\n",
      "Iteration: 132; Percent complete: 6.6%; Average loss: 8.5180\n",
      "Iteration: 133; Percent complete: 6.7%; Average loss: 8.2899\n",
      "Iteration: 134; Percent complete: 6.7%; Average loss: 8.4041\n",
      "Iteration: 135; Percent complete: 6.8%; Average loss: 8.4620\n",
      "Iteration: 136; Percent complete: 6.8%; Average loss: 8.2658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 137; Percent complete: 6.9%; Average loss: 8.4539\n",
      "Iteration: 138; Percent complete: 6.9%; Average loss: 8.5403\n",
      "Iteration: 139; Percent complete: 7.0%; Average loss: 8.3499\n",
      "Iteration: 140; Percent complete: 7.0%; Average loss: 8.4432\n",
      "Iteration: 141; Percent complete: 7.0%; Average loss: 8.4601\n",
      "Iteration: 142; Percent complete: 7.1%; Average loss: 8.5288\n",
      "Iteration: 143; Percent complete: 7.1%; Average loss: 8.1217\n",
      "Iteration: 144; Percent complete: 7.2%; Average loss: 8.1632\n",
      "Iteration: 145; Percent complete: 7.2%; Average loss: 8.4542\n",
      "Iteration: 146; Percent complete: 7.3%; Average loss: 8.1899\n",
      "Iteration: 147; Percent complete: 7.3%; Average loss: 8.5494\n",
      "Iteration: 148; Percent complete: 7.4%; Average loss: 8.4366\n",
      "Iteration: 149; Percent complete: 7.4%; Average loss: 8.2914\n",
      "Iteration: 150; Percent complete: 7.5%; Average loss: 8.1166\n",
      "Iteration: 151; Percent complete: 7.5%; Average loss: 8.6830\n",
      "Iteration: 152; Percent complete: 7.6%; Average loss: 8.3744\n",
      "Iteration: 153; Percent complete: 7.6%; Average loss: 8.4396\n",
      "Iteration: 154; Percent complete: 7.7%; Average loss: 8.6509\n",
      "Iteration: 155; Percent complete: 7.8%; Average loss: 8.6366\n",
      "Iteration: 156; Percent complete: 7.8%; Average loss: 8.4412\n",
      "Iteration: 157; Percent complete: 7.8%; Average loss: 8.3187\n",
      "Iteration: 158; Percent complete: 7.9%; Average loss: 8.3648\n",
      "Iteration: 159; Percent complete: 8.0%; Average loss: 8.4450\n",
      "Iteration: 160; Percent complete: 8.0%; Average loss: 8.0588\n",
      "Iteration: 161; Percent complete: 8.1%; Average loss: 8.5812\n",
      "Iteration: 162; Percent complete: 8.1%; Average loss: 8.2674\n",
      "Iteration: 163; Percent complete: 8.2%; Average loss: 8.4841\n",
      "Iteration: 164; Percent complete: 8.2%; Average loss: 8.6464\n",
      "Iteration: 165; Percent complete: 8.2%; Average loss: 8.2895\n",
      "Iteration: 166; Percent complete: 8.3%; Average loss: 8.5291\n",
      "Iteration: 167; Percent complete: 8.3%; Average loss: 8.8307\n",
      "Iteration: 168; Percent complete: 8.4%; Average loss: 8.2679\n",
      "Iteration: 169; Percent complete: 8.5%; Average loss: 8.4975\n",
      "Iteration: 170; Percent complete: 8.5%; Average loss: 8.6127\n",
      "Iteration: 171; Percent complete: 8.6%; Average loss: 8.2052\n",
      "Iteration: 172; Percent complete: 8.6%; Average loss: 8.6656\n",
      "Iteration: 173; Percent complete: 8.6%; Average loss: 8.2434\n",
      "Iteration: 174; Percent complete: 8.7%; Average loss: 8.0796\n",
      "Iteration: 175; Percent complete: 8.8%; Average loss: 8.5197\n",
      "Iteration: 176; Percent complete: 8.8%; Average loss: 8.3953\n",
      "Iteration: 177; Percent complete: 8.8%; Average loss: 8.3272\n",
      "Iteration: 178; Percent complete: 8.9%; Average loss: 8.7389\n",
      "Iteration: 179; Percent complete: 8.9%; Average loss: 8.5314\n",
      "Iteration: 180; Percent complete: 9.0%; Average loss: 8.4424\n",
      "Iteration: 181; Percent complete: 9.0%; Average loss: 8.3706\n",
      "Iteration: 182; Percent complete: 9.1%; Average loss: 8.0109\n",
      "Iteration: 183; Percent complete: 9.2%; Average loss: 8.0070\n",
      "Iteration: 184; Percent complete: 9.2%; Average loss: 7.9246\n",
      "Iteration: 185; Percent complete: 9.2%; Average loss: 8.3409\n",
      "Iteration: 186; Percent complete: 9.3%; Average loss: 8.2003\n",
      "Iteration: 187; Percent complete: 9.3%; Average loss: 8.5497\n",
      "Iteration: 188; Percent complete: 9.4%; Average loss: 8.3256\n",
      "Iteration: 189; Percent complete: 9.4%; Average loss: 8.4299\n",
      "Iteration: 190; Percent complete: 9.5%; Average loss: 7.9515\n",
      "Iteration: 191; Percent complete: 9.6%; Average loss: 8.0526\n",
      "Iteration: 192; Percent complete: 9.6%; Average loss: 8.5969\n",
      "Iteration: 193; Percent complete: 9.7%; Average loss: 8.3737\n",
      "Iteration: 194; Percent complete: 9.7%; Average loss: 8.4276\n",
      "Iteration: 195; Percent complete: 9.8%; Average loss: 8.4358\n",
      "Iteration: 196; Percent complete: 9.8%; Average loss: 8.2867\n",
      "Iteration: 197; Percent complete: 9.8%; Average loss: 8.4419\n",
      "Iteration: 198; Percent complete: 9.9%; Average loss: 8.1284\n",
      "Iteration: 199; Percent complete: 10.0%; Average loss: 8.3123\n",
      "Iteration: 200; Percent complete: 10.0%; Average loss: 8.2876\n",
      "Iteration: 201; Percent complete: 10.1%; Average loss: 8.3126\n",
      "Iteration: 202; Percent complete: 10.1%; Average loss: 8.0835\n",
      "Iteration: 203; Percent complete: 10.2%; Average loss: 8.5352\n",
      "Iteration: 204; Percent complete: 10.2%; Average loss: 8.3085\n",
      "Iteration: 205; Percent complete: 10.2%; Average loss: 8.3836\n",
      "Iteration: 206; Percent complete: 10.3%; Average loss: 8.4908\n",
      "Iteration: 207; Percent complete: 10.3%; Average loss: 7.9666\n",
      "Iteration: 208; Percent complete: 10.4%; Average loss: 8.0589\n",
      "Iteration: 209; Percent complete: 10.4%; Average loss: 8.5085\n",
      "Iteration: 210; Percent complete: 10.5%; Average loss: 8.3954\n",
      "Iteration: 211; Percent complete: 10.5%; Average loss: 8.3902\n",
      "Iteration: 212; Percent complete: 10.6%; Average loss: 8.0768\n",
      "Iteration: 213; Percent complete: 10.7%; Average loss: 8.1455\n",
      "Iteration: 214; Percent complete: 10.7%; Average loss: 8.1620\n",
      "Iteration: 215; Percent complete: 10.8%; Average loss: 8.0734\n",
      "Iteration: 216; Percent complete: 10.8%; Average loss: 8.1578\n",
      "Iteration: 217; Percent complete: 10.8%; Average loss: 7.7853\n",
      "Iteration: 218; Percent complete: 10.9%; Average loss: 8.2135\n",
      "Iteration: 219; Percent complete: 10.9%; Average loss: 8.2346\n",
      "Iteration: 220; Percent complete: 11.0%; Average loss: 8.2937\n",
      "Iteration: 221; Percent complete: 11.1%; Average loss: 7.6811\n",
      "Iteration: 222; Percent complete: 11.1%; Average loss: 8.1264\n",
      "Iteration: 223; Percent complete: 11.2%; Average loss: 7.9702\n",
      "Iteration: 224; Percent complete: 11.2%; Average loss: 8.2235\n",
      "Iteration: 225; Percent complete: 11.2%; Average loss: 8.2166\n",
      "Iteration: 226; Percent complete: 11.3%; Average loss: 8.4249\n",
      "Iteration: 227; Percent complete: 11.3%; Average loss: 8.2434\n",
      "Iteration: 228; Percent complete: 11.4%; Average loss: 8.2358\n",
      "Iteration: 229; Percent complete: 11.5%; Average loss: 8.4800\n",
      "Iteration: 230; Percent complete: 11.5%; Average loss: 8.1505\n",
      "Iteration: 231; Percent complete: 11.6%; Average loss: 8.3551\n",
      "Iteration: 232; Percent complete: 11.6%; Average loss: 8.4145\n",
      "Iteration: 233; Percent complete: 11.7%; Average loss: 8.4107\n",
      "Iteration: 234; Percent complete: 11.7%; Average loss: 8.4615\n",
      "Iteration: 235; Percent complete: 11.8%; Average loss: 8.0979\n",
      "Iteration: 236; Percent complete: 11.8%; Average loss: 8.2230\n",
      "Iteration: 237; Percent complete: 11.8%; Average loss: 8.4223\n",
      "Iteration: 238; Percent complete: 11.9%; Average loss: 8.3708\n",
      "Iteration: 239; Percent complete: 11.9%; Average loss: 8.3065\n",
      "Iteration: 240; Percent complete: 12.0%; Average loss: 8.3193\n",
      "Iteration: 241; Percent complete: 12.0%; Average loss: 7.6228\n",
      "Iteration: 242; Percent complete: 12.1%; Average loss: 8.0064\n",
      "Iteration: 243; Percent complete: 12.2%; Average loss: 8.3684\n",
      "Iteration: 244; Percent complete: 12.2%; Average loss: 8.0724\n",
      "Iteration: 245; Percent complete: 12.2%; Average loss: 8.1429\n",
      "Iteration: 246; Percent complete: 12.3%; Average loss: 8.2471\n",
      "Iteration: 247; Percent complete: 12.3%; Average loss: 8.2628\n",
      "Iteration: 248; Percent complete: 12.4%; Average loss: 8.3328\n",
      "Iteration: 249; Percent complete: 12.4%; Average loss: 8.2925\n",
      "Iteration: 250; Percent complete: 12.5%; Average loss: 8.6148\n",
      "Iteration: 251; Percent complete: 12.6%; Average loss: 8.0622\n",
      "Iteration: 252; Percent complete: 12.6%; Average loss: 8.1993\n",
      "Iteration: 253; Percent complete: 12.7%; Average loss: 8.0982\n",
      "Iteration: 254; Percent complete: 12.7%; Average loss: 8.2512\n",
      "Iteration: 255; Percent complete: 12.8%; Average loss: 8.0911\n",
      "Iteration: 256; Percent complete: 12.8%; Average loss: 7.8946\n",
      "Iteration: 257; Percent complete: 12.8%; Average loss: 8.3988\n",
      "Iteration: 258; Percent complete: 12.9%; Average loss: 8.3900\n",
      "Iteration: 259; Percent complete: 13.0%; Average loss: 8.0018\n",
      "Iteration: 260; Percent complete: 13.0%; Average loss: 8.2152\n",
      "Iteration: 261; Percent complete: 13.1%; Average loss: 8.4028\n",
      "Iteration: 262; Percent complete: 13.1%; Average loss: 8.1039\n",
      "Iteration: 263; Percent complete: 13.2%; Average loss: 8.3348\n",
      "Iteration: 264; Percent complete: 13.2%; Average loss: 8.2043\n",
      "Iteration: 265; Percent complete: 13.2%; Average loss: 8.1293\n",
      "Iteration: 266; Percent complete: 13.3%; Average loss: 8.6681\n",
      "Iteration: 267; Percent complete: 13.4%; Average loss: 8.2129\n",
      "Iteration: 268; Percent complete: 13.4%; Average loss: 7.8695\n",
      "Iteration: 269; Percent complete: 13.5%; Average loss: 8.1881\n",
      "Iteration: 270; Percent complete: 13.5%; Average loss: 8.2100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 271; Percent complete: 13.6%; Average loss: 8.1941\n",
      "Iteration: 272; Percent complete: 13.6%; Average loss: 8.6134\n",
      "Iteration: 273; Percent complete: 13.7%; Average loss: 8.0422\n",
      "Iteration: 274; Percent complete: 13.7%; Average loss: 8.2159\n",
      "Iteration: 275; Percent complete: 13.8%; Average loss: 8.2046\n",
      "Iteration: 276; Percent complete: 13.8%; Average loss: 8.4238\n",
      "Iteration: 277; Percent complete: 13.9%; Average loss: 8.2635\n",
      "Iteration: 278; Percent complete: 13.9%; Average loss: 8.1601\n",
      "Iteration: 279; Percent complete: 14.0%; Average loss: 8.1625\n",
      "Iteration: 280; Percent complete: 14.0%; Average loss: 8.2195\n",
      "Iteration: 281; Percent complete: 14.1%; Average loss: 8.3714\n",
      "Iteration: 282; Percent complete: 14.1%; Average loss: 7.9778\n",
      "Iteration: 283; Percent complete: 14.1%; Average loss: 8.0030\n",
      "Iteration: 284; Percent complete: 14.2%; Average loss: 8.2295\n",
      "Iteration: 285; Percent complete: 14.2%; Average loss: 8.3763\n",
      "Iteration: 286; Percent complete: 14.3%; Average loss: 8.1132\n",
      "Iteration: 287; Percent complete: 14.3%; Average loss: 8.2051\n",
      "Iteration: 288; Percent complete: 14.4%; Average loss: 8.2361\n",
      "Iteration: 289; Percent complete: 14.4%; Average loss: 8.2546\n",
      "Iteration: 290; Percent complete: 14.5%; Average loss: 8.2096\n",
      "Iteration: 291; Percent complete: 14.5%; Average loss: 8.4254\n",
      "Iteration: 292; Percent complete: 14.6%; Average loss: 7.8767\n",
      "Iteration: 293; Percent complete: 14.6%; Average loss: 8.3027\n",
      "Iteration: 294; Percent complete: 14.7%; Average loss: 7.7945\n",
      "Iteration: 295; Percent complete: 14.8%; Average loss: 8.0267\n",
      "Iteration: 296; Percent complete: 14.8%; Average loss: 8.1101\n",
      "Iteration: 297; Percent complete: 14.8%; Average loss: 7.8446\n",
      "Iteration: 298; Percent complete: 14.9%; Average loss: 7.9218\n",
      "Iteration: 299; Percent complete: 14.9%; Average loss: 7.9540\n",
      "Iteration: 300; Percent complete: 15.0%; Average loss: 8.4076\n",
      "Iteration: 301; Percent complete: 15.0%; Average loss: 7.9078\n",
      "Iteration: 302; Percent complete: 15.1%; Average loss: 8.3708\n",
      "Iteration: 303; Percent complete: 15.2%; Average loss: 8.0706\n",
      "Iteration: 304; Percent complete: 15.2%; Average loss: 8.3920\n",
      "Iteration: 305; Percent complete: 15.2%; Average loss: 8.6342\n",
      "Iteration: 306; Percent complete: 15.3%; Average loss: 8.3080\n",
      "Iteration: 307; Percent complete: 15.3%; Average loss: 8.1462\n",
      "Iteration: 308; Percent complete: 15.4%; Average loss: 8.0181\n",
      "Iteration: 309; Percent complete: 15.4%; Average loss: 8.3068\n",
      "Iteration: 310; Percent complete: 15.5%; Average loss: 8.2828\n",
      "Iteration: 311; Percent complete: 15.6%; Average loss: 8.2932\n",
      "Iteration: 312; Percent complete: 15.6%; Average loss: 8.3374\n",
      "Iteration: 313; Percent complete: 15.7%; Average loss: 8.0377\n",
      "Iteration: 314; Percent complete: 15.7%; Average loss: 8.1368\n",
      "Iteration: 315; Percent complete: 15.8%; Average loss: 8.2489\n",
      "Iteration: 316; Percent complete: 15.8%; Average loss: 8.5479\n",
      "Iteration: 317; Percent complete: 15.8%; Average loss: 8.2332\n",
      "Iteration: 318; Percent complete: 15.9%; Average loss: 8.2652\n",
      "Iteration: 319; Percent complete: 16.0%; Average loss: 7.8131\n",
      "Iteration: 320; Percent complete: 16.0%; Average loss: 8.5277\n",
      "Iteration: 321; Percent complete: 16.1%; Average loss: 8.4595\n",
      "Iteration: 322; Percent complete: 16.1%; Average loss: 8.4629\n",
      "Iteration: 323; Percent complete: 16.2%; Average loss: 8.0539\n",
      "Iteration: 324; Percent complete: 16.2%; Average loss: 8.0569\n",
      "Iteration: 325; Percent complete: 16.2%; Average loss: 7.9867\n",
      "Iteration: 326; Percent complete: 16.3%; Average loss: 8.2427\n",
      "Iteration: 327; Percent complete: 16.4%; Average loss: 8.1069\n",
      "Iteration: 328; Percent complete: 16.4%; Average loss: 7.7654\n",
      "Iteration: 329; Percent complete: 16.4%; Average loss: 7.6412\n",
      "Iteration: 330; Percent complete: 16.5%; Average loss: 7.9133\n",
      "Iteration: 331; Percent complete: 16.6%; Average loss: 7.8521\n",
      "Iteration: 332; Percent complete: 16.6%; Average loss: 7.8425\n",
      "Iteration: 333; Percent complete: 16.7%; Average loss: 8.2126\n",
      "Iteration: 334; Percent complete: 16.7%; Average loss: 8.0145\n",
      "Iteration: 335; Percent complete: 16.8%; Average loss: 8.2358\n",
      "Iteration: 336; Percent complete: 16.8%; Average loss: 7.9327\n",
      "Iteration: 337; Percent complete: 16.9%; Average loss: 8.2820\n",
      "Iteration: 338; Percent complete: 16.9%; Average loss: 8.3631\n",
      "Iteration: 339; Percent complete: 17.0%; Average loss: 8.1748\n",
      "Iteration: 340; Percent complete: 17.0%; Average loss: 8.0457\n",
      "Iteration: 341; Percent complete: 17.1%; Average loss: 8.1408\n",
      "Iteration: 342; Percent complete: 17.1%; Average loss: 8.0976\n",
      "Iteration: 343; Percent complete: 17.2%; Average loss: 7.6028\n",
      "Iteration: 344; Percent complete: 17.2%; Average loss: 7.9461\n",
      "Iteration: 345; Percent complete: 17.2%; Average loss: 7.9917\n",
      "Iteration: 346; Percent complete: 17.3%; Average loss: 8.2226\n",
      "Iteration: 347; Percent complete: 17.3%; Average loss: 8.2850\n",
      "Iteration: 348; Percent complete: 17.4%; Average loss: 8.2811\n",
      "Iteration: 349; Percent complete: 17.4%; Average loss: 7.8497\n",
      "Iteration: 350; Percent complete: 17.5%; Average loss: 8.1796\n",
      "Iteration: 351; Percent complete: 17.5%; Average loss: 8.1569\n",
      "Iteration: 352; Percent complete: 17.6%; Average loss: 8.0812\n",
      "Iteration: 353; Percent complete: 17.6%; Average loss: 7.9597\n",
      "Iteration: 354; Percent complete: 17.7%; Average loss: 8.1806\n",
      "Iteration: 355; Percent complete: 17.8%; Average loss: 8.2929\n",
      "Iteration: 356; Percent complete: 17.8%; Average loss: 8.3072\n",
      "Iteration: 357; Percent complete: 17.8%; Average loss: 8.5022\n",
      "Iteration: 358; Percent complete: 17.9%; Average loss: 7.6472\n",
      "Iteration: 359; Percent complete: 17.9%; Average loss: 7.8841\n",
      "Iteration: 360; Percent complete: 18.0%; Average loss: 8.1311\n",
      "Iteration: 361; Percent complete: 18.1%; Average loss: 8.0599\n",
      "Iteration: 362; Percent complete: 18.1%; Average loss: 8.6433\n",
      "Iteration: 363; Percent complete: 18.1%; Average loss: 7.8000\n",
      "Iteration: 364; Percent complete: 18.2%; Average loss: 7.9585\n",
      "Iteration: 365; Percent complete: 18.2%; Average loss: 7.9788\n",
      "Iteration: 366; Percent complete: 18.3%; Average loss: 8.1554\n",
      "Iteration: 367; Percent complete: 18.4%; Average loss: 7.7905\n",
      "Iteration: 368; Percent complete: 18.4%; Average loss: 7.7682\n",
      "Iteration: 369; Percent complete: 18.4%; Average loss: 7.9944\n",
      "Iteration: 370; Percent complete: 18.5%; Average loss: 7.7633\n",
      "Iteration: 371; Percent complete: 18.6%; Average loss: 8.0526\n",
      "Iteration: 372; Percent complete: 18.6%; Average loss: 7.7630\n",
      "Iteration: 373; Percent complete: 18.6%; Average loss: 7.8509\n",
      "Iteration: 374; Percent complete: 18.7%; Average loss: 7.8631\n",
      "Iteration: 375; Percent complete: 18.8%; Average loss: 8.1578\n",
      "Iteration: 376; Percent complete: 18.8%; Average loss: 8.0067\n",
      "Iteration: 377; Percent complete: 18.9%; Average loss: 7.9525\n",
      "Iteration: 378; Percent complete: 18.9%; Average loss: 8.0653\n",
      "Iteration: 379; Percent complete: 18.9%; Average loss: 8.1667\n",
      "Iteration: 380; Percent complete: 19.0%; Average loss: 7.7775\n",
      "Iteration: 381; Percent complete: 19.1%; Average loss: 8.0747\n",
      "Iteration: 382; Percent complete: 19.1%; Average loss: 7.7321\n",
      "Iteration: 383; Percent complete: 19.1%; Average loss: 7.9973\n",
      "Iteration: 384; Percent complete: 19.2%; Average loss: 8.1724\n",
      "Iteration: 385; Percent complete: 19.2%; Average loss: 8.1128\n",
      "Iteration: 386; Percent complete: 19.3%; Average loss: 7.8065\n",
      "Iteration: 387; Percent complete: 19.4%; Average loss: 8.2445\n",
      "Iteration: 388; Percent complete: 19.4%; Average loss: 7.9841\n",
      "Iteration: 389; Percent complete: 19.4%; Average loss: 8.1655\n",
      "Iteration: 390; Percent complete: 19.5%; Average loss: 7.6833\n",
      "Iteration: 391; Percent complete: 19.6%; Average loss: 7.9820\n",
      "Iteration: 392; Percent complete: 19.6%; Average loss: 8.0615\n",
      "Iteration: 393; Percent complete: 19.7%; Average loss: 7.9997\n",
      "Iteration: 394; Percent complete: 19.7%; Average loss: 8.1576\n",
      "Iteration: 395; Percent complete: 19.8%; Average loss: 8.2444\n",
      "Iteration: 396; Percent complete: 19.8%; Average loss: 8.0688\n",
      "Iteration: 397; Percent complete: 19.9%; Average loss: 8.0139\n",
      "Iteration: 398; Percent complete: 19.9%; Average loss: 8.3512\n",
      "Iteration: 399; Percent complete: 20.0%; Average loss: 8.3073\n",
      "Iteration: 400; Percent complete: 20.0%; Average loss: 7.9300\n",
      "Iteration: 401; Percent complete: 20.1%; Average loss: 8.2359\n",
      "Iteration: 402; Percent complete: 20.1%; Average loss: 7.9754\n",
      "Iteration: 403; Percent complete: 20.2%; Average loss: 8.2277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 404; Percent complete: 20.2%; Average loss: 8.1529\n",
      "Iteration: 405; Percent complete: 20.2%; Average loss: 8.0124\n",
      "Iteration: 406; Percent complete: 20.3%; Average loss: 7.9228\n",
      "Iteration: 407; Percent complete: 20.3%; Average loss: 7.9408\n",
      "Iteration: 408; Percent complete: 20.4%; Average loss: 8.0029\n",
      "Iteration: 409; Percent complete: 20.4%; Average loss: 8.2612\n",
      "Iteration: 410; Percent complete: 20.5%; Average loss: 8.2578\n",
      "Iteration: 411; Percent complete: 20.5%; Average loss: 8.2507\n",
      "Iteration: 412; Percent complete: 20.6%; Average loss: 8.0011\n",
      "Iteration: 413; Percent complete: 20.6%; Average loss: 7.7814\n",
      "Iteration: 414; Percent complete: 20.7%; Average loss: 7.6113\n",
      "Iteration: 415; Percent complete: 20.8%; Average loss: 8.2508\n",
      "Iteration: 416; Percent complete: 20.8%; Average loss: 7.8600\n",
      "Iteration: 417; Percent complete: 20.8%; Average loss: 7.8429\n",
      "Iteration: 418; Percent complete: 20.9%; Average loss: 8.0169\n",
      "Iteration: 419; Percent complete: 20.9%; Average loss: 7.9609\n",
      "Iteration: 420; Percent complete: 21.0%; Average loss: 7.5720\n",
      "Iteration: 421; Percent complete: 21.1%; Average loss: 8.0436\n",
      "Iteration: 422; Percent complete: 21.1%; Average loss: 7.7624\n",
      "Iteration: 423; Percent complete: 21.1%; Average loss: 8.1252\n",
      "Iteration: 424; Percent complete: 21.2%; Average loss: 7.7536\n",
      "Iteration: 425; Percent complete: 21.2%; Average loss: 7.6991\n",
      "Iteration: 426; Percent complete: 21.3%; Average loss: 7.8330\n",
      "Iteration: 427; Percent complete: 21.3%; Average loss: 8.1978\n",
      "Iteration: 428; Percent complete: 21.4%; Average loss: 7.8445\n",
      "Iteration: 429; Percent complete: 21.4%; Average loss: 7.9167\n",
      "Iteration: 430; Percent complete: 21.5%; Average loss: 7.9260\n",
      "Iteration: 431; Percent complete: 21.6%; Average loss: 8.0386\n",
      "Iteration: 432; Percent complete: 21.6%; Average loss: 7.8438\n",
      "Iteration: 433; Percent complete: 21.6%; Average loss: 8.1421\n",
      "Iteration: 434; Percent complete: 21.7%; Average loss: 8.2440\n",
      "Iteration: 435; Percent complete: 21.8%; Average loss: 8.0576\n",
      "Iteration: 436; Percent complete: 21.8%; Average loss: 8.0740\n",
      "Iteration: 437; Percent complete: 21.9%; Average loss: 7.9181\n",
      "Iteration: 438; Percent complete: 21.9%; Average loss: 7.8434\n",
      "Iteration: 439; Percent complete: 21.9%; Average loss: 8.0654\n",
      "Iteration: 440; Percent complete: 22.0%; Average loss: 8.0115\n",
      "Iteration: 441; Percent complete: 22.1%; Average loss: 7.8339\n",
      "Iteration: 442; Percent complete: 22.1%; Average loss: 7.7273\n",
      "Iteration: 443; Percent complete: 22.1%; Average loss: 7.5556\n",
      "Iteration: 444; Percent complete: 22.2%; Average loss: 7.6978\n",
      "Iteration: 445; Percent complete: 22.2%; Average loss: 7.8711\n",
      "Iteration: 446; Percent complete: 22.3%; Average loss: 8.1537\n",
      "Iteration: 447; Percent complete: 22.4%; Average loss: 7.8497\n",
      "Iteration: 448; Percent complete: 22.4%; Average loss: 7.8091\n",
      "Iteration: 449; Percent complete: 22.4%; Average loss: 8.1262\n",
      "Iteration: 450; Percent complete: 22.5%; Average loss: 7.6411\n",
      "Iteration: 451; Percent complete: 22.6%; Average loss: 7.6129\n",
      "Iteration: 452; Percent complete: 22.6%; Average loss: 8.1518\n",
      "Iteration: 453; Percent complete: 22.7%; Average loss: 7.9455\n",
      "Iteration: 454; Percent complete: 22.7%; Average loss: 7.6578\n",
      "Iteration: 455; Percent complete: 22.8%; Average loss: 7.9450\n",
      "Iteration: 456; Percent complete: 22.8%; Average loss: 7.8307\n",
      "Iteration: 457; Percent complete: 22.9%; Average loss: 7.9471\n",
      "Iteration: 458; Percent complete: 22.9%; Average loss: 8.0100\n",
      "Iteration: 459; Percent complete: 22.9%; Average loss: 7.9965\n",
      "Iteration: 460; Percent complete: 23.0%; Average loss: 7.6446\n",
      "Iteration: 461; Percent complete: 23.1%; Average loss: 7.9711\n",
      "Iteration: 462; Percent complete: 23.1%; Average loss: 8.0057\n",
      "Iteration: 463; Percent complete: 23.2%; Average loss: 8.2068\n",
      "Iteration: 464; Percent complete: 23.2%; Average loss: 7.8453\n",
      "Iteration: 465; Percent complete: 23.2%; Average loss: 7.7531\n",
      "Iteration: 466; Percent complete: 23.3%; Average loss: 7.7486\n",
      "Iteration: 467; Percent complete: 23.4%; Average loss: 7.7142\n",
      "Iteration: 468; Percent complete: 23.4%; Average loss: 7.8022\n",
      "Iteration: 469; Percent complete: 23.4%; Average loss: 8.0501\n",
      "Iteration: 470; Percent complete: 23.5%; Average loss: 8.1081\n",
      "Iteration: 471; Percent complete: 23.5%; Average loss: 7.5185\n",
      "Iteration: 472; Percent complete: 23.6%; Average loss: 8.1667\n",
      "Iteration: 473; Percent complete: 23.6%; Average loss: 8.0709\n",
      "Iteration: 474; Percent complete: 23.7%; Average loss: 8.0240\n",
      "Iteration: 475; Percent complete: 23.8%; Average loss: 7.4480\n",
      "Iteration: 476; Percent complete: 23.8%; Average loss: 7.8254\n",
      "Iteration: 477; Percent complete: 23.8%; Average loss: 7.9582\n",
      "Iteration: 478; Percent complete: 23.9%; Average loss: 8.2299\n",
      "Iteration: 479; Percent complete: 23.9%; Average loss: 8.0965\n",
      "Iteration: 480; Percent complete: 24.0%; Average loss: 7.9632\n",
      "Iteration: 481; Percent complete: 24.1%; Average loss: 7.5976\n",
      "Iteration: 482; Percent complete: 24.1%; Average loss: 7.8159\n",
      "Iteration: 483; Percent complete: 24.1%; Average loss: 7.7968\n",
      "Iteration: 484; Percent complete: 24.2%; Average loss: 7.7808\n",
      "Iteration: 485; Percent complete: 24.2%; Average loss: 7.7749\n",
      "Iteration: 486; Percent complete: 24.3%; Average loss: 7.8819\n",
      "Iteration: 487; Percent complete: 24.3%; Average loss: 7.8721\n",
      "Iteration: 488; Percent complete: 24.4%; Average loss: 7.8258\n",
      "Iteration: 489; Percent complete: 24.4%; Average loss: 7.9094\n",
      "Iteration: 490; Percent complete: 24.5%; Average loss: 8.0132\n",
      "Iteration: 491; Percent complete: 24.6%; Average loss: 7.9877\n",
      "Iteration: 492; Percent complete: 24.6%; Average loss: 7.4297\n",
      "Iteration: 493; Percent complete: 24.6%; Average loss: 8.0657\n",
      "Iteration: 494; Percent complete: 24.7%; Average loss: 7.8356\n",
      "Iteration: 495; Percent complete: 24.8%; Average loss: 8.1418\n",
      "Iteration: 496; Percent complete: 24.8%; Average loss: 7.8232\n",
      "Iteration: 497; Percent complete: 24.9%; Average loss: 7.5990\n",
      "Iteration: 498; Percent complete: 24.9%; Average loss: 7.8007\n",
      "Iteration: 499; Percent complete: 24.9%; Average loss: 7.7336\n",
      "Iteration: 500; Percent complete: 25.0%; Average loss: 7.9175\n",
      "Iteration: 501; Percent complete: 25.1%; Average loss: 8.1282\n",
      "Iteration: 502; Percent complete: 25.1%; Average loss: 8.1373\n",
      "Iteration: 503; Percent complete: 25.1%; Average loss: 7.7971\n",
      "Iteration: 504; Percent complete: 25.2%; Average loss: 8.0862\n",
      "Iteration: 505; Percent complete: 25.2%; Average loss: 7.8314\n",
      "Iteration: 506; Percent complete: 25.3%; Average loss: 7.8106\n",
      "Iteration: 507; Percent complete: 25.4%; Average loss: 8.1083\n",
      "Iteration: 508; Percent complete: 25.4%; Average loss: 7.9203\n",
      "Iteration: 509; Percent complete: 25.4%; Average loss: 7.9227\n",
      "Iteration: 510; Percent complete: 25.5%; Average loss: 7.9393\n",
      "Iteration: 511; Percent complete: 25.6%; Average loss: 7.8778\n",
      "Iteration: 512; Percent complete: 25.6%; Average loss: 7.6391\n",
      "Iteration: 513; Percent complete: 25.7%; Average loss: 7.9140\n",
      "Iteration: 514; Percent complete: 25.7%; Average loss: 7.8580\n",
      "Iteration: 515; Percent complete: 25.8%; Average loss: 7.8149\n",
      "Iteration: 516; Percent complete: 25.8%; Average loss: 8.2616\n",
      "Iteration: 517; Percent complete: 25.9%; Average loss: 7.9158\n",
      "Iteration: 518; Percent complete: 25.9%; Average loss: 8.0168\n",
      "Iteration: 519; Percent complete: 25.9%; Average loss: 7.7190\n",
      "Iteration: 520; Percent complete: 26.0%; Average loss: 7.7117\n",
      "Iteration: 521; Percent complete: 26.1%; Average loss: 8.0273\n",
      "Iteration: 522; Percent complete: 26.1%; Average loss: 7.6817\n",
      "Iteration: 523; Percent complete: 26.2%; Average loss: 7.8125\n",
      "Iteration: 524; Percent complete: 26.2%; Average loss: 7.6936\n",
      "Iteration: 525; Percent complete: 26.2%; Average loss: 7.8385\n",
      "Iteration: 526; Percent complete: 26.3%; Average loss: 8.0084\n",
      "Iteration: 527; Percent complete: 26.4%; Average loss: 7.8115\n",
      "Iteration: 528; Percent complete: 26.4%; Average loss: 7.7575\n",
      "Iteration: 529; Percent complete: 26.5%; Average loss: 7.9359\n",
      "Iteration: 530; Percent complete: 26.5%; Average loss: 7.3465\n",
      "Iteration: 531; Percent complete: 26.6%; Average loss: 8.1648\n",
      "Iteration: 532; Percent complete: 26.6%; Average loss: 8.0024\n",
      "Iteration: 533; Percent complete: 26.7%; Average loss: 8.1132\n",
      "Iteration: 534; Percent complete: 26.7%; Average loss: 8.0732\n",
      "Iteration: 535; Percent complete: 26.8%; Average loss: 7.6911\n",
      "Iteration: 536; Percent complete: 26.8%; Average loss: 7.8934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 537; Percent complete: 26.9%; Average loss: 7.9581\n",
      "Iteration: 538; Percent complete: 26.9%; Average loss: 7.5602\n",
      "Iteration: 539; Percent complete: 27.0%; Average loss: 7.5699\n",
      "Iteration: 540; Percent complete: 27.0%; Average loss: 7.8519\n",
      "Iteration: 541; Percent complete: 27.1%; Average loss: 7.9235\n",
      "Iteration: 542; Percent complete: 27.1%; Average loss: 8.0803\n",
      "Iteration: 543; Percent complete: 27.2%; Average loss: 7.8242\n",
      "Iteration: 544; Percent complete: 27.2%; Average loss: 7.8344\n",
      "Iteration: 545; Percent complete: 27.3%; Average loss: 7.9689\n",
      "Iteration: 546; Percent complete: 27.3%; Average loss: 7.5291\n",
      "Iteration: 547; Percent complete: 27.4%; Average loss: 7.7255\n",
      "Iteration: 548; Percent complete: 27.4%; Average loss: 7.8542\n",
      "Iteration: 549; Percent complete: 27.5%; Average loss: 7.6871\n",
      "Iteration: 550; Percent complete: 27.5%; Average loss: 7.9876\n",
      "Iteration: 551; Percent complete: 27.6%; Average loss: 7.8566\n",
      "Iteration: 552; Percent complete: 27.6%; Average loss: 7.7508\n",
      "Iteration: 553; Percent complete: 27.7%; Average loss: 8.0364\n",
      "Iteration: 554; Percent complete: 27.7%; Average loss: 7.5720\n",
      "Iteration: 555; Percent complete: 27.8%; Average loss: 8.0004\n",
      "Iteration: 556; Percent complete: 27.8%; Average loss: 7.6882\n",
      "Iteration: 557; Percent complete: 27.9%; Average loss: 7.9173\n",
      "Iteration: 558; Percent complete: 27.9%; Average loss: 7.4682\n",
      "Iteration: 559; Percent complete: 28.0%; Average loss: 7.4644\n",
      "Iteration: 560; Percent complete: 28.0%; Average loss: 7.7463\n",
      "Iteration: 561; Percent complete: 28.1%; Average loss: 7.8103\n",
      "Iteration: 562; Percent complete: 28.1%; Average loss: 7.8822\n",
      "Iteration: 563; Percent complete: 28.1%; Average loss: 7.4506\n",
      "Iteration: 564; Percent complete: 28.2%; Average loss: 7.7740\n",
      "Iteration: 565; Percent complete: 28.2%; Average loss: 7.6242\n",
      "Iteration: 566; Percent complete: 28.3%; Average loss: 7.5652\n",
      "Iteration: 567; Percent complete: 28.3%; Average loss: 7.6616\n",
      "Iteration: 568; Percent complete: 28.4%; Average loss: 7.3935\n",
      "Iteration: 569; Percent complete: 28.4%; Average loss: 7.8937\n",
      "Iteration: 570; Percent complete: 28.5%; Average loss: 8.0067\n",
      "Iteration: 571; Percent complete: 28.5%; Average loss: 7.8009\n",
      "Iteration: 572; Percent complete: 28.6%; Average loss: 7.4152\n",
      "Iteration: 573; Percent complete: 28.6%; Average loss: 8.0650\n",
      "Iteration: 574; Percent complete: 28.7%; Average loss: 7.7512\n",
      "Iteration: 575; Percent complete: 28.7%; Average loss: 7.8370\n",
      "Iteration: 576; Percent complete: 28.8%; Average loss: 7.6142\n",
      "Iteration: 577; Percent complete: 28.8%; Average loss: 7.9164\n",
      "Iteration: 578; Percent complete: 28.9%; Average loss: 7.8345\n",
      "Iteration: 579; Percent complete: 28.9%; Average loss: 7.6386\n",
      "Iteration: 580; Percent complete: 29.0%; Average loss: 7.5831\n",
      "Iteration: 581; Percent complete: 29.0%; Average loss: 7.6772\n",
      "Iteration: 582; Percent complete: 29.1%; Average loss: 7.9662\n",
      "Iteration: 583; Percent complete: 29.1%; Average loss: 7.3979\n",
      "Iteration: 584; Percent complete: 29.2%; Average loss: 7.9745\n",
      "Iteration: 585; Percent complete: 29.2%; Average loss: 8.0805\n",
      "Iteration: 586; Percent complete: 29.3%; Average loss: 7.8069\n",
      "Iteration: 587; Percent complete: 29.3%; Average loss: 7.3851\n",
      "Iteration: 588; Percent complete: 29.4%; Average loss: 7.3429\n",
      "Iteration: 589; Percent complete: 29.4%; Average loss: 7.7307\n",
      "Iteration: 590; Percent complete: 29.5%; Average loss: 7.7167\n",
      "Iteration: 591; Percent complete: 29.5%; Average loss: 7.5778\n",
      "Iteration: 592; Percent complete: 29.6%; Average loss: 7.4368\n",
      "Iteration: 593; Percent complete: 29.6%; Average loss: 8.0533\n",
      "Iteration: 594; Percent complete: 29.7%; Average loss: 7.8019\n",
      "Iteration: 595; Percent complete: 29.8%; Average loss: 7.8947\n",
      "Iteration: 596; Percent complete: 29.8%; Average loss: 8.0058\n",
      "Iteration: 597; Percent complete: 29.8%; Average loss: 7.7165\n",
      "Iteration: 598; Percent complete: 29.9%; Average loss: 7.7686\n",
      "Iteration: 599; Percent complete: 29.9%; Average loss: 7.8779\n",
      "Iteration: 600; Percent complete: 30.0%; Average loss: 7.7335\n",
      "Iteration: 601; Percent complete: 30.0%; Average loss: 7.6643\n",
      "Iteration: 602; Percent complete: 30.1%; Average loss: 7.6827\n",
      "Iteration: 603; Percent complete: 30.1%; Average loss: 7.8070\n",
      "Iteration: 604; Percent complete: 30.2%; Average loss: 7.7161\n",
      "Iteration: 605; Percent complete: 30.2%; Average loss: 7.5877\n",
      "Iteration: 606; Percent complete: 30.3%; Average loss: 7.8519\n",
      "Iteration: 607; Percent complete: 30.3%; Average loss: 7.8224\n",
      "Iteration: 608; Percent complete: 30.4%; Average loss: 7.5614\n",
      "Iteration: 609; Percent complete: 30.4%; Average loss: 7.8808\n",
      "Iteration: 610; Percent complete: 30.5%; Average loss: 7.5620\n",
      "Iteration: 611; Percent complete: 30.6%; Average loss: 7.6066\n",
      "Iteration: 612; Percent complete: 30.6%; Average loss: 7.4300\n",
      "Iteration: 613; Percent complete: 30.6%; Average loss: 7.6545\n",
      "Iteration: 614; Percent complete: 30.7%; Average loss: 7.6690\n",
      "Iteration: 615; Percent complete: 30.8%; Average loss: 7.6047\n",
      "Iteration: 616; Percent complete: 30.8%; Average loss: 7.8449\n",
      "Iteration: 617; Percent complete: 30.9%; Average loss: 7.5159\n",
      "Iteration: 618; Percent complete: 30.9%; Average loss: 7.5371\n",
      "Iteration: 619; Percent complete: 30.9%; Average loss: 7.8551\n",
      "Iteration: 620; Percent complete: 31.0%; Average loss: 7.7313\n",
      "Iteration: 621; Percent complete: 31.1%; Average loss: 7.5927\n",
      "Iteration: 622; Percent complete: 31.1%; Average loss: 7.6430\n",
      "Iteration: 623; Percent complete: 31.1%; Average loss: 7.7658\n",
      "Iteration: 624; Percent complete: 31.2%; Average loss: 7.9388\n",
      "Iteration: 625; Percent complete: 31.2%; Average loss: 7.4387\n",
      "Iteration: 626; Percent complete: 31.3%; Average loss: 7.6609\n",
      "Iteration: 627; Percent complete: 31.4%; Average loss: 7.9075\n",
      "Iteration: 628; Percent complete: 31.4%; Average loss: 7.8215\n",
      "Iteration: 629; Percent complete: 31.4%; Average loss: 7.6615\n",
      "Iteration: 630; Percent complete: 31.5%; Average loss: 7.8955\n",
      "Iteration: 631; Percent complete: 31.6%; Average loss: 7.3717\n",
      "Iteration: 632; Percent complete: 31.6%; Average loss: 7.8256\n",
      "Iteration: 633; Percent complete: 31.6%; Average loss: 7.7178\n",
      "Iteration: 634; Percent complete: 31.7%; Average loss: 8.0308\n",
      "Iteration: 635; Percent complete: 31.8%; Average loss: 7.5959\n",
      "Iteration: 636; Percent complete: 31.8%; Average loss: 7.6275\n",
      "Iteration: 637; Percent complete: 31.9%; Average loss: 7.6914\n",
      "Iteration: 638; Percent complete: 31.9%; Average loss: 7.6997\n",
      "Iteration: 639; Percent complete: 31.9%; Average loss: 7.5918\n",
      "Iteration: 640; Percent complete: 32.0%; Average loss: 7.7191\n",
      "Iteration: 641; Percent complete: 32.0%; Average loss: 7.6153\n",
      "Iteration: 642; Percent complete: 32.1%; Average loss: 7.7439\n",
      "Iteration: 643; Percent complete: 32.1%; Average loss: 7.4855\n",
      "Iteration: 644; Percent complete: 32.2%; Average loss: 7.6968\n",
      "Iteration: 645; Percent complete: 32.2%; Average loss: 7.6319\n",
      "Iteration: 646; Percent complete: 32.3%; Average loss: 7.6534\n",
      "Iteration: 647; Percent complete: 32.4%; Average loss: 7.7560\n",
      "Iteration: 648; Percent complete: 32.4%; Average loss: 7.5982\n",
      "Iteration: 649; Percent complete: 32.5%; Average loss: 7.7147\n",
      "Iteration: 650; Percent complete: 32.5%; Average loss: 7.8226\n",
      "Iteration: 651; Percent complete: 32.6%; Average loss: 7.8954\n",
      "Iteration: 652; Percent complete: 32.6%; Average loss: 8.2233\n",
      "Iteration: 653; Percent complete: 32.6%; Average loss: 7.8003\n",
      "Iteration: 654; Percent complete: 32.7%; Average loss: 7.8497\n",
      "Iteration: 655; Percent complete: 32.8%; Average loss: 7.7834\n",
      "Iteration: 656; Percent complete: 32.8%; Average loss: 7.8087\n",
      "Iteration: 657; Percent complete: 32.9%; Average loss: 7.5505\n",
      "Iteration: 658; Percent complete: 32.9%; Average loss: 7.5659\n",
      "Iteration: 659; Percent complete: 33.0%; Average loss: 7.6750\n",
      "Iteration: 660; Percent complete: 33.0%; Average loss: 7.9040\n",
      "Iteration: 661; Percent complete: 33.1%; Average loss: 7.5396\n",
      "Iteration: 662; Percent complete: 33.1%; Average loss: 7.5196\n",
      "Iteration: 663; Percent complete: 33.1%; Average loss: 8.0179\n",
      "Iteration: 664; Percent complete: 33.2%; Average loss: 7.6020\n",
      "Iteration: 665; Percent complete: 33.2%; Average loss: 7.4976\n",
      "Iteration: 666; Percent complete: 33.3%; Average loss: 7.7188\n",
      "Iteration: 667; Percent complete: 33.4%; Average loss: 8.0352\n",
      "Iteration: 668; Percent complete: 33.4%; Average loss: 7.4381\n",
      "Iteration: 669; Percent complete: 33.5%; Average loss: 7.5275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 670; Percent complete: 33.5%; Average loss: 7.6413\n",
      "Iteration: 671; Percent complete: 33.6%; Average loss: 7.4873\n",
      "Iteration: 672; Percent complete: 33.6%; Average loss: 7.4948\n",
      "Iteration: 673; Percent complete: 33.7%; Average loss: 7.4526\n",
      "Iteration: 674; Percent complete: 33.7%; Average loss: 7.3345\n",
      "Iteration: 675; Percent complete: 33.8%; Average loss: 7.6978\n",
      "Iteration: 676; Percent complete: 33.8%; Average loss: 7.9946\n",
      "Iteration: 677; Percent complete: 33.9%; Average loss: 7.7594\n",
      "Iteration: 678; Percent complete: 33.9%; Average loss: 7.6194\n",
      "Iteration: 679; Percent complete: 34.0%; Average loss: 8.0458\n",
      "Iteration: 680; Percent complete: 34.0%; Average loss: 7.1365\n",
      "Iteration: 681; Percent complete: 34.1%; Average loss: 7.6830\n",
      "Iteration: 682; Percent complete: 34.1%; Average loss: 7.7503\n",
      "Iteration: 683; Percent complete: 34.2%; Average loss: 7.5517\n",
      "Iteration: 684; Percent complete: 34.2%; Average loss: 7.5825\n",
      "Iteration: 685; Percent complete: 34.2%; Average loss: 7.7233\n",
      "Iteration: 686; Percent complete: 34.3%; Average loss: 7.5323\n",
      "Iteration: 687; Percent complete: 34.4%; Average loss: 7.8921\n",
      "Iteration: 688; Percent complete: 34.4%; Average loss: 7.6797\n",
      "Iteration: 689; Percent complete: 34.4%; Average loss: 7.7453\n",
      "Iteration: 690; Percent complete: 34.5%; Average loss: 7.7952\n",
      "Iteration: 691; Percent complete: 34.5%; Average loss: 7.3926\n",
      "Iteration: 692; Percent complete: 34.6%; Average loss: 7.5909\n",
      "Iteration: 693; Percent complete: 34.6%; Average loss: 7.6946\n",
      "Iteration: 694; Percent complete: 34.7%; Average loss: 7.5342\n",
      "Iteration: 695; Percent complete: 34.8%; Average loss: 7.7314\n",
      "Iteration: 696; Percent complete: 34.8%; Average loss: 7.7245\n",
      "Iteration: 697; Percent complete: 34.8%; Average loss: 7.7708\n",
      "Iteration: 698; Percent complete: 34.9%; Average loss: 7.5750\n",
      "Iteration: 699; Percent complete: 34.9%; Average loss: 7.7133\n",
      "Iteration: 700; Percent complete: 35.0%; Average loss: 7.5465\n",
      "Iteration: 701; Percent complete: 35.0%; Average loss: 8.1421\n",
      "Iteration: 702; Percent complete: 35.1%; Average loss: 7.3364\n",
      "Iteration: 703; Percent complete: 35.1%; Average loss: 7.2930\n",
      "Iteration: 704; Percent complete: 35.2%; Average loss: 7.7416\n",
      "Iteration: 705; Percent complete: 35.2%; Average loss: 7.6116\n",
      "Iteration: 706; Percent complete: 35.3%; Average loss: 7.6275\n",
      "Iteration: 707; Percent complete: 35.4%; Average loss: 7.5210\n",
      "Iteration: 708; Percent complete: 35.4%; Average loss: 7.4137\n",
      "Iteration: 709; Percent complete: 35.4%; Average loss: 7.7794\n",
      "Iteration: 710; Percent complete: 35.5%; Average loss: 7.5596\n",
      "Iteration: 711; Percent complete: 35.5%; Average loss: 7.8162\n",
      "Iteration: 712; Percent complete: 35.6%; Average loss: 7.4606\n",
      "Iteration: 713; Percent complete: 35.6%; Average loss: 7.6300\n",
      "Iteration: 714; Percent complete: 35.7%; Average loss: 7.5039\n",
      "Iteration: 715; Percent complete: 35.8%; Average loss: 7.5932\n",
      "Iteration: 716; Percent complete: 35.8%; Average loss: 7.4540\n",
      "Iteration: 717; Percent complete: 35.9%; Average loss: 7.9231\n",
      "Iteration: 718; Percent complete: 35.9%; Average loss: 7.4095\n",
      "Iteration: 719; Percent complete: 35.9%; Average loss: 7.3494\n",
      "Iteration: 720; Percent complete: 36.0%; Average loss: 7.6422\n",
      "Iteration: 721; Percent complete: 36.0%; Average loss: 7.6275\n",
      "Iteration: 722; Percent complete: 36.1%; Average loss: 7.3280\n",
      "Iteration: 723; Percent complete: 36.1%; Average loss: 7.4006\n",
      "Iteration: 724; Percent complete: 36.2%; Average loss: 7.7548\n",
      "Iteration: 725; Percent complete: 36.2%; Average loss: 7.4932\n",
      "Iteration: 726; Percent complete: 36.3%; Average loss: 7.2503\n",
      "Iteration: 727; Percent complete: 36.4%; Average loss: 7.4507\n",
      "Iteration: 728; Percent complete: 36.4%; Average loss: 7.5895\n",
      "Iteration: 729; Percent complete: 36.4%; Average loss: 7.6459\n",
      "Iteration: 730; Percent complete: 36.5%; Average loss: 7.4802\n",
      "Iteration: 731; Percent complete: 36.5%; Average loss: 7.4920\n",
      "Iteration: 732; Percent complete: 36.6%; Average loss: 7.6489\n",
      "Iteration: 733; Percent complete: 36.6%; Average loss: 7.3610\n",
      "Iteration: 734; Percent complete: 36.7%; Average loss: 7.5700\n",
      "Iteration: 735; Percent complete: 36.8%; Average loss: 7.6763\n",
      "Iteration: 736; Percent complete: 36.8%; Average loss: 7.8556\n",
      "Iteration: 737; Percent complete: 36.9%; Average loss: 7.6221\n",
      "Iteration: 738; Percent complete: 36.9%; Average loss: 7.6702\n",
      "Iteration: 739; Percent complete: 37.0%; Average loss: 7.4194\n",
      "Iteration: 740; Percent complete: 37.0%; Average loss: 7.4334\n",
      "Iteration: 741; Percent complete: 37.0%; Average loss: 7.1979\n",
      "Iteration: 742; Percent complete: 37.1%; Average loss: 7.4312\n",
      "Iteration: 743; Percent complete: 37.1%; Average loss: 7.4626\n",
      "Iteration: 744; Percent complete: 37.2%; Average loss: 7.5788\n",
      "Iteration: 745; Percent complete: 37.2%; Average loss: 7.5238\n",
      "Iteration: 746; Percent complete: 37.3%; Average loss: 7.6586\n",
      "Iteration: 747; Percent complete: 37.4%; Average loss: 7.4719\n",
      "Iteration: 748; Percent complete: 37.4%; Average loss: 7.4031\n",
      "Iteration: 749; Percent complete: 37.5%; Average loss: 7.5203\n",
      "Iteration: 750; Percent complete: 37.5%; Average loss: 7.4759\n",
      "Iteration: 751; Percent complete: 37.5%; Average loss: 7.2205\n",
      "Iteration: 752; Percent complete: 37.6%; Average loss: 7.7048\n",
      "Iteration: 753; Percent complete: 37.6%; Average loss: 7.8933\n",
      "Iteration: 754; Percent complete: 37.7%; Average loss: 7.5635\n",
      "Iteration: 755; Percent complete: 37.8%; Average loss: 7.3597\n",
      "Iteration: 756; Percent complete: 37.8%; Average loss: 7.2005\n",
      "Iteration: 757; Percent complete: 37.9%; Average loss: 7.4311\n",
      "Iteration: 758; Percent complete: 37.9%; Average loss: 7.0066\n",
      "Iteration: 759; Percent complete: 38.0%; Average loss: 7.6460\n",
      "Iteration: 760; Percent complete: 38.0%; Average loss: 7.7846\n",
      "Iteration: 761; Percent complete: 38.0%; Average loss: 7.7871\n",
      "Iteration: 762; Percent complete: 38.1%; Average loss: 7.4079\n",
      "Iteration: 763; Percent complete: 38.1%; Average loss: 7.5498\n",
      "Iteration: 764; Percent complete: 38.2%; Average loss: 7.3091\n",
      "Iteration: 765; Percent complete: 38.2%; Average loss: 7.7204\n",
      "Iteration: 766; Percent complete: 38.3%; Average loss: 7.6677\n",
      "Iteration: 767; Percent complete: 38.4%; Average loss: 7.8015\n",
      "Iteration: 768; Percent complete: 38.4%; Average loss: 7.2119\n",
      "Iteration: 769; Percent complete: 38.5%; Average loss: 7.6236\n",
      "Iteration: 770; Percent complete: 38.5%; Average loss: 7.3910\n",
      "Iteration: 771; Percent complete: 38.6%; Average loss: 7.5097\n",
      "Iteration: 772; Percent complete: 38.6%; Average loss: 7.2697\n",
      "Iteration: 773; Percent complete: 38.6%; Average loss: 7.1322\n",
      "Iteration: 774; Percent complete: 38.7%; Average loss: 7.5061\n",
      "Iteration: 775; Percent complete: 38.8%; Average loss: 7.5746\n",
      "Iteration: 776; Percent complete: 38.8%; Average loss: 7.6202\n",
      "Iteration: 777; Percent complete: 38.9%; Average loss: 7.9109\n",
      "Iteration: 778; Percent complete: 38.9%; Average loss: 7.2218\n",
      "Iteration: 779; Percent complete: 39.0%; Average loss: 7.1641\n",
      "Iteration: 780; Percent complete: 39.0%; Average loss: 7.5401\n",
      "Iteration: 781; Percent complete: 39.1%; Average loss: 7.3976\n",
      "Iteration: 782; Percent complete: 39.1%; Average loss: 7.3791\n",
      "Iteration: 783; Percent complete: 39.1%; Average loss: 7.4013\n",
      "Iteration: 784; Percent complete: 39.2%; Average loss: 7.6523\n",
      "Iteration: 785; Percent complete: 39.2%; Average loss: 7.4479\n",
      "Iteration: 786; Percent complete: 39.3%; Average loss: 7.3381\n",
      "Iteration: 787; Percent complete: 39.4%; Average loss: 7.3665\n",
      "Iteration: 788; Percent complete: 39.4%; Average loss: 7.3288\n",
      "Iteration: 789; Percent complete: 39.5%; Average loss: 7.7017\n",
      "Iteration: 790; Percent complete: 39.5%; Average loss: 7.4343\n",
      "Iteration: 791; Percent complete: 39.6%; Average loss: 7.5107\n",
      "Iteration: 792; Percent complete: 39.6%; Average loss: 7.3529\n",
      "Iteration: 793; Percent complete: 39.6%; Average loss: 7.6672\n",
      "Iteration: 794; Percent complete: 39.7%; Average loss: 7.3850\n",
      "Iteration: 795; Percent complete: 39.8%; Average loss: 7.6319\n",
      "Iteration: 796; Percent complete: 39.8%; Average loss: 7.6571\n",
      "Iteration: 797; Percent complete: 39.9%; Average loss: 7.3510\n",
      "Iteration: 798; Percent complete: 39.9%; Average loss: 7.4524\n",
      "Iteration: 799; Percent complete: 40.0%; Average loss: 7.4441\n",
      "Iteration: 800; Percent complete: 40.0%; Average loss: 7.3723\n",
      "Iteration: 801; Percent complete: 40.1%; Average loss: 7.4477\n",
      "Iteration: 802; Percent complete: 40.1%; Average loss: 7.6332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 803; Percent complete: 40.2%; Average loss: 7.5879\n",
      "Iteration: 804; Percent complete: 40.2%; Average loss: 7.5102\n",
      "Iteration: 805; Percent complete: 40.2%; Average loss: 7.7704\n",
      "Iteration: 806; Percent complete: 40.3%; Average loss: 7.6139\n",
      "Iteration: 807; Percent complete: 40.4%; Average loss: 7.3771\n",
      "Iteration: 808; Percent complete: 40.4%; Average loss: 7.1276\n",
      "Iteration: 809; Percent complete: 40.5%; Average loss: 7.4459\n",
      "Iteration: 810; Percent complete: 40.5%; Average loss: 7.2704\n",
      "Iteration: 811; Percent complete: 40.6%; Average loss: 7.3420\n",
      "Iteration: 812; Percent complete: 40.6%; Average loss: 7.4342\n",
      "Iteration: 813; Percent complete: 40.6%; Average loss: 7.4246\n",
      "Iteration: 814; Percent complete: 40.7%; Average loss: 7.4742\n",
      "Iteration: 815; Percent complete: 40.8%; Average loss: 7.2645\n",
      "Iteration: 816; Percent complete: 40.8%; Average loss: 7.2739\n",
      "Iteration: 817; Percent complete: 40.8%; Average loss: 7.5245\n",
      "Iteration: 818; Percent complete: 40.9%; Average loss: 7.5134\n",
      "Iteration: 819; Percent complete: 40.9%; Average loss: 7.3540\n",
      "Iteration: 820; Percent complete: 41.0%; Average loss: 7.3422\n",
      "Iteration: 821; Percent complete: 41.0%; Average loss: 7.5234\n",
      "Iteration: 822; Percent complete: 41.1%; Average loss: 7.2067\n",
      "Iteration: 823; Percent complete: 41.1%; Average loss: 7.2870\n",
      "Iteration: 824; Percent complete: 41.2%; Average loss: 7.3887\n",
      "Iteration: 825; Percent complete: 41.2%; Average loss: 7.3140\n",
      "Iteration: 826; Percent complete: 41.3%; Average loss: 7.6880\n",
      "Iteration: 827; Percent complete: 41.3%; Average loss: 7.3720\n",
      "Iteration: 828; Percent complete: 41.4%; Average loss: 7.3414\n",
      "Iteration: 829; Percent complete: 41.4%; Average loss: 7.2082\n",
      "Iteration: 830; Percent complete: 41.5%; Average loss: 7.3101\n",
      "Iteration: 831; Percent complete: 41.5%; Average loss: 7.4551\n",
      "Iteration: 832; Percent complete: 41.6%; Average loss: 7.4565\n",
      "Iteration: 833; Percent complete: 41.6%; Average loss: 7.2203\n",
      "Iteration: 834; Percent complete: 41.7%; Average loss: 7.6880\n",
      "Iteration: 835; Percent complete: 41.8%; Average loss: 7.6652\n",
      "Iteration: 836; Percent complete: 41.8%; Average loss: 7.4212\n",
      "Iteration: 837; Percent complete: 41.9%; Average loss: 7.2814\n",
      "Iteration: 838; Percent complete: 41.9%; Average loss: 7.2950\n",
      "Iteration: 839; Percent complete: 41.9%; Average loss: 7.4454\n",
      "Iteration: 840; Percent complete: 42.0%; Average loss: 7.6756\n",
      "Iteration: 841; Percent complete: 42.0%; Average loss: 7.4697\n",
      "Iteration: 842; Percent complete: 42.1%; Average loss: 7.3530\n",
      "Iteration: 843; Percent complete: 42.1%; Average loss: 7.3272\n",
      "Iteration: 844; Percent complete: 42.2%; Average loss: 7.5951\n",
      "Iteration: 845; Percent complete: 42.2%; Average loss: 7.1582\n",
      "Iteration: 846; Percent complete: 42.3%; Average loss: 7.7397\n",
      "Iteration: 847; Percent complete: 42.4%; Average loss: 7.3862\n",
      "Iteration: 848; Percent complete: 42.4%; Average loss: 7.3306\n",
      "Iteration: 849; Percent complete: 42.4%; Average loss: 7.2273\n",
      "Iteration: 850; Percent complete: 42.5%; Average loss: 7.7437\n",
      "Iteration: 851; Percent complete: 42.5%; Average loss: 7.0890\n",
      "Iteration: 852; Percent complete: 42.6%; Average loss: 7.4711\n",
      "Iteration: 853; Percent complete: 42.6%; Average loss: 7.6844\n",
      "Iteration: 854; Percent complete: 42.7%; Average loss: 7.4089\n",
      "Iteration: 855; Percent complete: 42.8%; Average loss: 7.5626\n",
      "Iteration: 856; Percent complete: 42.8%; Average loss: 7.2746\n",
      "Iteration: 857; Percent complete: 42.9%; Average loss: 7.3377\n",
      "Iteration: 858; Percent complete: 42.9%; Average loss: 7.2247\n",
      "Iteration: 859; Percent complete: 43.0%; Average loss: 7.1750\n",
      "Iteration: 860; Percent complete: 43.0%; Average loss: 7.4120\n",
      "Iteration: 861; Percent complete: 43.0%; Average loss: 7.4317\n",
      "Iteration: 862; Percent complete: 43.1%; Average loss: 7.1232\n",
      "Iteration: 863; Percent complete: 43.1%; Average loss: 7.4039\n",
      "Iteration: 864; Percent complete: 43.2%; Average loss: 7.4022\n",
      "Iteration: 865; Percent complete: 43.2%; Average loss: 7.1364\n",
      "Iteration: 866; Percent complete: 43.3%; Average loss: 7.4581\n",
      "Iteration: 867; Percent complete: 43.4%; Average loss: 7.2799\n",
      "Iteration: 868; Percent complete: 43.4%; Average loss: 7.1338\n",
      "Iteration: 869; Percent complete: 43.5%; Average loss: 7.2444\n",
      "Iteration: 870; Percent complete: 43.5%; Average loss: 7.2376\n",
      "Iteration: 871; Percent complete: 43.5%; Average loss: 7.5203\n",
      "Iteration: 872; Percent complete: 43.6%; Average loss: 7.4595\n",
      "Iteration: 873; Percent complete: 43.6%; Average loss: 7.0345\n",
      "Iteration: 874; Percent complete: 43.7%; Average loss: 6.7584\n",
      "Iteration: 875; Percent complete: 43.8%; Average loss: 7.6433\n",
      "Iteration: 876; Percent complete: 43.8%; Average loss: 7.4199\n",
      "Iteration: 877; Percent complete: 43.9%; Average loss: 7.1332\n",
      "Iteration: 878; Percent complete: 43.9%; Average loss: 7.6508\n",
      "Iteration: 879; Percent complete: 44.0%; Average loss: 7.2685\n",
      "Iteration: 880; Percent complete: 44.0%; Average loss: 6.9213\n",
      "Iteration: 881; Percent complete: 44.0%; Average loss: 7.1115\n",
      "Iteration: 882; Percent complete: 44.1%; Average loss: 7.1659\n",
      "Iteration: 883; Percent complete: 44.1%; Average loss: 7.3657\n",
      "Iteration: 884; Percent complete: 44.2%; Average loss: 7.0151\n",
      "Iteration: 885; Percent complete: 44.2%; Average loss: 7.2574\n",
      "Iteration: 886; Percent complete: 44.3%; Average loss: 7.2903\n",
      "Iteration: 887; Percent complete: 44.4%; Average loss: 7.4282\n",
      "Iteration: 888; Percent complete: 44.4%; Average loss: 7.2511\n",
      "Iteration: 889; Percent complete: 44.5%; Average loss: 7.6505\n",
      "Iteration: 890; Percent complete: 44.5%; Average loss: 7.2745\n",
      "Iteration: 891; Percent complete: 44.5%; Average loss: 7.0356\n",
      "Iteration: 892; Percent complete: 44.6%; Average loss: 7.3885\n",
      "Iteration: 893; Percent complete: 44.6%; Average loss: 7.1138\n",
      "Iteration: 894; Percent complete: 44.7%; Average loss: 7.9755\n",
      "Iteration: 895; Percent complete: 44.8%; Average loss: 7.3162\n",
      "Iteration: 896; Percent complete: 44.8%; Average loss: 7.1709\n",
      "Iteration: 897; Percent complete: 44.9%; Average loss: 7.5073\n",
      "Iteration: 898; Percent complete: 44.9%; Average loss: 6.8915\n",
      "Iteration: 899; Percent complete: 45.0%; Average loss: 7.3374\n",
      "Iteration: 900; Percent complete: 45.0%; Average loss: 6.8814\n",
      "Iteration: 901; Percent complete: 45.1%; Average loss: 7.1918\n",
      "Iteration: 902; Percent complete: 45.1%; Average loss: 7.3590\n",
      "Iteration: 903; Percent complete: 45.1%; Average loss: 6.9438\n",
      "Iteration: 904; Percent complete: 45.2%; Average loss: 7.2256\n",
      "Iteration: 905; Percent complete: 45.2%; Average loss: 7.5978\n",
      "Iteration: 906; Percent complete: 45.3%; Average loss: 7.8052\n",
      "Iteration: 907; Percent complete: 45.4%; Average loss: 7.1713\n",
      "Iteration: 908; Percent complete: 45.4%; Average loss: 7.2563\n",
      "Iteration: 909; Percent complete: 45.5%; Average loss: 7.3089\n",
      "Iteration: 910; Percent complete: 45.5%; Average loss: 7.2969\n",
      "Iteration: 911; Percent complete: 45.6%; Average loss: 7.2605\n",
      "Iteration: 912; Percent complete: 45.6%; Average loss: 6.8674\n",
      "Iteration: 913; Percent complete: 45.6%; Average loss: 7.3583\n",
      "Iteration: 914; Percent complete: 45.7%; Average loss: 7.2765\n",
      "Iteration: 915; Percent complete: 45.8%; Average loss: 7.3433\n",
      "Iteration: 916; Percent complete: 45.8%; Average loss: 7.4063\n",
      "Iteration: 917; Percent complete: 45.9%; Average loss: 7.4919\n",
      "Iteration: 918; Percent complete: 45.9%; Average loss: 7.1574\n",
      "Iteration: 919; Percent complete: 46.0%; Average loss: 7.2257\n",
      "Iteration: 920; Percent complete: 46.0%; Average loss: 6.9766\n",
      "Iteration: 921; Percent complete: 46.1%; Average loss: 7.1195\n",
      "Iteration: 922; Percent complete: 46.1%; Average loss: 7.0436\n",
      "Iteration: 923; Percent complete: 46.2%; Average loss: 6.7227\n",
      "Iteration: 924; Percent complete: 46.2%; Average loss: 7.0028\n",
      "Iteration: 925; Percent complete: 46.2%; Average loss: 7.1902\n",
      "Iteration: 926; Percent complete: 46.3%; Average loss: 7.3750\n",
      "Iteration: 927; Percent complete: 46.4%; Average loss: 7.2290\n",
      "Iteration: 928; Percent complete: 46.4%; Average loss: 7.5119\n",
      "Iteration: 929; Percent complete: 46.5%; Average loss: 7.4355\n",
      "Iteration: 930; Percent complete: 46.5%; Average loss: 7.0377\n",
      "Iteration: 931; Percent complete: 46.6%; Average loss: 7.1876\n",
      "Iteration: 932; Percent complete: 46.6%; Average loss: 6.8034\n",
      "Iteration: 933; Percent complete: 46.7%; Average loss: 7.1164\n",
      "Iteration: 934; Percent complete: 46.7%; Average loss: 7.0684\n",
      "Iteration: 935; Percent complete: 46.8%; Average loss: 6.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 936; Percent complete: 46.8%; Average loss: 7.1472\n",
      "Iteration: 937; Percent complete: 46.9%; Average loss: 7.3025\n",
      "Iteration: 938; Percent complete: 46.9%; Average loss: 7.2461\n",
      "Iteration: 939; Percent complete: 46.9%; Average loss: 6.8974\n",
      "Iteration: 940; Percent complete: 47.0%; Average loss: 7.2517\n",
      "Iteration: 941; Percent complete: 47.0%; Average loss: 7.0496\n",
      "Iteration: 942; Percent complete: 47.1%; Average loss: 7.2351\n",
      "Iteration: 943; Percent complete: 47.1%; Average loss: 7.1403\n",
      "Iteration: 944; Percent complete: 47.2%; Average loss: 7.3626\n",
      "Iteration: 945; Percent complete: 47.2%; Average loss: 7.0668\n",
      "Iteration: 946; Percent complete: 47.3%; Average loss: 7.1261\n",
      "Iteration: 947; Percent complete: 47.3%; Average loss: 7.0273\n",
      "Iteration: 948; Percent complete: 47.4%; Average loss: 6.7612\n",
      "Iteration: 949; Percent complete: 47.4%; Average loss: 6.8696\n",
      "Iteration: 950; Percent complete: 47.5%; Average loss: 7.2216\n",
      "Iteration: 951; Percent complete: 47.5%; Average loss: 7.1828\n",
      "Iteration: 952; Percent complete: 47.6%; Average loss: 7.0712\n",
      "Iteration: 953; Percent complete: 47.6%; Average loss: 7.2245\n",
      "Iteration: 954; Percent complete: 47.7%; Average loss: 7.0180\n",
      "Iteration: 955; Percent complete: 47.8%; Average loss: 6.5077\n",
      "Iteration: 956; Percent complete: 47.8%; Average loss: 7.0545\n",
      "Iteration: 957; Percent complete: 47.9%; Average loss: 7.0027\n",
      "Iteration: 958; Percent complete: 47.9%; Average loss: 6.7728\n",
      "Iteration: 959; Percent complete: 47.9%; Average loss: 7.2300\n",
      "Iteration: 960; Percent complete: 48.0%; Average loss: 7.1951\n",
      "Iteration: 961; Percent complete: 48.0%; Average loss: 7.0289\n",
      "Iteration: 962; Percent complete: 48.1%; Average loss: 7.5427\n",
      "Iteration: 963; Percent complete: 48.1%; Average loss: 7.3595\n",
      "Iteration: 964; Percent complete: 48.2%; Average loss: 6.9386\n",
      "Iteration: 965; Percent complete: 48.2%; Average loss: 7.0170\n",
      "Iteration: 966; Percent complete: 48.3%; Average loss: 7.1029\n",
      "Iteration: 967; Percent complete: 48.4%; Average loss: 7.0027\n",
      "Iteration: 968; Percent complete: 48.4%; Average loss: 6.9447\n",
      "Iteration: 969; Percent complete: 48.4%; Average loss: 7.2287\n",
      "Iteration: 970; Percent complete: 48.5%; Average loss: 7.3491\n",
      "Iteration: 971; Percent complete: 48.5%; Average loss: 7.2950\n",
      "Iteration: 972; Percent complete: 48.6%; Average loss: 6.9672\n",
      "Iteration: 973; Percent complete: 48.6%; Average loss: 7.0928\n",
      "Iteration: 974; Percent complete: 48.7%; Average loss: 7.4736\n",
      "Iteration: 975; Percent complete: 48.8%; Average loss: 7.2770\n",
      "Iteration: 976; Percent complete: 48.8%; Average loss: 7.1919\n",
      "Iteration: 977; Percent complete: 48.9%; Average loss: 7.1473\n",
      "Iteration: 978; Percent complete: 48.9%; Average loss: 7.3798\n",
      "Iteration: 979; Percent complete: 48.9%; Average loss: 7.1374\n",
      "Iteration: 980; Percent complete: 49.0%; Average loss: 7.1544\n",
      "Iteration: 981; Percent complete: 49.0%; Average loss: 7.1421\n",
      "Iteration: 982; Percent complete: 49.1%; Average loss: 7.0492\n",
      "Iteration: 983; Percent complete: 49.1%; Average loss: 6.8670\n",
      "Iteration: 984; Percent complete: 49.2%; Average loss: 7.2603\n",
      "Iteration: 985; Percent complete: 49.2%; Average loss: 7.0727\n",
      "Iteration: 986; Percent complete: 49.3%; Average loss: 7.2751\n",
      "Iteration: 987; Percent complete: 49.4%; Average loss: 6.9427\n",
      "Iteration: 988; Percent complete: 49.4%; Average loss: 7.1488\n",
      "Iteration: 989; Percent complete: 49.5%; Average loss: 6.9453\n",
      "Iteration: 990; Percent complete: 49.5%; Average loss: 6.8954\n",
      "Iteration: 991; Percent complete: 49.5%; Average loss: 7.0650\n",
      "Iteration: 992; Percent complete: 49.6%; Average loss: 7.2620\n",
      "Iteration: 993; Percent complete: 49.6%; Average loss: 7.1105\n",
      "Iteration: 994; Percent complete: 49.7%; Average loss: 6.9811\n",
      "Iteration: 995; Percent complete: 49.8%; Average loss: 6.9188\n",
      "Iteration: 996; Percent complete: 49.8%; Average loss: 7.2934\n",
      "Iteration: 997; Percent complete: 49.9%; Average loss: 7.2403\n",
      "Iteration: 998; Percent complete: 49.9%; Average loss: 7.1615\n",
      "Iteration: 999; Percent complete: 50.0%; Average loss: 6.8529\n",
      "Iteration: 1000; Percent complete: 50.0%; Average loss: 6.9862\n",
      "Iteration: 1001; Percent complete: 50.0%; Average loss: 7.2195\n",
      "Iteration: 1002; Percent complete: 50.1%; Average loss: 7.6529\n",
      "Iteration: 1003; Percent complete: 50.1%; Average loss: 7.1955\n",
      "Iteration: 1004; Percent complete: 50.2%; Average loss: 7.1653\n",
      "Iteration: 1005; Percent complete: 50.2%; Average loss: 6.9955\n",
      "Iteration: 1006; Percent complete: 50.3%; Average loss: 6.9283\n",
      "Iteration: 1007; Percent complete: 50.3%; Average loss: 7.1543\n",
      "Iteration: 1008; Percent complete: 50.4%; Average loss: 6.8572\n",
      "Iteration: 1009; Percent complete: 50.4%; Average loss: 6.9669\n",
      "Iteration: 1010; Percent complete: 50.5%; Average loss: 7.0582\n",
      "Iteration: 1011; Percent complete: 50.5%; Average loss: 6.8797\n",
      "Iteration: 1012; Percent complete: 50.6%; Average loss: 7.2907\n",
      "Iteration: 1013; Percent complete: 50.6%; Average loss: 6.7159\n",
      "Iteration: 1014; Percent complete: 50.7%; Average loss: 6.6453\n",
      "Iteration: 1015; Percent complete: 50.7%; Average loss: 7.2151\n",
      "Iteration: 1016; Percent complete: 50.8%; Average loss: 6.6806\n",
      "Iteration: 1017; Percent complete: 50.8%; Average loss: 7.0319\n",
      "Iteration: 1018; Percent complete: 50.9%; Average loss: 7.0811\n",
      "Iteration: 1019; Percent complete: 50.9%; Average loss: 7.1784\n",
      "Iteration: 1020; Percent complete: 51.0%; Average loss: 6.6472\n",
      "Iteration: 1021; Percent complete: 51.0%; Average loss: 7.1006\n",
      "Iteration: 1022; Percent complete: 51.1%; Average loss: 6.9061\n",
      "Iteration: 1023; Percent complete: 51.1%; Average loss: 7.1442\n",
      "Iteration: 1024; Percent complete: 51.2%; Average loss: 7.1454\n",
      "Iteration: 1025; Percent complete: 51.2%; Average loss: 7.0392\n",
      "Iteration: 1026; Percent complete: 51.3%; Average loss: 6.8552\n",
      "Iteration: 1027; Percent complete: 51.3%; Average loss: 6.4720\n",
      "Iteration: 1028; Percent complete: 51.4%; Average loss: 7.2927\n",
      "Iteration: 1029; Percent complete: 51.4%; Average loss: 6.9364\n",
      "Iteration: 1030; Percent complete: 51.5%; Average loss: 7.0422\n",
      "Iteration: 1031; Percent complete: 51.5%; Average loss: 6.9322\n",
      "Iteration: 1032; Percent complete: 51.6%; Average loss: 7.1666\n",
      "Iteration: 1033; Percent complete: 51.6%; Average loss: 6.4690\n",
      "Iteration: 1034; Percent complete: 51.7%; Average loss: 6.7633\n",
      "Iteration: 1035; Percent complete: 51.7%; Average loss: 6.8059\n",
      "Iteration: 1036; Percent complete: 51.8%; Average loss: 6.8575\n",
      "Iteration: 1037; Percent complete: 51.8%; Average loss: 6.9428\n",
      "Iteration: 1038; Percent complete: 51.9%; Average loss: 7.0733\n",
      "Iteration: 1039; Percent complete: 51.9%; Average loss: 6.9606\n",
      "Iteration: 1040; Percent complete: 52.0%; Average loss: 6.9236\n",
      "Iteration: 1041; Percent complete: 52.0%; Average loss: 6.8187\n",
      "Iteration: 1042; Percent complete: 52.1%; Average loss: 7.3572\n",
      "Iteration: 1043; Percent complete: 52.1%; Average loss: 6.4301\n",
      "Iteration: 1044; Percent complete: 52.2%; Average loss: 6.7197\n",
      "Iteration: 1045; Percent complete: 52.2%; Average loss: 6.6440\n",
      "Iteration: 1046; Percent complete: 52.3%; Average loss: 6.7454\n",
      "Iteration: 1047; Percent complete: 52.3%; Average loss: 6.8781\n",
      "Iteration: 1048; Percent complete: 52.4%; Average loss: 7.1553\n",
      "Iteration: 1049; Percent complete: 52.4%; Average loss: 7.0214\n",
      "Iteration: 1050; Percent complete: 52.5%; Average loss: 7.1131\n",
      "Iteration: 1051; Percent complete: 52.5%; Average loss: 6.7679\n",
      "Iteration: 1052; Percent complete: 52.6%; Average loss: 7.1494\n",
      "Iteration: 1053; Percent complete: 52.6%; Average loss: 6.9460\n",
      "Iteration: 1054; Percent complete: 52.7%; Average loss: 6.9193\n",
      "Iteration: 1055; Percent complete: 52.8%; Average loss: 6.7297\n",
      "Iteration: 1056; Percent complete: 52.8%; Average loss: 7.0213\n",
      "Iteration: 1057; Percent complete: 52.8%; Average loss: 6.8773\n",
      "Iteration: 1058; Percent complete: 52.9%; Average loss: 6.6588\n",
      "Iteration: 1059; Percent complete: 52.9%; Average loss: 6.8472\n",
      "Iteration: 1060; Percent complete: 53.0%; Average loss: 6.9453\n",
      "Iteration: 1061; Percent complete: 53.0%; Average loss: 6.9607\n",
      "Iteration: 1062; Percent complete: 53.1%; Average loss: 7.0253\n",
      "Iteration: 1063; Percent complete: 53.1%; Average loss: 7.3619\n",
      "Iteration: 1064; Percent complete: 53.2%; Average loss: 7.2581\n",
      "Iteration: 1065; Percent complete: 53.2%; Average loss: 6.8962\n",
      "Iteration: 1066; Percent complete: 53.3%; Average loss: 6.6774\n",
      "Iteration: 1067; Percent complete: 53.3%; Average loss: 6.7588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1068; Percent complete: 53.4%; Average loss: 7.1112\n",
      "Iteration: 1069; Percent complete: 53.4%; Average loss: 7.0729\n",
      "Iteration: 1070; Percent complete: 53.5%; Average loss: 6.6547\n",
      "Iteration: 1071; Percent complete: 53.5%; Average loss: 6.8290\n",
      "Iteration: 1072; Percent complete: 53.6%; Average loss: 6.8000\n",
      "Iteration: 1073; Percent complete: 53.6%; Average loss: 7.0266\n",
      "Iteration: 1074; Percent complete: 53.7%; Average loss: 6.6143\n",
      "Iteration: 1075; Percent complete: 53.8%; Average loss: 6.3008\n",
      "Iteration: 1076; Percent complete: 53.8%; Average loss: 6.4462\n",
      "Iteration: 1077; Percent complete: 53.8%; Average loss: 6.7761\n",
      "Iteration: 1078; Percent complete: 53.9%; Average loss: 6.7044\n",
      "Iteration: 1079; Percent complete: 53.9%; Average loss: 6.7707\n",
      "Iteration: 1080; Percent complete: 54.0%; Average loss: 6.9777\n",
      "Iteration: 1081; Percent complete: 54.0%; Average loss: 7.0660\n",
      "Iteration: 1082; Percent complete: 54.1%; Average loss: 6.5409\n",
      "Iteration: 1083; Percent complete: 54.1%; Average loss: 7.3403\n",
      "Iteration: 1084; Percent complete: 54.2%; Average loss: 6.7197\n",
      "Iteration: 1085; Percent complete: 54.2%; Average loss: 6.5319\n",
      "Iteration: 1086; Percent complete: 54.3%; Average loss: 6.7649\n",
      "Iteration: 1087; Percent complete: 54.4%; Average loss: 6.8039\n",
      "Iteration: 1088; Percent complete: 54.4%; Average loss: 7.0776\n",
      "Iteration: 1089; Percent complete: 54.4%; Average loss: 6.6032\n",
      "Iteration: 1090; Percent complete: 54.5%; Average loss: 6.4278\n",
      "Iteration: 1091; Percent complete: 54.5%; Average loss: 6.7809\n",
      "Iteration: 1092; Percent complete: 54.6%; Average loss: 6.9732\n",
      "Iteration: 1093; Percent complete: 54.6%; Average loss: 7.0466\n",
      "Iteration: 1094; Percent complete: 54.7%; Average loss: 6.6568\n",
      "Iteration: 1095; Percent complete: 54.8%; Average loss: 7.2013\n",
      "Iteration: 1096; Percent complete: 54.8%; Average loss: 6.8254\n",
      "Iteration: 1097; Percent complete: 54.9%; Average loss: 6.8076\n",
      "Iteration: 1098; Percent complete: 54.9%; Average loss: 6.7809\n",
      "Iteration: 1099; Percent complete: 54.9%; Average loss: 6.8710\n",
      "Iteration: 1100; Percent complete: 55.0%; Average loss: 7.0480\n",
      "Iteration: 1101; Percent complete: 55.0%; Average loss: 6.9258\n",
      "Iteration: 1102; Percent complete: 55.1%; Average loss: 6.7850\n",
      "Iteration: 1103; Percent complete: 55.1%; Average loss: 7.0079\n",
      "Iteration: 1104; Percent complete: 55.2%; Average loss: 6.5132\n",
      "Iteration: 1105; Percent complete: 55.2%; Average loss: 6.8883\n",
      "Iteration: 1106; Percent complete: 55.3%; Average loss: 6.2740\n",
      "Iteration: 1107; Percent complete: 55.4%; Average loss: 6.7685\n",
      "Iteration: 1108; Percent complete: 55.4%; Average loss: 6.8049\n",
      "Iteration: 1109; Percent complete: 55.5%; Average loss: 6.7721\n",
      "Iteration: 1110; Percent complete: 55.5%; Average loss: 6.9970\n",
      "Iteration: 1111; Percent complete: 55.5%; Average loss: 6.5469\n",
      "Iteration: 1112; Percent complete: 55.6%; Average loss: 6.5481\n",
      "Iteration: 1113; Percent complete: 55.6%; Average loss: 7.0926\n",
      "Iteration: 1114; Percent complete: 55.7%; Average loss: 6.6676\n",
      "Iteration: 1115; Percent complete: 55.8%; Average loss: 6.3202\n",
      "Iteration: 1116; Percent complete: 55.8%; Average loss: 6.5297\n",
      "Iteration: 1117; Percent complete: 55.9%; Average loss: 6.8189\n",
      "Iteration: 1118; Percent complete: 55.9%; Average loss: 6.6400\n",
      "Iteration: 1119; Percent complete: 56.0%; Average loss: 6.6175\n",
      "Iteration: 1120; Percent complete: 56.0%; Average loss: 7.1494\n",
      "Iteration: 1121; Percent complete: 56.0%; Average loss: 7.0012\n",
      "Iteration: 1122; Percent complete: 56.1%; Average loss: 6.8300\n",
      "Iteration: 1123; Percent complete: 56.1%; Average loss: 6.6363\n",
      "Iteration: 1124; Percent complete: 56.2%; Average loss: 7.0050\n",
      "Iteration: 1125; Percent complete: 56.2%; Average loss: 6.7842\n",
      "Iteration: 1126; Percent complete: 56.3%; Average loss: 6.6021\n",
      "Iteration: 1127; Percent complete: 56.4%; Average loss: 6.8091\n",
      "Iteration: 1128; Percent complete: 56.4%; Average loss: 6.4459\n",
      "Iteration: 1129; Percent complete: 56.5%; Average loss: 6.6928\n",
      "Iteration: 1130; Percent complete: 56.5%; Average loss: 6.9849\n",
      "Iteration: 1131; Percent complete: 56.5%; Average loss: 6.6396\n",
      "Iteration: 1132; Percent complete: 56.6%; Average loss: 6.3310\n",
      "Iteration: 1133; Percent complete: 56.6%; Average loss: 6.9933\n",
      "Iteration: 1134; Percent complete: 56.7%; Average loss: 6.9172\n",
      "Iteration: 1135; Percent complete: 56.8%; Average loss: 6.1601\n",
      "Iteration: 1136; Percent complete: 56.8%; Average loss: 6.4423\n",
      "Iteration: 1137; Percent complete: 56.9%; Average loss: 6.9704\n",
      "Iteration: 1138; Percent complete: 56.9%; Average loss: 6.7331\n",
      "Iteration: 1139; Percent complete: 57.0%; Average loss: 6.6632\n",
      "Iteration: 1140; Percent complete: 57.0%; Average loss: 6.4806\n",
      "Iteration: 1141; Percent complete: 57.0%; Average loss: 6.5790\n",
      "Iteration: 1142; Percent complete: 57.1%; Average loss: 6.8216\n",
      "Iteration: 1143; Percent complete: 57.1%; Average loss: 6.7294\n",
      "Iteration: 1144; Percent complete: 57.2%; Average loss: 6.5291\n",
      "Iteration: 1145; Percent complete: 57.2%; Average loss: 6.7072\n",
      "Iteration: 1146; Percent complete: 57.3%; Average loss: 6.8101\n",
      "Iteration: 1147; Percent complete: 57.4%; Average loss: 6.5493\n",
      "Iteration: 1148; Percent complete: 57.4%; Average loss: 6.7392\n",
      "Iteration: 1149; Percent complete: 57.5%; Average loss: 6.6265\n",
      "Iteration: 1150; Percent complete: 57.5%; Average loss: 6.4263\n",
      "Iteration: 1151; Percent complete: 57.6%; Average loss: 6.6341\n",
      "Iteration: 1152; Percent complete: 57.6%; Average loss: 6.1327\n",
      "Iteration: 1153; Percent complete: 57.6%; Average loss: 6.8740\n",
      "Iteration: 1154; Percent complete: 57.7%; Average loss: 6.5900\n",
      "Iteration: 1155; Percent complete: 57.8%; Average loss: 6.6108\n",
      "Iteration: 1156; Percent complete: 57.8%; Average loss: 6.5489\n",
      "Iteration: 1157; Percent complete: 57.9%; Average loss: 6.3381\n",
      "Iteration: 1158; Percent complete: 57.9%; Average loss: 6.5274\n",
      "Iteration: 1159; Percent complete: 58.0%; Average loss: 6.7225\n",
      "Iteration: 1160; Percent complete: 58.0%; Average loss: 6.1579\n",
      "Iteration: 1161; Percent complete: 58.1%; Average loss: 6.8444\n",
      "Iteration: 1162; Percent complete: 58.1%; Average loss: 6.1739\n",
      "Iteration: 1163; Percent complete: 58.1%; Average loss: 6.5434\n",
      "Iteration: 1164; Percent complete: 58.2%; Average loss: 6.6189\n",
      "Iteration: 1165; Percent complete: 58.2%; Average loss: 6.9482\n",
      "Iteration: 1166; Percent complete: 58.3%; Average loss: 7.0196\n",
      "Iteration: 1167; Percent complete: 58.4%; Average loss: 6.8536\n",
      "Iteration: 1168; Percent complete: 58.4%; Average loss: 6.2601\n",
      "Iteration: 1169; Percent complete: 58.5%; Average loss: 6.3862\n",
      "Iteration: 1170; Percent complete: 58.5%; Average loss: 6.5315\n",
      "Iteration: 1171; Percent complete: 58.6%; Average loss: 6.8527\n",
      "Iteration: 1172; Percent complete: 58.6%; Average loss: 7.2248\n",
      "Iteration: 1173; Percent complete: 58.7%; Average loss: 6.8317\n",
      "Iteration: 1174; Percent complete: 58.7%; Average loss: 6.5088\n",
      "Iteration: 1175; Percent complete: 58.8%; Average loss: 6.2675\n",
      "Iteration: 1176; Percent complete: 58.8%; Average loss: 6.3802\n",
      "Iteration: 1177; Percent complete: 58.9%; Average loss: 6.6565\n",
      "Iteration: 1178; Percent complete: 58.9%; Average loss: 6.6312\n",
      "Iteration: 1179; Percent complete: 59.0%; Average loss: 6.3846\n",
      "Iteration: 1180; Percent complete: 59.0%; Average loss: 6.1322\n",
      "Iteration: 1181; Percent complete: 59.1%; Average loss: 6.8193\n",
      "Iteration: 1182; Percent complete: 59.1%; Average loss: 6.7351\n",
      "Iteration: 1183; Percent complete: 59.2%; Average loss: 6.6862\n",
      "Iteration: 1184; Percent complete: 59.2%; Average loss: 6.4950\n",
      "Iteration: 1185; Percent complete: 59.2%; Average loss: 6.7589\n",
      "Iteration: 1186; Percent complete: 59.3%; Average loss: 6.3594\n",
      "Iteration: 1187; Percent complete: 59.4%; Average loss: 6.6725\n",
      "Iteration: 1188; Percent complete: 59.4%; Average loss: 6.4948\n",
      "Iteration: 1189; Percent complete: 59.5%; Average loss: 6.6788\n",
      "Iteration: 1190; Percent complete: 59.5%; Average loss: 6.7827\n",
      "Iteration: 1191; Percent complete: 59.6%; Average loss: 6.5121\n",
      "Iteration: 1192; Percent complete: 59.6%; Average loss: 7.0239\n",
      "Iteration: 1193; Percent complete: 59.7%; Average loss: 6.6485\n",
      "Iteration: 1194; Percent complete: 59.7%; Average loss: 6.5616\n",
      "Iteration: 1195; Percent complete: 59.8%; Average loss: 6.2990\n",
      "Iteration: 1196; Percent complete: 59.8%; Average loss: 6.5560\n",
      "Iteration: 1197; Percent complete: 59.9%; Average loss: 6.8620\n",
      "Iteration: 1198; Percent complete: 59.9%; Average loss: 6.6546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1199; Percent complete: 60.0%; Average loss: 6.4999\n",
      "Iteration: 1200; Percent complete: 60.0%; Average loss: 6.3683\n",
      "Iteration: 1201; Percent complete: 60.1%; Average loss: 6.4193\n",
      "Iteration: 1202; Percent complete: 60.1%; Average loss: 6.0892\n",
      "Iteration: 1203; Percent complete: 60.2%; Average loss: 6.1928\n",
      "Iteration: 1204; Percent complete: 60.2%; Average loss: 6.7350\n",
      "Iteration: 1205; Percent complete: 60.2%; Average loss: 6.7908\n",
      "Iteration: 1206; Percent complete: 60.3%; Average loss: 6.4613\n",
      "Iteration: 1207; Percent complete: 60.4%; Average loss: 6.3966\n",
      "Iteration: 1208; Percent complete: 60.4%; Average loss: 6.4442\n",
      "Iteration: 1209; Percent complete: 60.5%; Average loss: 6.9394\n",
      "Iteration: 1210; Percent complete: 60.5%; Average loss: 6.8355\n",
      "Iteration: 1211; Percent complete: 60.6%; Average loss: 6.5107\n",
      "Iteration: 1212; Percent complete: 60.6%; Average loss: 6.1479\n",
      "Iteration: 1213; Percent complete: 60.7%; Average loss: 6.4706\n",
      "Iteration: 1214; Percent complete: 60.7%; Average loss: 6.1521\n",
      "Iteration: 1215; Percent complete: 60.8%; Average loss: 6.4351\n",
      "Iteration: 1216; Percent complete: 60.8%; Average loss: 6.4752\n",
      "Iteration: 1217; Percent complete: 60.9%; Average loss: 6.3452\n",
      "Iteration: 1218; Percent complete: 60.9%; Average loss: 6.8266\n",
      "Iteration: 1219; Percent complete: 61.0%; Average loss: 6.7748\n",
      "Iteration: 1220; Percent complete: 61.0%; Average loss: 6.5331\n",
      "Iteration: 1221; Percent complete: 61.1%; Average loss: 6.5111\n",
      "Iteration: 1222; Percent complete: 61.1%; Average loss: 6.1724\n",
      "Iteration: 1223; Percent complete: 61.2%; Average loss: 6.6920\n",
      "Iteration: 1224; Percent complete: 61.2%; Average loss: 6.3067\n",
      "Iteration: 1225; Percent complete: 61.3%; Average loss: 6.4517\n",
      "Iteration: 1226; Percent complete: 61.3%; Average loss: 6.3690\n",
      "Iteration: 1227; Percent complete: 61.4%; Average loss: 6.4226\n",
      "Iteration: 1228; Percent complete: 61.4%; Average loss: 6.5197\n",
      "Iteration: 1229; Percent complete: 61.5%; Average loss: 6.1938\n",
      "Iteration: 1230; Percent complete: 61.5%; Average loss: 6.6183\n",
      "Iteration: 1231; Percent complete: 61.6%; Average loss: 6.2238\n",
      "Iteration: 1232; Percent complete: 61.6%; Average loss: 6.4724\n",
      "Iteration: 1233; Percent complete: 61.7%; Average loss: 6.4855\n",
      "Iteration: 1234; Percent complete: 61.7%; Average loss: 6.4597\n",
      "Iteration: 1235; Percent complete: 61.8%; Average loss: 7.1143\n",
      "Iteration: 1236; Percent complete: 61.8%; Average loss: 6.4907\n",
      "Iteration: 1237; Percent complete: 61.9%; Average loss: 6.0635\n",
      "Iteration: 1238; Percent complete: 61.9%; Average loss: 6.0637\n",
      "Iteration: 1239; Percent complete: 62.0%; Average loss: 6.2809\n",
      "Iteration: 1240; Percent complete: 62.0%; Average loss: 6.3356\n",
      "Iteration: 1241; Percent complete: 62.1%; Average loss: 6.5204\n",
      "Iteration: 1242; Percent complete: 62.1%; Average loss: 6.6156\n",
      "Iteration: 1243; Percent complete: 62.2%; Average loss: 6.5001\n",
      "Iteration: 1244; Percent complete: 62.2%; Average loss: 6.5192\n",
      "Iteration: 1245; Percent complete: 62.3%; Average loss: 6.4556\n",
      "Iteration: 1246; Percent complete: 62.3%; Average loss: 6.0795\n",
      "Iteration: 1247; Percent complete: 62.4%; Average loss: 6.1319\n",
      "Iteration: 1248; Percent complete: 62.4%; Average loss: 6.1741\n",
      "Iteration: 1249; Percent complete: 62.5%; Average loss: 6.2232\n",
      "Iteration: 1250; Percent complete: 62.5%; Average loss: 6.5629\n",
      "Iteration: 1251; Percent complete: 62.5%; Average loss: 5.9935\n",
      "Iteration: 1252; Percent complete: 62.6%; Average loss: 6.6463\n",
      "Iteration: 1253; Percent complete: 62.6%; Average loss: 6.2545\n",
      "Iteration: 1254; Percent complete: 62.7%; Average loss: 6.5244\n",
      "Iteration: 1255; Percent complete: 62.7%; Average loss: 6.5238\n",
      "Iteration: 1256; Percent complete: 62.8%; Average loss: 6.4053\n",
      "Iteration: 1257; Percent complete: 62.8%; Average loss: 6.3104\n",
      "Iteration: 1258; Percent complete: 62.9%; Average loss: 6.5853\n",
      "Iteration: 1259; Percent complete: 62.9%; Average loss: 5.9738\n",
      "Iteration: 1260; Percent complete: 63.0%; Average loss: 6.2166\n",
      "Iteration: 1261; Percent complete: 63.0%; Average loss: 6.4866\n",
      "Iteration: 1262; Percent complete: 63.1%; Average loss: 5.7605\n",
      "Iteration: 1263; Percent complete: 63.1%; Average loss: 6.2662\n",
      "Iteration: 1264; Percent complete: 63.2%; Average loss: 5.9707\n",
      "Iteration: 1265; Percent complete: 63.2%; Average loss: 6.3583\n",
      "Iteration: 1266; Percent complete: 63.3%; Average loss: 6.3465\n",
      "Iteration: 1267; Percent complete: 63.3%; Average loss: 6.2434\n",
      "Iteration: 1268; Percent complete: 63.4%; Average loss: 6.4224\n",
      "Iteration: 1269; Percent complete: 63.4%; Average loss: 6.3177\n",
      "Iteration: 1270; Percent complete: 63.5%; Average loss: 6.6960\n",
      "Iteration: 1271; Percent complete: 63.5%; Average loss: 6.0439\n",
      "Iteration: 1272; Percent complete: 63.6%; Average loss: 6.4584\n",
      "Iteration: 1273; Percent complete: 63.6%; Average loss: 6.4348\n",
      "Iteration: 1274; Percent complete: 63.7%; Average loss: 6.2139\n",
      "Iteration: 1275; Percent complete: 63.7%; Average loss: 6.3549\n",
      "Iteration: 1276; Percent complete: 63.8%; Average loss: 5.8892\n",
      "Iteration: 1277; Percent complete: 63.8%; Average loss: 6.5845\n",
      "Iteration: 1278; Percent complete: 63.9%; Average loss: 6.6479\n",
      "Iteration: 1279; Percent complete: 63.9%; Average loss: 6.2350\n",
      "Iteration: 1280; Percent complete: 64.0%; Average loss: 6.3017\n",
      "Iteration: 1281; Percent complete: 64.0%; Average loss: 6.5635\n",
      "Iteration: 1282; Percent complete: 64.1%; Average loss: 6.2633\n",
      "Iteration: 1283; Percent complete: 64.1%; Average loss: 6.3854\n",
      "Iteration: 1284; Percent complete: 64.2%; Average loss: 6.3147\n",
      "Iteration: 1285; Percent complete: 64.2%; Average loss: 6.3130\n",
      "Iteration: 1286; Percent complete: 64.3%; Average loss: 6.0621\n",
      "Iteration: 1287; Percent complete: 64.3%; Average loss: 6.4045\n",
      "Iteration: 1288; Percent complete: 64.4%; Average loss: 6.1612\n",
      "Iteration: 1289; Percent complete: 64.5%; Average loss: 6.2637\n",
      "Iteration: 1290; Percent complete: 64.5%; Average loss: 6.8462\n",
      "Iteration: 1291; Percent complete: 64.5%; Average loss: 6.4862\n",
      "Iteration: 1292; Percent complete: 64.6%; Average loss: 6.1569\n",
      "Iteration: 1293; Percent complete: 64.6%; Average loss: 5.9991\n",
      "Iteration: 1294; Percent complete: 64.7%; Average loss: 5.9337\n",
      "Iteration: 1295; Percent complete: 64.8%; Average loss: 6.2956\n",
      "Iteration: 1296; Percent complete: 64.8%; Average loss: 6.7009\n",
      "Iteration: 1297; Percent complete: 64.8%; Average loss: 6.2192\n",
      "Iteration: 1298; Percent complete: 64.9%; Average loss: 6.0528\n",
      "Iteration: 1299; Percent complete: 65.0%; Average loss: 6.2896\n",
      "Iteration: 1300; Percent complete: 65.0%; Average loss: 6.1709\n",
      "Iteration: 1301; Percent complete: 65.0%; Average loss: 6.1541\n",
      "Iteration: 1302; Percent complete: 65.1%; Average loss: 6.2442\n",
      "Iteration: 1303; Percent complete: 65.1%; Average loss: 6.1312\n",
      "Iteration: 1304; Percent complete: 65.2%; Average loss: 5.8026\n",
      "Iteration: 1305; Percent complete: 65.2%; Average loss: 6.1756\n",
      "Iteration: 1306; Percent complete: 65.3%; Average loss: 6.5921\n",
      "Iteration: 1307; Percent complete: 65.3%; Average loss: 6.1447\n",
      "Iteration: 1308; Percent complete: 65.4%; Average loss: 6.0048\n",
      "Iteration: 1309; Percent complete: 65.5%; Average loss: 6.4177\n",
      "Iteration: 1310; Percent complete: 65.5%; Average loss: 6.3072\n",
      "Iteration: 1311; Percent complete: 65.5%; Average loss: 6.0515\n",
      "Iteration: 1312; Percent complete: 65.6%; Average loss: 6.5416\n",
      "Iteration: 1313; Percent complete: 65.6%; Average loss: 6.2258\n",
      "Iteration: 1314; Percent complete: 65.7%; Average loss: 6.5428\n",
      "Iteration: 1315; Percent complete: 65.8%; Average loss: 6.5441\n",
      "Iteration: 1316; Percent complete: 65.8%; Average loss: 6.3113\n",
      "Iteration: 1317; Percent complete: 65.8%; Average loss: 6.1064\n",
      "Iteration: 1318; Percent complete: 65.9%; Average loss: 5.7614\n",
      "Iteration: 1319; Percent complete: 66.0%; Average loss: 6.1520\n",
      "Iteration: 1320; Percent complete: 66.0%; Average loss: 5.7870\n",
      "Iteration: 1321; Percent complete: 66.0%; Average loss: 6.3293\n",
      "Iteration: 1322; Percent complete: 66.1%; Average loss: 5.7115\n",
      "Iteration: 1323; Percent complete: 66.1%; Average loss: 6.1071\n",
      "Iteration: 1324; Percent complete: 66.2%; Average loss: 6.2927\n",
      "Iteration: 1325; Percent complete: 66.2%; Average loss: 6.1578\n",
      "Iteration: 1326; Percent complete: 66.3%; Average loss: 6.1499\n",
      "Iteration: 1327; Percent complete: 66.3%; Average loss: 6.0983\n",
      "Iteration: 1328; Percent complete: 66.4%; Average loss: 6.3635\n",
      "Iteration: 1329; Percent complete: 66.5%; Average loss: 5.9279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1330; Percent complete: 66.5%; Average loss: 6.0218\n",
      "Iteration: 1331; Percent complete: 66.5%; Average loss: 5.9449\n",
      "Iteration: 1332; Percent complete: 66.6%; Average loss: 5.9766\n",
      "Iteration: 1333; Percent complete: 66.6%; Average loss: 5.7155\n",
      "Iteration: 1334; Percent complete: 66.7%; Average loss: 6.0678\n",
      "Iteration: 1335; Percent complete: 66.8%; Average loss: 6.0803\n",
      "Iteration: 1336; Percent complete: 66.8%; Average loss: 6.3071\n",
      "Iteration: 1337; Percent complete: 66.8%; Average loss: 6.1690\n",
      "Iteration: 1338; Percent complete: 66.9%; Average loss: 5.9469\n",
      "Iteration: 1339; Percent complete: 67.0%; Average loss: 6.3935\n",
      "Iteration: 1340; Percent complete: 67.0%; Average loss: 5.8492\n",
      "Iteration: 1341; Percent complete: 67.0%; Average loss: 6.4645\n",
      "Iteration: 1342; Percent complete: 67.1%; Average loss: 6.0178\n",
      "Iteration: 1343; Percent complete: 67.2%; Average loss: 6.0717\n",
      "Iteration: 1344; Percent complete: 67.2%; Average loss: 6.4656\n",
      "Iteration: 1345; Percent complete: 67.2%; Average loss: 6.6954\n",
      "Iteration: 1346; Percent complete: 67.3%; Average loss: 6.1354\n",
      "Iteration: 1347; Percent complete: 67.3%; Average loss: 6.2835\n",
      "Iteration: 1348; Percent complete: 67.4%; Average loss: 6.0663\n",
      "Iteration: 1349; Percent complete: 67.5%; Average loss: 6.0538\n",
      "Iteration: 1350; Percent complete: 67.5%; Average loss: 6.4104\n",
      "Iteration: 1351; Percent complete: 67.5%; Average loss: 6.3961\n",
      "Iteration: 1352; Percent complete: 67.6%; Average loss: 6.5904\n",
      "Iteration: 1353; Percent complete: 67.7%; Average loss: 5.8337\n",
      "Iteration: 1354; Percent complete: 67.7%; Average loss: 6.0202\n",
      "Iteration: 1355; Percent complete: 67.8%; Average loss: 6.0532\n",
      "Iteration: 1356; Percent complete: 67.8%; Average loss: 6.1636\n",
      "Iteration: 1357; Percent complete: 67.8%; Average loss: 6.1578\n",
      "Iteration: 1358; Percent complete: 67.9%; Average loss: 6.0681\n",
      "Iteration: 1359; Percent complete: 68.0%; Average loss: 6.1513\n",
      "Iteration: 1360; Percent complete: 68.0%; Average loss: 5.9514\n",
      "Iteration: 1361; Percent complete: 68.0%; Average loss: 6.0968\n",
      "Iteration: 1362; Percent complete: 68.1%; Average loss: 6.1118\n",
      "Iteration: 1363; Percent complete: 68.2%; Average loss: 6.0499\n",
      "Iteration: 1364; Percent complete: 68.2%; Average loss: 6.4343\n",
      "Iteration: 1365; Percent complete: 68.2%; Average loss: 6.1461\n",
      "Iteration: 1366; Percent complete: 68.3%; Average loss: 5.9641\n",
      "Iteration: 1367; Percent complete: 68.3%; Average loss: 6.0824\n",
      "Iteration: 1368; Percent complete: 68.4%; Average loss: 5.7633\n",
      "Iteration: 1369; Percent complete: 68.5%; Average loss: 6.0740\n",
      "Iteration: 1370; Percent complete: 68.5%; Average loss: 5.9400\n",
      "Iteration: 1371; Percent complete: 68.5%; Average loss: 5.9131\n",
      "Iteration: 1372; Percent complete: 68.6%; Average loss: 5.7737\n",
      "Iteration: 1373; Percent complete: 68.7%; Average loss: 6.0143\n",
      "Iteration: 1374; Percent complete: 68.7%; Average loss: 5.5275\n",
      "Iteration: 1375; Percent complete: 68.8%; Average loss: 5.8602\n",
      "Iteration: 1376; Percent complete: 68.8%; Average loss: 5.7688\n",
      "Iteration: 1377; Percent complete: 68.8%; Average loss: 6.1000\n",
      "Iteration: 1378; Percent complete: 68.9%; Average loss: 5.9148\n",
      "Iteration: 1379; Percent complete: 69.0%; Average loss: 6.4558\n",
      "Iteration: 1380; Percent complete: 69.0%; Average loss: 6.3077\n",
      "Iteration: 1381; Percent complete: 69.0%; Average loss: 6.6021\n",
      "Iteration: 1382; Percent complete: 69.1%; Average loss: 5.9489\n",
      "Iteration: 1383; Percent complete: 69.2%; Average loss: 5.9395\n",
      "Iteration: 1384; Percent complete: 69.2%; Average loss: 5.8992\n",
      "Iteration: 1385; Percent complete: 69.2%; Average loss: 5.7696\n",
      "Iteration: 1386; Percent complete: 69.3%; Average loss: 6.3930\n",
      "Iteration: 1387; Percent complete: 69.3%; Average loss: 6.4860\n",
      "Iteration: 1388; Percent complete: 69.4%; Average loss: 5.6639\n",
      "Iteration: 1389; Percent complete: 69.5%; Average loss: 6.1635\n",
      "Iteration: 1390; Percent complete: 69.5%; Average loss: 6.0474\n",
      "Iteration: 1391; Percent complete: 69.5%; Average loss: 6.0856\n",
      "Iteration: 1392; Percent complete: 69.6%; Average loss: 6.0401\n",
      "Iteration: 1393; Percent complete: 69.7%; Average loss: 6.2001\n",
      "Iteration: 1394; Percent complete: 69.7%; Average loss: 6.3792\n",
      "Iteration: 1395; Percent complete: 69.8%; Average loss: 6.0950\n",
      "Iteration: 1396; Percent complete: 69.8%; Average loss: 6.1517\n",
      "Iteration: 1397; Percent complete: 69.8%; Average loss: 6.0000\n",
      "Iteration: 1398; Percent complete: 69.9%; Average loss: 5.9651\n",
      "Iteration: 1399; Percent complete: 70.0%; Average loss: 5.7367\n",
      "Iteration: 1400; Percent complete: 70.0%; Average loss: 6.4773\n",
      "Iteration: 1401; Percent complete: 70.0%; Average loss: 5.9808\n",
      "Iteration: 1402; Percent complete: 70.1%; Average loss: 5.6295\n",
      "Iteration: 1403; Percent complete: 70.2%; Average loss: 5.9400\n",
      "Iteration: 1404; Percent complete: 70.2%; Average loss: 5.8800\n",
      "Iteration: 1405; Percent complete: 70.2%; Average loss: 5.8820\n",
      "Iteration: 1406; Percent complete: 70.3%; Average loss: 5.9373\n",
      "Iteration: 1407; Percent complete: 70.3%; Average loss: 6.0101\n",
      "Iteration: 1408; Percent complete: 70.4%; Average loss: 5.9146\n",
      "Iteration: 1409; Percent complete: 70.5%; Average loss: 6.3394\n",
      "Iteration: 1410; Percent complete: 70.5%; Average loss: 6.0409\n",
      "Iteration: 1411; Percent complete: 70.5%; Average loss: 6.2942\n",
      "Iteration: 1412; Percent complete: 70.6%; Average loss: 5.5208\n",
      "Iteration: 1413; Percent complete: 70.7%; Average loss: 5.9066\n",
      "Iteration: 1414; Percent complete: 70.7%; Average loss: 5.7570\n",
      "Iteration: 1415; Percent complete: 70.8%; Average loss: 5.2744\n",
      "Iteration: 1416; Percent complete: 70.8%; Average loss: 6.0389\n",
      "Iteration: 1417; Percent complete: 70.9%; Average loss: 5.7188\n",
      "Iteration: 1418; Percent complete: 70.9%; Average loss: 6.1606\n",
      "Iteration: 1419; Percent complete: 71.0%; Average loss: 5.7038\n",
      "Iteration: 1420; Percent complete: 71.0%; Average loss: 6.0491\n",
      "Iteration: 1421; Percent complete: 71.0%; Average loss: 6.2854\n",
      "Iteration: 1422; Percent complete: 71.1%; Average loss: 5.8377\n",
      "Iteration: 1423; Percent complete: 71.2%; Average loss: 5.7331\n",
      "Iteration: 1424; Percent complete: 71.2%; Average loss: 5.4808\n",
      "Iteration: 1425; Percent complete: 71.2%; Average loss: 5.8475\n",
      "Iteration: 1426; Percent complete: 71.3%; Average loss: 6.0619\n",
      "Iteration: 1427; Percent complete: 71.4%; Average loss: 5.8303\n",
      "Iteration: 1428; Percent complete: 71.4%; Average loss: 5.6742\n",
      "Iteration: 1429; Percent complete: 71.5%; Average loss: 5.6679\n",
      "Iteration: 1430; Percent complete: 71.5%; Average loss: 5.6821\n",
      "Iteration: 1431; Percent complete: 71.5%; Average loss: 5.5942\n",
      "Iteration: 1432; Percent complete: 71.6%; Average loss: 5.7504\n",
      "Iteration: 1433; Percent complete: 71.7%; Average loss: 5.8954\n",
      "Iteration: 1434; Percent complete: 71.7%; Average loss: 6.0242\n",
      "Iteration: 1435; Percent complete: 71.8%; Average loss: 5.8719\n",
      "Iteration: 1436; Percent complete: 71.8%; Average loss: 5.7776\n",
      "Iteration: 1437; Percent complete: 71.9%; Average loss: 5.7987\n",
      "Iteration: 1438; Percent complete: 71.9%; Average loss: 6.0417\n",
      "Iteration: 1439; Percent complete: 72.0%; Average loss: 5.9531\n",
      "Iteration: 1440; Percent complete: 72.0%; Average loss: 5.6718\n",
      "Iteration: 1441; Percent complete: 72.0%; Average loss: 5.8815\n",
      "Iteration: 1442; Percent complete: 72.1%; Average loss: 5.8287\n",
      "Iteration: 1443; Percent complete: 72.2%; Average loss: 5.7387\n",
      "Iteration: 1444; Percent complete: 72.2%; Average loss: 5.7027\n",
      "Iteration: 1445; Percent complete: 72.2%; Average loss: 5.7938\n",
      "Iteration: 1446; Percent complete: 72.3%; Average loss: 5.8638\n",
      "Iteration: 1447; Percent complete: 72.4%; Average loss: 6.1176\n",
      "Iteration: 1448; Percent complete: 72.4%; Average loss: 5.6950\n",
      "Iteration: 1449; Percent complete: 72.5%; Average loss: 6.0609\n",
      "Iteration: 1450; Percent complete: 72.5%; Average loss: 6.2617\n",
      "Iteration: 1451; Percent complete: 72.5%; Average loss: 5.8728\n",
      "Iteration: 1452; Percent complete: 72.6%; Average loss: 6.4944\n",
      "Iteration: 1453; Percent complete: 72.7%; Average loss: 5.8798\n",
      "Iteration: 1454; Percent complete: 72.7%; Average loss: 6.3852\n",
      "Iteration: 1455; Percent complete: 72.8%; Average loss: 6.2186\n",
      "Iteration: 1456; Percent complete: 72.8%; Average loss: 5.7161\n",
      "Iteration: 1457; Percent complete: 72.9%; Average loss: 5.9234\n",
      "Iteration: 1458; Percent complete: 72.9%; Average loss: 6.0358\n",
      "Iteration: 1459; Percent complete: 73.0%; Average loss: 5.6888\n",
      "Iteration: 1460; Percent complete: 73.0%; Average loss: 5.8404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1461; Percent complete: 73.0%; Average loss: 5.8277\n",
      "Iteration: 1462; Percent complete: 73.1%; Average loss: 5.8263\n",
      "Iteration: 1463; Percent complete: 73.2%; Average loss: 5.6594\n",
      "Iteration: 1464; Percent complete: 73.2%; Average loss: 5.6630\n",
      "Iteration: 1465; Percent complete: 73.2%; Average loss: 5.7245\n",
      "Iteration: 1466; Percent complete: 73.3%; Average loss: 5.4774\n",
      "Iteration: 1467; Percent complete: 73.4%; Average loss: 5.2760\n",
      "Iteration: 1468; Percent complete: 73.4%; Average loss: 6.0352\n",
      "Iteration: 1469; Percent complete: 73.5%; Average loss: 5.5757\n",
      "Iteration: 1470; Percent complete: 73.5%; Average loss: 5.7464\n",
      "Iteration: 1471; Percent complete: 73.6%; Average loss: 5.8538\n",
      "Iteration: 1472; Percent complete: 73.6%; Average loss: 6.3362\n",
      "Iteration: 1473; Percent complete: 73.7%; Average loss: 5.5279\n",
      "Iteration: 1474; Percent complete: 73.7%; Average loss: 5.3281\n",
      "Iteration: 1475; Percent complete: 73.8%; Average loss: 6.2315\n",
      "Iteration: 1476; Percent complete: 73.8%; Average loss: 5.8953\n",
      "Iteration: 1477; Percent complete: 73.9%; Average loss: 5.2519\n",
      "Iteration: 1478; Percent complete: 73.9%; Average loss: 6.3955\n",
      "Iteration: 1479; Percent complete: 74.0%; Average loss: 6.3748\n",
      "Iteration: 1480; Percent complete: 74.0%; Average loss: 5.6854\n",
      "Iteration: 1481; Percent complete: 74.1%; Average loss: 6.0657\n",
      "Iteration: 1482; Percent complete: 74.1%; Average loss: 5.9101\n",
      "Iteration: 1483; Percent complete: 74.2%; Average loss: 5.4415\n",
      "Iteration: 1484; Percent complete: 74.2%; Average loss: 6.1584\n",
      "Iteration: 1485; Percent complete: 74.2%; Average loss: 5.4664\n",
      "Iteration: 1486; Percent complete: 74.3%; Average loss: 5.6757\n",
      "Iteration: 1487; Percent complete: 74.4%; Average loss: 6.6206\n",
      "Iteration: 1488; Percent complete: 74.4%; Average loss: 5.7928\n",
      "Iteration: 1489; Percent complete: 74.5%; Average loss: 5.5211\n",
      "Iteration: 1490; Percent complete: 74.5%; Average loss: 6.2266\n",
      "Iteration: 1491; Percent complete: 74.6%; Average loss: 5.6629\n",
      "Iteration: 1492; Percent complete: 74.6%; Average loss: 5.7252\n",
      "Iteration: 1493; Percent complete: 74.7%; Average loss: 5.6632\n",
      "Iteration: 1494; Percent complete: 74.7%; Average loss: 5.3007\n",
      "Iteration: 1495; Percent complete: 74.8%; Average loss: 5.9576\n",
      "Iteration: 1496; Percent complete: 74.8%; Average loss: 5.3524\n",
      "Iteration: 1497; Percent complete: 74.9%; Average loss: 6.0998\n",
      "Iteration: 1498; Percent complete: 74.9%; Average loss: 5.2731\n",
      "Iteration: 1499; Percent complete: 75.0%; Average loss: 5.8796\n",
      "Iteration: 1500; Percent complete: 75.0%; Average loss: 5.6876\n",
      "Iteration: 1501; Percent complete: 75.0%; Average loss: 5.4607\n",
      "Iteration: 1502; Percent complete: 75.1%; Average loss: 5.2664\n",
      "Iteration: 1503; Percent complete: 75.1%; Average loss: 5.7767\n",
      "Iteration: 1504; Percent complete: 75.2%; Average loss: 5.5591\n",
      "Iteration: 1505; Percent complete: 75.2%; Average loss: 5.6449\n",
      "Iteration: 1506; Percent complete: 75.3%; Average loss: 5.9346\n",
      "Iteration: 1507; Percent complete: 75.3%; Average loss: 5.4612\n",
      "Iteration: 1508; Percent complete: 75.4%; Average loss: 5.8745\n",
      "Iteration: 1509; Percent complete: 75.4%; Average loss: 5.3833\n",
      "Iteration: 1510; Percent complete: 75.5%; Average loss: 6.2632\n",
      "Iteration: 1511; Percent complete: 75.5%; Average loss: 5.5150\n",
      "Iteration: 1512; Percent complete: 75.6%; Average loss: 5.3820\n",
      "Iteration: 1513; Percent complete: 75.6%; Average loss: 5.5058\n",
      "Iteration: 1514; Percent complete: 75.7%; Average loss: 5.6375\n",
      "Iteration: 1515; Percent complete: 75.8%; Average loss: 5.2529\n",
      "Iteration: 1516; Percent complete: 75.8%; Average loss: 6.3477\n",
      "Iteration: 1517; Percent complete: 75.8%; Average loss: 6.0564\n",
      "Iteration: 1518; Percent complete: 75.9%; Average loss: 5.3492\n",
      "Iteration: 1519; Percent complete: 75.9%; Average loss: 5.4626\n",
      "Iteration: 1520; Percent complete: 76.0%; Average loss: 5.9836\n",
      "Iteration: 1521; Percent complete: 76.0%; Average loss: 5.8016\n",
      "Iteration: 1522; Percent complete: 76.1%; Average loss: 5.6129\n",
      "Iteration: 1523; Percent complete: 76.1%; Average loss: 5.8996\n",
      "Iteration: 1524; Percent complete: 76.2%; Average loss: 5.3148\n",
      "Iteration: 1525; Percent complete: 76.2%; Average loss: 5.7656\n",
      "Iteration: 1526; Percent complete: 76.3%; Average loss: 5.4262\n",
      "Iteration: 1527; Percent complete: 76.3%; Average loss: 5.1230\n",
      "Iteration: 1528; Percent complete: 76.4%; Average loss: 5.7719\n",
      "Iteration: 1529; Percent complete: 76.4%; Average loss: 5.2044\n",
      "Iteration: 1530; Percent complete: 76.5%; Average loss: 5.9462\n",
      "Iteration: 1531; Percent complete: 76.5%; Average loss: 5.5943\n",
      "Iteration: 1532; Percent complete: 76.6%; Average loss: 5.8225\n",
      "Iteration: 1533; Percent complete: 76.6%; Average loss: 5.2013\n",
      "Iteration: 1534; Percent complete: 76.7%; Average loss: 5.9344\n",
      "Iteration: 1535; Percent complete: 76.8%; Average loss: 5.0764\n",
      "Iteration: 1536; Percent complete: 76.8%; Average loss: 5.5846\n",
      "Iteration: 1537; Percent complete: 76.8%; Average loss: 5.3495\n",
      "Iteration: 1538; Percent complete: 76.9%; Average loss: 5.7487\n",
      "Iteration: 1539; Percent complete: 77.0%; Average loss: 5.4593\n",
      "Iteration: 1540; Percent complete: 77.0%; Average loss: 5.3693\n",
      "Iteration: 1541; Percent complete: 77.0%; Average loss: 5.1708\n",
      "Iteration: 1542; Percent complete: 77.1%; Average loss: 5.3185\n",
      "Iteration: 1543; Percent complete: 77.1%; Average loss: 5.6540\n",
      "Iteration: 1544; Percent complete: 77.2%; Average loss: 5.8888\n",
      "Iteration: 1545; Percent complete: 77.2%; Average loss: 5.6120\n",
      "Iteration: 1546; Percent complete: 77.3%; Average loss: 5.6500\n",
      "Iteration: 1547; Percent complete: 77.3%; Average loss: 5.1331\n",
      "Iteration: 1548; Percent complete: 77.4%; Average loss: 5.2892\n",
      "Iteration: 1549; Percent complete: 77.5%; Average loss: 5.5712\n",
      "Iteration: 1550; Percent complete: 77.5%; Average loss: 5.3588\n",
      "Iteration: 1551; Percent complete: 77.5%; Average loss: 6.1017\n",
      "Iteration: 1552; Percent complete: 77.6%; Average loss: 5.8894\n",
      "Iteration: 1553; Percent complete: 77.6%; Average loss: 5.6201\n",
      "Iteration: 1554; Percent complete: 77.7%; Average loss: 5.5913\n",
      "Iteration: 1555; Percent complete: 77.8%; Average loss: 5.5021\n",
      "Iteration: 1556; Percent complete: 77.8%; Average loss: 5.5763\n",
      "Iteration: 1557; Percent complete: 77.8%; Average loss: 5.0084\n",
      "Iteration: 1558; Percent complete: 77.9%; Average loss: 5.4853\n",
      "Iteration: 1559; Percent complete: 78.0%; Average loss: 5.3426\n",
      "Iteration: 1560; Percent complete: 78.0%; Average loss: 5.8259\n",
      "Iteration: 1561; Percent complete: 78.0%; Average loss: 5.4670\n",
      "Iteration: 1562; Percent complete: 78.1%; Average loss: 5.7986\n",
      "Iteration: 1563; Percent complete: 78.1%; Average loss: 5.6904\n",
      "Iteration: 1564; Percent complete: 78.2%; Average loss: 5.5338\n",
      "Iteration: 1565; Percent complete: 78.2%; Average loss: 5.4219\n",
      "Iteration: 1566; Percent complete: 78.3%; Average loss: 5.5355\n",
      "Iteration: 1567; Percent complete: 78.3%; Average loss: 5.4054\n",
      "Iteration: 1568; Percent complete: 78.4%; Average loss: 5.7922\n",
      "Iteration: 1569; Percent complete: 78.5%; Average loss: 5.4112\n",
      "Iteration: 1570; Percent complete: 78.5%; Average loss: 4.8248\n",
      "Iteration: 1571; Percent complete: 78.5%; Average loss: 5.2398\n",
      "Iteration: 1572; Percent complete: 78.6%; Average loss: 5.4736\n",
      "Iteration: 1573; Percent complete: 78.6%; Average loss: 4.6539\n",
      "Iteration: 1574; Percent complete: 78.7%; Average loss: 5.7069\n",
      "Iteration: 1575; Percent complete: 78.8%; Average loss: 5.7848\n",
      "Iteration: 1576; Percent complete: 78.8%; Average loss: 5.3834\n",
      "Iteration: 1577; Percent complete: 78.8%; Average loss: 5.7047\n",
      "Iteration: 1578; Percent complete: 78.9%; Average loss: 5.3524\n",
      "Iteration: 1579; Percent complete: 79.0%; Average loss: 5.2534\n",
      "Iteration: 1580; Percent complete: 79.0%; Average loss: 5.7359\n",
      "Iteration: 1581; Percent complete: 79.0%; Average loss: 5.4983\n",
      "Iteration: 1582; Percent complete: 79.1%; Average loss: 5.5209\n",
      "Iteration: 1583; Percent complete: 79.1%; Average loss: 5.0139\n",
      "Iteration: 1584; Percent complete: 79.2%; Average loss: 5.5285\n",
      "Iteration: 1585; Percent complete: 79.2%; Average loss: 5.4532\n",
      "Iteration: 1586; Percent complete: 79.3%; Average loss: 5.1907\n",
      "Iteration: 1587; Percent complete: 79.3%; Average loss: 5.3795\n",
      "Iteration: 1588; Percent complete: 79.4%; Average loss: 5.2390\n",
      "Iteration: 1589; Percent complete: 79.5%; Average loss: 5.2115\n",
      "Iteration: 1590; Percent complete: 79.5%; Average loss: 5.6334\n",
      "Iteration: 1591; Percent complete: 79.5%; Average loss: 5.2708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1592; Percent complete: 79.6%; Average loss: 5.3542\n",
      "Iteration: 1593; Percent complete: 79.7%; Average loss: 5.2039\n",
      "Iteration: 1594; Percent complete: 79.7%; Average loss: 5.2791\n",
      "Iteration: 1595; Percent complete: 79.8%; Average loss: 5.3808\n",
      "Iteration: 1596; Percent complete: 79.8%; Average loss: 5.5323\n",
      "Iteration: 1597; Percent complete: 79.8%; Average loss: 5.4665\n",
      "Iteration: 1598; Percent complete: 79.9%; Average loss: 4.8319\n",
      "Iteration: 1599; Percent complete: 80.0%; Average loss: 5.2433\n",
      "Iteration: 1600; Percent complete: 80.0%; Average loss: 6.1415\n",
      "Iteration: 1601; Percent complete: 80.0%; Average loss: 5.4927\n",
      "Iteration: 1602; Percent complete: 80.1%; Average loss: 5.4095\n",
      "Iteration: 1603; Percent complete: 80.2%; Average loss: 4.8277\n",
      "Iteration: 1604; Percent complete: 80.2%; Average loss: 5.3002\n",
      "Iteration: 1605; Percent complete: 80.2%; Average loss: 5.3460\n",
      "Iteration: 1606; Percent complete: 80.3%; Average loss: 5.3361\n",
      "Iteration: 1607; Percent complete: 80.3%; Average loss: 5.5133\n",
      "Iteration: 1608; Percent complete: 80.4%; Average loss: 5.9921\n",
      "Iteration: 1609; Percent complete: 80.5%; Average loss: 5.3163\n",
      "Iteration: 1610; Percent complete: 80.5%; Average loss: 5.0703\n",
      "Iteration: 1611; Percent complete: 80.5%; Average loss: 5.5018\n",
      "Iteration: 1612; Percent complete: 80.6%; Average loss: 5.5922\n",
      "Iteration: 1613; Percent complete: 80.7%; Average loss: 5.2389\n",
      "Iteration: 1614; Percent complete: 80.7%; Average loss: 6.0228\n",
      "Iteration: 1615; Percent complete: 80.8%; Average loss: 5.0412\n",
      "Iteration: 1616; Percent complete: 80.8%; Average loss: 5.2516\n",
      "Iteration: 1617; Percent complete: 80.8%; Average loss: 5.4921\n",
      "Iteration: 1618; Percent complete: 80.9%; Average loss: 4.8534\n",
      "Iteration: 1619; Percent complete: 81.0%; Average loss: 5.0613\n",
      "Iteration: 1620; Percent complete: 81.0%; Average loss: 5.0180\n",
      "Iteration: 1621; Percent complete: 81.0%; Average loss: 5.2158\n",
      "Iteration: 1622; Percent complete: 81.1%; Average loss: 5.4371\n",
      "Iteration: 1623; Percent complete: 81.2%; Average loss: 5.2529\n",
      "Iteration: 1624; Percent complete: 81.2%; Average loss: 5.2416\n",
      "Iteration: 1625; Percent complete: 81.2%; Average loss: 5.4079\n",
      "Iteration: 1626; Percent complete: 81.3%; Average loss: 5.5982\n",
      "Iteration: 1627; Percent complete: 81.3%; Average loss: 5.4813\n",
      "Iteration: 1628; Percent complete: 81.4%; Average loss: 5.1504\n",
      "Iteration: 1629; Percent complete: 81.5%; Average loss: 5.1445\n",
      "Iteration: 1630; Percent complete: 81.5%; Average loss: 5.4334\n",
      "Iteration: 1631; Percent complete: 81.5%; Average loss: 5.4609\n",
      "Iteration: 1632; Percent complete: 81.6%; Average loss: 5.3595\n",
      "Iteration: 1633; Percent complete: 81.7%; Average loss: 4.4872\n",
      "Iteration: 1634; Percent complete: 81.7%; Average loss: 4.9557\n",
      "Iteration: 1635; Percent complete: 81.8%; Average loss: 5.4784\n",
      "Iteration: 1636; Percent complete: 81.8%; Average loss: 5.3787\n",
      "Iteration: 1637; Percent complete: 81.8%; Average loss: 4.9013\n",
      "Iteration: 1638; Percent complete: 81.9%; Average loss: 5.0673\n",
      "Iteration: 1639; Percent complete: 82.0%; Average loss: 5.4500\n",
      "Iteration: 1640; Percent complete: 82.0%; Average loss: 5.2487\n",
      "Iteration: 1641; Percent complete: 82.0%; Average loss: 5.1360\n",
      "Iteration: 1642; Percent complete: 82.1%; Average loss: 5.4781\n",
      "Iteration: 1643; Percent complete: 82.2%; Average loss: 5.3864\n",
      "Iteration: 1644; Percent complete: 82.2%; Average loss: 4.7564\n",
      "Iteration: 1645; Percent complete: 82.2%; Average loss: 4.8552\n",
      "Iteration: 1646; Percent complete: 82.3%; Average loss: 4.8462\n",
      "Iteration: 1647; Percent complete: 82.3%; Average loss: 4.7894\n",
      "Iteration: 1648; Percent complete: 82.4%; Average loss: 5.6001\n",
      "Iteration: 1649; Percent complete: 82.5%; Average loss: 4.8283\n",
      "Iteration: 1650; Percent complete: 82.5%; Average loss: 5.1918\n",
      "Iteration: 1651; Percent complete: 82.5%; Average loss: 4.9662\n",
      "Iteration: 1652; Percent complete: 82.6%; Average loss: 5.4544\n",
      "Iteration: 1653; Percent complete: 82.7%; Average loss: 5.1386\n",
      "Iteration: 1654; Percent complete: 82.7%; Average loss: 5.3431\n",
      "Iteration: 1655; Percent complete: 82.8%; Average loss: 4.8178\n",
      "Iteration: 1656; Percent complete: 82.8%; Average loss: 5.1138\n",
      "Iteration: 1657; Percent complete: 82.8%; Average loss: 5.2791\n",
      "Iteration: 1658; Percent complete: 82.9%; Average loss: 4.8249\n",
      "Iteration: 1659; Percent complete: 83.0%; Average loss: 5.4625\n",
      "Iteration: 1660; Percent complete: 83.0%; Average loss: 5.0146\n",
      "Iteration: 1661; Percent complete: 83.0%; Average loss: 5.5868\n",
      "Iteration: 1662; Percent complete: 83.1%; Average loss: 5.3707\n",
      "Iteration: 1663; Percent complete: 83.2%; Average loss: 5.6156\n",
      "Iteration: 1664; Percent complete: 83.2%; Average loss: 5.3217\n",
      "Iteration: 1665; Percent complete: 83.2%; Average loss: 5.1405\n",
      "Iteration: 1666; Percent complete: 83.3%; Average loss: 5.1117\n",
      "Iteration: 1667; Percent complete: 83.4%; Average loss: 5.1250\n",
      "Iteration: 1668; Percent complete: 83.4%; Average loss: 5.4089\n",
      "Iteration: 1669; Percent complete: 83.5%; Average loss: 5.1990\n",
      "Iteration: 1670; Percent complete: 83.5%; Average loss: 4.6830\n",
      "Iteration: 1671; Percent complete: 83.5%; Average loss: 5.1619\n",
      "Iteration: 1672; Percent complete: 83.6%; Average loss: 4.8919\n",
      "Iteration: 1673; Percent complete: 83.7%; Average loss: 5.1831\n",
      "Iteration: 1674; Percent complete: 83.7%; Average loss: 4.7171\n",
      "Iteration: 1675; Percent complete: 83.8%; Average loss: 5.3132\n",
      "Iteration: 1676; Percent complete: 83.8%; Average loss: 4.8126\n",
      "Iteration: 1677; Percent complete: 83.9%; Average loss: 5.4399\n",
      "Iteration: 1678; Percent complete: 83.9%; Average loss: 4.6885\n",
      "Iteration: 1679; Percent complete: 84.0%; Average loss: 4.8537\n",
      "Iteration: 1680; Percent complete: 84.0%; Average loss: 5.4568\n",
      "Iteration: 1681; Percent complete: 84.0%; Average loss: 5.2150\n",
      "Iteration: 1682; Percent complete: 84.1%; Average loss: 4.9842\n",
      "Iteration: 1683; Percent complete: 84.2%; Average loss: 4.9562\n",
      "Iteration: 1684; Percent complete: 84.2%; Average loss: 4.8978\n",
      "Iteration: 1685; Percent complete: 84.2%; Average loss: 4.9677\n",
      "Iteration: 1686; Percent complete: 84.3%; Average loss: 5.1858\n",
      "Iteration: 1687; Percent complete: 84.4%; Average loss: 4.9590\n",
      "Iteration: 1688; Percent complete: 84.4%; Average loss: 5.4421\n",
      "Iteration: 1689; Percent complete: 84.5%; Average loss: 4.8613\n",
      "Iteration: 1690; Percent complete: 84.5%; Average loss: 4.7525\n",
      "Iteration: 1691; Percent complete: 84.5%; Average loss: 4.9360\n",
      "Iteration: 1692; Percent complete: 84.6%; Average loss: 4.7489\n",
      "Iteration: 1693; Percent complete: 84.7%; Average loss: 4.9490\n",
      "Iteration: 1694; Percent complete: 84.7%; Average loss: 5.1768\n",
      "Iteration: 1695; Percent complete: 84.8%; Average loss: 5.3317\n",
      "Iteration: 1696; Percent complete: 84.8%; Average loss: 5.2086\n",
      "Iteration: 1697; Percent complete: 84.9%; Average loss: 4.9828\n",
      "Iteration: 1698; Percent complete: 84.9%; Average loss: 5.1124\n",
      "Iteration: 1699; Percent complete: 85.0%; Average loss: 5.3768\n",
      "Iteration: 1700; Percent complete: 85.0%; Average loss: 4.7318\n",
      "Iteration: 1701; Percent complete: 85.0%; Average loss: 5.1538\n",
      "Iteration: 1702; Percent complete: 85.1%; Average loss: 5.2442\n",
      "Iteration: 1703; Percent complete: 85.2%; Average loss: 4.8743\n",
      "Iteration: 1704; Percent complete: 85.2%; Average loss: 5.7200\n",
      "Iteration: 1705; Percent complete: 85.2%; Average loss: 5.2702\n",
      "Iteration: 1706; Percent complete: 85.3%; Average loss: 4.8252\n",
      "Iteration: 1707; Percent complete: 85.4%; Average loss: 5.3486\n",
      "Iteration: 1708; Percent complete: 85.4%; Average loss: 5.0759\n",
      "Iteration: 1709; Percent complete: 85.5%; Average loss: 5.1074\n",
      "Iteration: 1710; Percent complete: 85.5%; Average loss: 4.9037\n",
      "Iteration: 1711; Percent complete: 85.5%; Average loss: 5.2629\n",
      "Iteration: 1712; Percent complete: 85.6%; Average loss: 5.0290\n",
      "Iteration: 1713; Percent complete: 85.7%; Average loss: 5.0283\n",
      "Iteration: 1714; Percent complete: 85.7%; Average loss: 5.4035\n",
      "Iteration: 1715; Percent complete: 85.8%; Average loss: 4.7801\n",
      "Iteration: 1716; Percent complete: 85.8%; Average loss: 5.1939\n",
      "Iteration: 1717; Percent complete: 85.9%; Average loss: 4.8979\n",
      "Iteration: 1718; Percent complete: 85.9%; Average loss: 4.6630\n",
      "Iteration: 1719; Percent complete: 86.0%; Average loss: 5.2304\n",
      "Iteration: 1720; Percent complete: 86.0%; Average loss: 5.0234\n",
      "Iteration: 1721; Percent complete: 86.1%; Average loss: 5.1586\n",
      "Iteration: 1722; Percent complete: 86.1%; Average loss: 5.4445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1723; Percent complete: 86.2%; Average loss: 4.9667\n",
      "Iteration: 1724; Percent complete: 86.2%; Average loss: 4.7104\n",
      "Iteration: 1725; Percent complete: 86.2%; Average loss: 4.8947\n",
      "Iteration: 1726; Percent complete: 86.3%; Average loss: 5.1731\n",
      "Iteration: 1727; Percent complete: 86.4%; Average loss: 4.6534\n",
      "Iteration: 1728; Percent complete: 86.4%; Average loss: 5.1016\n",
      "Iteration: 1729; Percent complete: 86.5%; Average loss: 4.8400\n",
      "Iteration: 1730; Percent complete: 86.5%; Average loss: 5.3826\n",
      "Iteration: 1731; Percent complete: 86.6%; Average loss: 5.3960\n",
      "Iteration: 1732; Percent complete: 86.6%; Average loss: 4.9792\n",
      "Iteration: 1733; Percent complete: 86.7%; Average loss: 4.9727\n",
      "Iteration: 1734; Percent complete: 86.7%; Average loss: 4.9468\n",
      "Iteration: 1735; Percent complete: 86.8%; Average loss: 5.4517\n",
      "Iteration: 1736; Percent complete: 86.8%; Average loss: 5.0913\n",
      "Iteration: 1737; Percent complete: 86.9%; Average loss: 5.3407\n",
      "Iteration: 1738; Percent complete: 86.9%; Average loss: 5.3208\n",
      "Iteration: 1739; Percent complete: 87.0%; Average loss: 4.8568\n",
      "Iteration: 1740; Percent complete: 87.0%; Average loss: 5.2841\n",
      "Iteration: 1741; Percent complete: 87.1%; Average loss: 5.5619\n",
      "Iteration: 1742; Percent complete: 87.1%; Average loss: 5.2960\n",
      "Iteration: 1743; Percent complete: 87.2%; Average loss: 4.8610\n",
      "Iteration: 1744; Percent complete: 87.2%; Average loss: 4.7792\n",
      "Iteration: 1745; Percent complete: 87.2%; Average loss: 4.8414\n",
      "Iteration: 1746; Percent complete: 87.3%; Average loss: 5.2325\n",
      "Iteration: 1747; Percent complete: 87.4%; Average loss: 5.0154\n",
      "Iteration: 1748; Percent complete: 87.4%; Average loss: 5.7134\n",
      "Iteration: 1749; Percent complete: 87.5%; Average loss: 5.2287\n",
      "Iteration: 1750; Percent complete: 87.5%; Average loss: 5.2956\n",
      "Iteration: 1751; Percent complete: 87.5%; Average loss: 5.1828\n",
      "Iteration: 1752; Percent complete: 87.6%; Average loss: 4.5603\n",
      "Iteration: 1753; Percent complete: 87.6%; Average loss: 5.0867\n",
      "Iteration: 1754; Percent complete: 87.7%; Average loss: 5.1983\n",
      "Iteration: 1755; Percent complete: 87.8%; Average loss: 4.9670\n",
      "Iteration: 1756; Percent complete: 87.8%; Average loss: 4.6521\n",
      "Iteration: 1757; Percent complete: 87.8%; Average loss: 5.3462\n",
      "Iteration: 1758; Percent complete: 87.9%; Average loss: 4.9301\n",
      "Iteration: 1759; Percent complete: 87.9%; Average loss: 5.2241\n",
      "Iteration: 1760; Percent complete: 88.0%; Average loss: 4.7175\n",
      "Iteration: 1761; Percent complete: 88.0%; Average loss: 5.0147\n",
      "Iteration: 1762; Percent complete: 88.1%; Average loss: 5.4309\n",
      "Iteration: 1763; Percent complete: 88.1%; Average loss: 5.0482\n",
      "Iteration: 1764; Percent complete: 88.2%; Average loss: 5.0487\n",
      "Iteration: 1765; Percent complete: 88.2%; Average loss: 5.0325\n",
      "Iteration: 1766; Percent complete: 88.3%; Average loss: 4.7252\n",
      "Iteration: 1767; Percent complete: 88.3%; Average loss: 4.5520\n",
      "Iteration: 1768; Percent complete: 88.4%; Average loss: 5.0353\n",
      "Iteration: 1769; Percent complete: 88.4%; Average loss: 5.3215\n",
      "Iteration: 1770; Percent complete: 88.5%; Average loss: 4.9571\n",
      "Iteration: 1771; Percent complete: 88.5%; Average loss: 4.9702\n",
      "Iteration: 1772; Percent complete: 88.6%; Average loss: 4.5656\n",
      "Iteration: 1773; Percent complete: 88.6%; Average loss: 4.7601\n",
      "Iteration: 1774; Percent complete: 88.7%; Average loss: 4.3383\n",
      "Iteration: 1775; Percent complete: 88.8%; Average loss: 4.7160\n",
      "Iteration: 1776; Percent complete: 88.8%; Average loss: 5.2496\n",
      "Iteration: 1777; Percent complete: 88.8%; Average loss: 4.8498\n",
      "Iteration: 1778; Percent complete: 88.9%; Average loss: 5.0804\n",
      "Iteration: 1779; Percent complete: 88.9%; Average loss: 4.9842\n",
      "Iteration: 1780; Percent complete: 89.0%; Average loss: 4.7236\n",
      "Iteration: 1781; Percent complete: 89.0%; Average loss: 4.4993\n",
      "Iteration: 1782; Percent complete: 89.1%; Average loss: 4.8291\n",
      "Iteration: 1783; Percent complete: 89.1%; Average loss: 5.2146\n",
      "Iteration: 1784; Percent complete: 89.2%; Average loss: 4.3272\n",
      "Iteration: 1785; Percent complete: 89.2%; Average loss: 5.0477\n",
      "Iteration: 1786; Percent complete: 89.3%; Average loss: 4.6416\n",
      "Iteration: 1787; Percent complete: 89.3%; Average loss: 4.8114\n",
      "Iteration: 1788; Percent complete: 89.4%; Average loss: 5.0749\n",
      "Iteration: 1789; Percent complete: 89.5%; Average loss: 4.9556\n",
      "Iteration: 1790; Percent complete: 89.5%; Average loss: 4.7390\n",
      "Iteration: 1791; Percent complete: 89.5%; Average loss: 4.9217\n",
      "Iteration: 1792; Percent complete: 89.6%; Average loss: 4.8157\n",
      "Iteration: 1793; Percent complete: 89.6%; Average loss: 5.5194\n",
      "Iteration: 1794; Percent complete: 89.7%; Average loss: 4.9398\n",
      "Iteration: 1795; Percent complete: 89.8%; Average loss: 4.7751\n",
      "Iteration: 1796; Percent complete: 89.8%; Average loss: 4.9307\n",
      "Iteration: 1797; Percent complete: 89.8%; Average loss: 4.5244\n",
      "Iteration: 1798; Percent complete: 89.9%; Average loss: 5.3390\n",
      "Iteration: 1799; Percent complete: 90.0%; Average loss: 4.9576\n",
      "Iteration: 1800; Percent complete: 90.0%; Average loss: 5.0945\n",
      "Iteration: 1801; Percent complete: 90.0%; Average loss: 5.1484\n",
      "Iteration: 1802; Percent complete: 90.1%; Average loss: 5.0430\n",
      "Iteration: 1803; Percent complete: 90.1%; Average loss: 5.0309\n",
      "Iteration: 1804; Percent complete: 90.2%; Average loss: 4.4134\n",
      "Iteration: 1805; Percent complete: 90.2%; Average loss: 4.7432\n",
      "Iteration: 1806; Percent complete: 90.3%; Average loss: 4.8574\n",
      "Iteration: 1807; Percent complete: 90.3%; Average loss: 4.5615\n",
      "Iteration: 1808; Percent complete: 90.4%; Average loss: 4.6142\n",
      "Iteration: 1809; Percent complete: 90.5%; Average loss: 4.6889\n",
      "Iteration: 1810; Percent complete: 90.5%; Average loss: 4.6974\n",
      "Iteration: 1811; Percent complete: 90.5%; Average loss: 5.1328\n",
      "Iteration: 1812; Percent complete: 90.6%; Average loss: 4.6554\n",
      "Iteration: 1813; Percent complete: 90.6%; Average loss: 4.8912\n",
      "Iteration: 1814; Percent complete: 90.7%; Average loss: 4.8910\n",
      "Iteration: 1815; Percent complete: 90.8%; Average loss: 4.9062\n",
      "Iteration: 1816; Percent complete: 90.8%; Average loss: 4.4693\n",
      "Iteration: 1817; Percent complete: 90.8%; Average loss: 4.9839\n",
      "Iteration: 1818; Percent complete: 90.9%; Average loss: 4.7050\n",
      "Iteration: 1819; Percent complete: 91.0%; Average loss: 4.4474\n",
      "Iteration: 1820; Percent complete: 91.0%; Average loss: 4.8579\n",
      "Iteration: 1821; Percent complete: 91.0%; Average loss: 4.8410\n",
      "Iteration: 1822; Percent complete: 91.1%; Average loss: 4.8664\n",
      "Iteration: 1823; Percent complete: 91.1%; Average loss: 4.6120\n",
      "Iteration: 1824; Percent complete: 91.2%; Average loss: 4.9244\n",
      "Iteration: 1825; Percent complete: 91.2%; Average loss: 5.0615\n",
      "Iteration: 1826; Percent complete: 91.3%; Average loss: 4.4585\n",
      "Iteration: 1827; Percent complete: 91.3%; Average loss: 4.6180\n",
      "Iteration: 1828; Percent complete: 91.4%; Average loss: 4.7347\n",
      "Iteration: 1829; Percent complete: 91.5%; Average loss: 4.5254\n",
      "Iteration: 1830; Percent complete: 91.5%; Average loss: 4.2210\n",
      "Iteration: 1831; Percent complete: 91.5%; Average loss: 4.6104\n",
      "Iteration: 1832; Percent complete: 91.6%; Average loss: 5.2471\n",
      "Iteration: 1833; Percent complete: 91.6%; Average loss: 4.5975\n",
      "Iteration: 1834; Percent complete: 91.7%; Average loss: 4.9592\n",
      "Iteration: 1835; Percent complete: 91.8%; Average loss: 4.8041\n",
      "Iteration: 1836; Percent complete: 91.8%; Average loss: 4.7199\n",
      "Iteration: 1837; Percent complete: 91.8%; Average loss: 4.7595\n",
      "Iteration: 1838; Percent complete: 91.9%; Average loss: 4.5014\n",
      "Iteration: 1839; Percent complete: 92.0%; Average loss: 4.8102\n",
      "Iteration: 1840; Percent complete: 92.0%; Average loss: 4.5251\n",
      "Iteration: 1841; Percent complete: 92.0%; Average loss: 4.3807\n",
      "Iteration: 1842; Percent complete: 92.1%; Average loss: 4.7830\n",
      "Iteration: 1843; Percent complete: 92.2%; Average loss: 4.8088\n",
      "Iteration: 1844; Percent complete: 92.2%; Average loss: 5.0275\n",
      "Iteration: 1845; Percent complete: 92.2%; Average loss: 4.7795\n",
      "Iteration: 1846; Percent complete: 92.3%; Average loss: 5.2387\n",
      "Iteration: 1847; Percent complete: 92.3%; Average loss: 4.8458\n",
      "Iteration: 1848; Percent complete: 92.4%; Average loss: 5.3457\n",
      "Iteration: 1849; Percent complete: 92.5%; Average loss: 5.0728\n",
      "Iteration: 1850; Percent complete: 92.5%; Average loss: 4.9356\n",
      "Iteration: 1851; Percent complete: 92.5%; Average loss: 5.4575\n",
      "Iteration: 1852; Percent complete: 92.6%; Average loss: 5.0715\n",
      "Iteration: 1853; Percent complete: 92.7%; Average loss: 4.4038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1854; Percent complete: 92.7%; Average loss: 4.4120\n",
      "Iteration: 1855; Percent complete: 92.8%; Average loss: 4.6083\n",
      "Iteration: 1856; Percent complete: 92.8%; Average loss: 5.3898\n",
      "Iteration: 1857; Percent complete: 92.8%; Average loss: 4.7744\n",
      "Iteration: 1858; Percent complete: 92.9%; Average loss: 4.8730\n",
      "Iteration: 1859; Percent complete: 93.0%; Average loss: 4.7196\n",
      "Iteration: 1860; Percent complete: 93.0%; Average loss: 4.7832\n",
      "Iteration: 1861; Percent complete: 93.0%; Average loss: 4.6496\n",
      "Iteration: 1862; Percent complete: 93.1%; Average loss: 4.3573\n",
      "Iteration: 1863; Percent complete: 93.2%; Average loss: 4.3935\n",
      "Iteration: 1864; Percent complete: 93.2%; Average loss: 4.8745\n",
      "Iteration: 1865; Percent complete: 93.2%; Average loss: 4.4638\n",
      "Iteration: 1866; Percent complete: 93.3%; Average loss: 4.6509\n",
      "Iteration: 1867; Percent complete: 93.3%; Average loss: 4.9739\n",
      "Iteration: 1868; Percent complete: 93.4%; Average loss: 4.3325\n",
      "Iteration: 1869; Percent complete: 93.5%; Average loss: 4.7198\n",
      "Iteration: 1870; Percent complete: 93.5%; Average loss: 4.2095\n",
      "Iteration: 1871; Percent complete: 93.5%; Average loss: 4.4623\n",
      "Iteration: 1872; Percent complete: 93.6%; Average loss: 4.4789\n",
      "Iteration: 1873; Percent complete: 93.7%; Average loss: 4.4058\n",
      "Iteration: 1874; Percent complete: 93.7%; Average loss: 5.0327\n",
      "Iteration: 1875; Percent complete: 93.8%; Average loss: 4.6546\n",
      "Iteration: 1876; Percent complete: 93.8%; Average loss: 5.1292\n",
      "Iteration: 1877; Percent complete: 93.8%; Average loss: 5.0315\n",
      "Iteration: 1878; Percent complete: 93.9%; Average loss: 4.9184\n",
      "Iteration: 1879; Percent complete: 94.0%; Average loss: 4.3036\n",
      "Iteration: 1880; Percent complete: 94.0%; Average loss: 4.7639\n",
      "Iteration: 1881; Percent complete: 94.0%; Average loss: 4.6387\n",
      "Iteration: 1882; Percent complete: 94.1%; Average loss: 4.5543\n",
      "Iteration: 1883; Percent complete: 94.2%; Average loss: 5.0445\n",
      "Iteration: 1884; Percent complete: 94.2%; Average loss: 4.4960\n",
      "Iteration: 1885; Percent complete: 94.2%; Average loss: 4.7686\n",
      "Iteration: 1886; Percent complete: 94.3%; Average loss: 4.1764\n",
      "Iteration: 1887; Percent complete: 94.3%; Average loss: 5.0706\n",
      "Iteration: 1888; Percent complete: 94.4%; Average loss: 3.9425\n",
      "Iteration: 1889; Percent complete: 94.5%; Average loss: 4.4989\n",
      "Iteration: 1890; Percent complete: 94.5%; Average loss: 4.8182\n",
      "Iteration: 1891; Percent complete: 94.5%; Average loss: 4.4706\n",
      "Iteration: 1892; Percent complete: 94.6%; Average loss: 4.7330\n",
      "Iteration: 1893; Percent complete: 94.7%; Average loss: 4.1709\n",
      "Iteration: 1894; Percent complete: 94.7%; Average loss: 4.7796\n",
      "Iteration: 1895; Percent complete: 94.8%; Average loss: 4.4445\n",
      "Iteration: 1896; Percent complete: 94.8%; Average loss: 4.3103\n",
      "Iteration: 1897; Percent complete: 94.8%; Average loss: 4.4377\n",
      "Iteration: 1898; Percent complete: 94.9%; Average loss: 4.7147\n",
      "Iteration: 1899; Percent complete: 95.0%; Average loss: 4.5833\n",
      "Iteration: 1900; Percent complete: 95.0%; Average loss: 4.3540\n",
      "Iteration: 1901; Percent complete: 95.0%; Average loss: 4.8642\n",
      "Iteration: 1902; Percent complete: 95.1%; Average loss: 4.6137\n",
      "Iteration: 1903; Percent complete: 95.2%; Average loss: 4.8732\n",
      "Iteration: 1904; Percent complete: 95.2%; Average loss: 4.2539\n",
      "Iteration: 1905; Percent complete: 95.2%; Average loss: 5.0380\n",
      "Iteration: 1906; Percent complete: 95.3%; Average loss: 4.1477\n",
      "Iteration: 1907; Percent complete: 95.3%; Average loss: 5.0113\n",
      "Iteration: 1908; Percent complete: 95.4%; Average loss: 4.6656\n",
      "Iteration: 1909; Percent complete: 95.5%; Average loss: 4.3228\n",
      "Iteration: 1910; Percent complete: 95.5%; Average loss: 4.3719\n",
      "Iteration: 1911; Percent complete: 95.5%; Average loss: 4.7049\n",
      "Iteration: 1912; Percent complete: 95.6%; Average loss: 4.6041\n",
      "Iteration: 1913; Percent complete: 95.7%; Average loss: 4.3964\n",
      "Iteration: 1914; Percent complete: 95.7%; Average loss: 4.0935\n",
      "Iteration: 1915; Percent complete: 95.8%; Average loss: 4.8571\n",
      "Iteration: 1916; Percent complete: 95.8%; Average loss: 4.7794\n",
      "Iteration: 1917; Percent complete: 95.9%; Average loss: 4.8222\n",
      "Iteration: 1918; Percent complete: 95.9%; Average loss: 4.1210\n",
      "Iteration: 1919; Percent complete: 96.0%; Average loss: 4.6011\n",
      "Iteration: 1920; Percent complete: 96.0%; Average loss: 4.6981\n",
      "Iteration: 1921; Percent complete: 96.0%; Average loss: 4.7255\n",
      "Iteration: 1922; Percent complete: 96.1%; Average loss: 4.3443\n",
      "Iteration: 1923; Percent complete: 96.2%; Average loss: 4.6911\n",
      "Iteration: 1924; Percent complete: 96.2%; Average loss: 4.8150\n",
      "Iteration: 1925; Percent complete: 96.2%; Average loss: 3.9299\n",
      "Iteration: 1926; Percent complete: 96.3%; Average loss: 4.3259\n",
      "Iteration: 1927; Percent complete: 96.4%; Average loss: 4.6766\n",
      "Iteration: 1928; Percent complete: 96.4%; Average loss: 4.4467\n",
      "Iteration: 1929; Percent complete: 96.5%; Average loss: 5.2095\n",
      "Iteration: 1930; Percent complete: 96.5%; Average loss: 4.6042\n",
      "Iteration: 1931; Percent complete: 96.5%; Average loss: 4.3572\n",
      "Iteration: 1932; Percent complete: 96.6%; Average loss: 4.7077\n",
      "Iteration: 1933; Percent complete: 96.7%; Average loss: 4.3494\n",
      "Iteration: 1934; Percent complete: 96.7%; Average loss: 4.4026\n",
      "Iteration: 1935; Percent complete: 96.8%; Average loss: 4.4817\n",
      "Iteration: 1936; Percent complete: 96.8%; Average loss: 4.1784\n",
      "Iteration: 1937; Percent complete: 96.9%; Average loss: 4.1399\n",
      "Iteration: 1938; Percent complete: 96.9%; Average loss: 4.7917\n",
      "Iteration: 1939; Percent complete: 97.0%; Average loss: 4.2650\n",
      "Iteration: 1940; Percent complete: 97.0%; Average loss: 4.8764\n",
      "Iteration: 1941; Percent complete: 97.0%; Average loss: 4.0745\n",
      "Iteration: 1942; Percent complete: 97.1%; Average loss: 4.1178\n",
      "Iteration: 1943; Percent complete: 97.2%; Average loss: 4.2020\n",
      "Iteration: 1944; Percent complete: 97.2%; Average loss: 4.3324\n",
      "Iteration: 1945; Percent complete: 97.2%; Average loss: 4.8200\n",
      "Iteration: 1946; Percent complete: 97.3%; Average loss: 4.4218\n",
      "Iteration: 1947; Percent complete: 97.4%; Average loss: 4.1508\n",
      "Iteration: 1948; Percent complete: 97.4%; Average loss: 4.0088\n",
      "Iteration: 1949; Percent complete: 97.5%; Average loss: 4.2706\n",
      "Iteration: 1950; Percent complete: 97.5%; Average loss: 4.1490\n",
      "Iteration: 1951; Percent complete: 97.5%; Average loss: 4.1011\n",
      "Iteration: 1952; Percent complete: 97.6%; Average loss: 4.9538\n",
      "Iteration: 1953; Percent complete: 97.7%; Average loss: 4.5485\n",
      "Iteration: 1954; Percent complete: 97.7%; Average loss: 4.4484\n",
      "Iteration: 1955; Percent complete: 97.8%; Average loss: 4.6655\n",
      "Iteration: 1956; Percent complete: 97.8%; Average loss: 4.5126\n",
      "Iteration: 1957; Percent complete: 97.9%; Average loss: 4.6354\n",
      "Iteration: 1958; Percent complete: 97.9%; Average loss: 4.2095\n",
      "Iteration: 1959; Percent complete: 98.0%; Average loss: 4.8414\n",
      "Iteration: 1960; Percent complete: 98.0%; Average loss: 4.4204\n",
      "Iteration: 1961; Percent complete: 98.0%; Average loss: 4.3669\n",
      "Iteration: 1962; Percent complete: 98.1%; Average loss: 4.7869\n",
      "Iteration: 1963; Percent complete: 98.2%; Average loss: 4.6907\n",
      "Iteration: 1964; Percent complete: 98.2%; Average loss: 4.2631\n",
      "Iteration: 1965; Percent complete: 98.2%; Average loss: 4.3178\n",
      "Iteration: 1966; Percent complete: 98.3%; Average loss: 4.5229\n",
      "Iteration: 1967; Percent complete: 98.4%; Average loss: 4.4621\n",
      "Iteration: 1968; Percent complete: 98.4%; Average loss: 4.5368\n",
      "Iteration: 1969; Percent complete: 98.5%; Average loss: 4.4145\n",
      "Iteration: 1970; Percent complete: 98.5%; Average loss: 4.7449\n",
      "Iteration: 1971; Percent complete: 98.6%; Average loss: 4.8730\n",
      "Iteration: 1972; Percent complete: 98.6%; Average loss: 4.8149\n",
      "Iteration: 1973; Percent complete: 98.7%; Average loss: 4.5402\n",
      "Iteration: 1974; Percent complete: 98.7%; Average loss: 3.9643\n",
      "Iteration: 1975; Percent complete: 98.8%; Average loss: 4.2833\n",
      "Iteration: 1976; Percent complete: 98.8%; Average loss: 3.6939\n",
      "Iteration: 1977; Percent complete: 98.9%; Average loss: 4.6703\n",
      "Iteration: 1978; Percent complete: 98.9%; Average loss: 4.2015\n",
      "Iteration: 1979; Percent complete: 99.0%; Average loss: 4.3072\n",
      "Iteration: 1980; Percent complete: 99.0%; Average loss: 4.4554\n",
      "Iteration: 1981; Percent complete: 99.1%; Average loss: 4.3120\n",
      "Iteration: 1982; Percent complete: 99.1%; Average loss: 4.8647\n",
      "Iteration: 1983; Percent complete: 99.2%; Average loss: 4.5902\n",
      "Iteration: 1984; Percent complete: 99.2%; Average loss: 5.1752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1985; Percent complete: 99.2%; Average loss: 4.5590\n",
      "Iteration: 1986; Percent complete: 99.3%; Average loss: 4.5639\n",
      "Iteration: 1987; Percent complete: 99.4%; Average loss: 4.5170\n",
      "Iteration: 1988; Percent complete: 99.4%; Average loss: 4.9153\n",
      "Iteration: 1989; Percent complete: 99.5%; Average loss: 4.8076\n",
      "Iteration: 1990; Percent complete: 99.5%; Average loss: 4.3938\n",
      "Iteration: 1991; Percent complete: 99.6%; Average loss: 4.5889\n",
      "Iteration: 1992; Percent complete: 99.6%; Average loss: 4.8505\n",
      "Iteration: 1993; Percent complete: 99.7%; Average loss: 3.6283\n",
      "Iteration: 1994; Percent complete: 99.7%; Average loss: 3.9676\n",
      "Iteration: 1995; Percent complete: 99.8%; Average loss: 4.6513\n",
      "Iteration: 1996; Percent complete: 99.8%; Average loss: 4.6122\n",
      "Iteration: 1997; Percent complete: 99.9%; Average loss: 4.2987\n",
      "Iteration: 1998; Percent complete: 99.9%; Average loss: 5.0141\n",
      "Iteration: 1999; Percent complete: 100.0%; Average loss: 4.0956\n",
      "Iteration: 2000; Percent complete: 100.0%; Average loss: 4.4517\n"
     ]
    }
   ],
   "source": [
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 2000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Dropout 레이어를 학습 모드로 둡니다\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Optimizer를 초기화합니다\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# 학습 단계를 수행합니다\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 안녕\n",
      "Bot: 예.\"\n",
      "> 인사해\n",
      "Error: Encountered unknown word.\n",
      "> 장보러가니?\n",
      "Error: Encountered unknown word.\n",
      "> 나 우울해\n",
      "Error: Encountered unknown word.\n",
      "> 가\n",
      "Bot: 너 그렇게 일있어?\"\n",
      "> 뭔소리야\n",
      "Error: Encountered unknown word.\n",
      "> 무슨 소리야\n",
      "Bot: 갑자기는 왜 나한테 나한테 나한테 이러면 이러면 튀냐고.\"\n",
      "> 엥?\n",
      "Bot: 너 잠깐 하고 하고 내맘이 너 아직 와요?\"\n",
      "> 뭐래는거니\n",
      "Error: Encountered unknown word.\n",
      "> quit\n"
     ]
    }
   ],
   "source": [
    "# Dropout 레이어를 평가 모드로 설정합니다\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# 탐색 모듈을 초기화합니다\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# 채팅을 시작합니다 (다음 줄의 주석을 제거하면 시작해볼 수 있습니다)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
