{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import itertools\n",
    "import math\n",
    "from torch.jit import script, trace\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "corpus_name = 'cornell_movie_dialogs_corpus'\n",
    "corpus = os.path.join(r'C:\\Users\\abc\\jupyter\\pytorch', corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "        \n",
    "printLines(os.path.join(corpus, 'movie_lines.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLines(fileName, fields):\n",
    "    lines = {}\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            lineObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                lineObj[field] = values[i]\n",
    "                # {'lineID': 'L1045', 'characterID': 'u0', 'movieID': 'm0', 'character': 'BIANCA', 'text': 'They do not!\\n'}튜플형식 저장\n",
    "            lines[lineObj['lineID']] = lineObj \n",
    "            # {'L1045': {'lineID': 'L1045', 'characterID': 'u0', 'movieID': 'm0', character': 'BIANCA', 'text': 'They do not!\\n'}\n",
    "            # 튜플 형식으로 저장\n",
    "    return lines\n",
    "\n",
    "\n",
    "def loadConversations(fileName, lines, fields):\n",
    "    conversations = []\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            convObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                convObj[field] = values[i]\n",
    "                # {'character1ID': 'u0', 'character2ID': 'u2', 'movieID': 'm0', 'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\"}\n",
    "            utterance_id_pattern = re.compile('L[0-9]+')\n",
    "            lineIds = utterance_id_pattern.findall(convObj[\"utteranceIDs\"]) # ['L194', 'L195', 'L196', 'L197']\n",
    "            convObj[\"lines\"] = []\n",
    "            for lineId in lineIds:\n",
    "                convObj[\"lines\"].append(lines[lineId])\n",
    "                # lineId에 맞는 lines저장\n",
    "            conversations.append(convObj)\n",
    "            # {'character1ID': 'u0', 'character2ID': 'u2', 'movieID': 'm0', 'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\", \n",
    "            # 'lines': [{'lineID': 'L194', 'characterID': 'u0', 'movieID': 'm0', 'character': 'BIANCA', \n",
    "            # 'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'}\n",
    "    return conversations\n",
    "\n",
    "\n",
    "\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        for i in range(len(conversation[\"lines\"]) - 1): # len : L194, L195, L196, L197 -> 4\n",
    "            # inputLine과 targetLine으로 나눈 이유는 총4개의 대화에서 3번째 대화에 대해 마지막 4번째가 응답이기 때문에\n",
    "            # 마지막 4번째는 응답할게 없으므로\n",
    "            # 대화의 마지막 대사는 (그에 대한 응답이 없으므로) 무시합니다\n",
    "            # 결과적으로 i0 -> i1, i1->i2, i2->i3, i3->i4\n",
    "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "            \n",
    "            # 잘못된 샘플은 제거(리스트가 하나라도 비어 있는 경우)\n",
    "            if inputLine and targetLine:\n",
    "                qa_pairs.append([inputLine, '[SEP]', targetLine])\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus...\n",
      "\n",
      "Loading conversations...\n",
      "\n",
      "Writing newly formatted file...\n",
      "\n",
      "Sample lines from file:\n",
      "b'\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\" [SEP] \"Well, I thought we\\'d start with pronunciation, if that\\'s okay with you.\"\\r\\n'\n",
      "b'\"Well, I thought we\\'d start with pronunciation, if that\\'s okay with you.\" [SEP] \"Not the hacking and gagging and spitting part.  Please.\"\\r\\n'\n",
      "b'\"Not the hacking and gagging and spitting part.  Please.\" [SEP] \"Okay... then how \\'bout we try out some French cuisine.  Saturday?  Night?\"\\r\\n'\n",
      "b'\"You\\'re asking me out.  That\\'s so cute. What\\'s your name again?\" [SEP] \"Forget it.\"\\r\\n'\n",
      "b'\"No, no, it\\'s my fault -- we didn\\'t have a proper introduction ---\" [SEP] Cameron.\\r\\n'\n",
      "b'Cameron. [SEP] \"The thing is, Cameron -- I\\'m at the mercy of a particularly hideous breed of loser.  My sister.  I can\\'t date until she does.\"\\r\\n'\n",
      "b'\"The thing is, Cameron -- I\\'m at the mercy of a particularly hideous breed of loser.  My sister.  I can\\'t date until she does.\" [SEP] \"Seems like she could get a date easy enough...\"\\r\\n'\n",
      "b'Why? [SEP] \"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\"\\r\\n'\n",
      "b'\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\" [SEP] \"That\\'s a shame.\"\\r\\n'\n",
      "b'\"Gosh, if only we could find Kat a boyfriend...\" [SEP] \"Let me see what I can do.\"\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "# 새로운 파일에 따로 저장\n",
    "datafile = os.path.join(corpus, \"bert_formatted_movie_lines.txt\")\n",
    "\n",
    "delimiter = ' '\n",
    "# 구분자에 대해 unescape 함수를 호출합니다\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "lines = {}\n",
    "conversations = []\n",
    "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "\n",
    "print(\"\\nProcessing corpus...\")\n",
    "lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
    "print(\"\\nLoading conversations...\")\n",
    "conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\n",
    "                                  lines, MOVIE_CONVERSATIONS_FIELDS)\n",
    "\n",
    "# 결과를 새로운 csv 파일로 저장합니다\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extractSentencePairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "\n",
    "# 몇 줄을 예제 삼아 출력해 봅니다\n",
    "print(\"\\nSample lines from file:\")\n",
    "printLines(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "CLS_token = 101\n",
    "SEP_token = 102\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token : 'PAD', SOS_token:'SOS', EOS_token:'EOS', CLS_token : 'CLS', SEP_token : 'SEP'}\n",
    "        self.num_words = 5\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = []\n",
    "        \n",
    "    def addSentence(self, sentence): # sentence : 잘 마실게\n",
    "        sentence = self.tokenizer.encode(sentence)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(sentence)\n",
    "        self.tokens = tokens\n",
    "        # 잘 마실게. [SEP] 오후에 학부모 대표 모임에 사과하러 가는 거 교감선생님만 가시면 안돼요? 전 약속이 있어서..\n",
    "        #for word in tokens:\n",
    "            #self.addWord(word)\n",
    "        #print(tokens)\n",
    "        return tokens            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 221282 sentence pairs\n",
      "Trimmed to 88189 sentence pairs\n",
      "['\"Gosh, if only we could find Kat a boyfriend...\"', '\"Let me see what I can do.\"']\n",
      "['\"C\\'esc ma tete. This is my head\"', '\"Right.  See?  You\\'re ready for the quiz.\"']\n",
      "['\"That\\'s because it\\'s such a nice one.\"', '\"Forget French.\"']\n",
      "['There.', 'Where?']\n",
      "['\"You have my word.  As a gentleman\"', '\"You\\'re sweet.\"']\n",
      "['Hi.', '\"Looks like things worked out tonight, huh?\"']\n",
      "['\"You know Chastity?\"', '\"I believe we share an art instructor\"']\n",
      "['\"Have fun tonight?\"', 'Tons']\n",
      "['\"Well, no...\"', '\"Then that\\'s all you had to say.\"']\n",
      "['\"Then that\\'s all you had to say.\"', 'But']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10  \n",
    "\n",
    "# 유니코드 문자열을 아스키로 변환합니다\n",
    "# https://stackoverflow.com/a/518232/2809427 참고\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# 소문자로 만들고, 공백을 넣고, 알파벳 외의 글자를 제거합니다\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def readVocs(datafile, corpus_name): # corpus_name : chatData / datafile : formatted_ko_conversations.txt\n",
    "    print('Reading lines...')\n",
    "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [l.strip().split(' [SEP] ') for l in lines]\n",
    "    #print(pairs[0])\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs # voc : 문서 단어집합 / pairs : 문장 쌍 집합\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name) # voc : 단어집합, pairs : 질문 쌍\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    for pair in pairs:\n",
    "        sentence1 = voc.addSentence(pair[0].strip())\n",
    "        #print(sentence1)\n",
    "        sentence2 = voc.addSentence(pair[1].strip())\n",
    "    return voc, pairs\n",
    "\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[  101,   101,   101,   101,   101],\n",
      "        [  107,   107,   107,   107, 47464],\n",
      "        [91327, 35936, 12916, 10377,   119],\n",
      "        [13028,   117,   119,   112,   102],\n",
      "        [  119, 25157, 11474,   187,     0],\n",
      "        [  146,   117, 65987, 14616,     0],\n",
      "        [  112, 10105, 10115,   119,     0],\n",
      "        [22469, 20694,   136,   107,     0],\n",
      "        [13009, 14013,   107,   102,     0],\n",
      "        [13028, 28036,   102,     0,     0],\n",
      "        [17367, 10301,     0,     0,     0],\n",
      "        [10114, 21005,     0,     0,     0],\n",
      "        [  118,   106,     0,     0,     0],\n",
      "        [  118,   107,     0,     0,     0],\n",
      "        [  107,   102,     0,     0,     0],\n",
      "        [  102,     0,     0,     0,     0]])\n",
      "lengths: tensor([16, 15, 10,  9,  4])\n",
      "target_variable: tensor([[  101,   101,   101,   101,   101],\n",
      "        [40690,   107, 16423, 13098,   107],\n",
      "        [  119, 12865, 10123, 78067, 12489],\n",
      "        [  102, 14819,   136, 10667, 10149],\n",
      "        [    0, 28086,   102,   136, 13028],\n",
      "        [    0, 11345,     0,   102, 69423],\n",
      "        [    0,   106,     0,     0,   136],\n",
      "        [    0,   107,     0,     0,   107],\n",
      "        [    0,   102,     0,     0,   102]])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 1]], dtype=torch.uint8)\n",
      "max_target_len: 9\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    #print([voc.word2index[word] for word in tokens] + [SEP_token])\n",
    "    return tokenizer.encode(sentence)\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    # print(list(itertools.zip_longest(*l, fillvalue=fillvalue))) : (1957, 2, 2, 2, 0)\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "def inputVar(l, voc): # l : 응. 새벽에도 일찍 나갔어. 온다간다 말도없이.\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l] \n",
    "    # print(indexes_batch) : [[5027, 1239, 9433, 2], [5951, 4686, 1476, 2], [1116, 5309, 2], [319, 2], [186, 2]]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) \n",
    "    # print(lengths) : tensor([4, 4, 3, 2, 2])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    # print(padList) : [[5027, 1239, 9433, 2], [5951, 4686, 1476, 2], [1116, 5309, 2, 0], [319, 2, 0, 0], [186, 2, 0, 0]]\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    #print(indexes_batch)\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch]) \n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList) \n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(tokenizer.tokenize(x[0])), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, \n",
    "                          dropout = (0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq) # input_seq : shape=(max_length, batch_size), input_variable\n",
    "        # print(input_lengths) : =lengths\n",
    "        # print(embedded.shape) : torch.Size([10, 64, 500]) [max_length, batch_size, hidden_size(은닉상태 크기)]\n",
    "        \n",
    "        # nn.utils.rnn.pack_padded_sequence : 패딩연산처리 쉽게하기 위해 중간에 빈공간 제거(형태 : tensor)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # input_lengths : shape=(batch_size)\n",
    "        #print(packed.batch_sizes)\n",
    "        # print(packed.batch_sizes) : tensor([64, 64, 64, 58, 52, 45, 38, 17,  8,  2])\n",
    "        \n",
    "        outputs, hidden = self.gru(packed, hidden) # 입력hidden : shape=(n_layers * num_directions, batch_size, hidden_size)\n",
    "        # print(outputs.batch_sizes) : tensor([64, 64, 63, 52, 47, 34, 24, 18, 12,  6])\n",
    "        # print(hidden.shape) : torch.Size([4, 64, 500]) [층 * 양방향이면2 아니면1, batch_size, hidden_size]\n",
    "        \n",
    "        # nn.utils.rnn.pad_packed_sequence : 패딩연산이 끝난 것을 다시 원래대로 (형태 : torch)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        #print(outputs.shape)\n",
    "        # print(outputs.shape)# : torch.Size([10, 64, 1000]) # [max_length, batch_size, hidden_size(양방향으로 진행했으면 *2)]\n",
    "        \n",
    "        # 양방향 GRU의 출력을 합산합니다\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # print(outputs.shape) : torch.Size([10, 64, 500])\n",
    "        \n",
    "        # hidden : GRU의 최종 은닉 상태\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, 'is not an appropriate attention method.')\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "            \n",
    "    # 가중치 계산을 dot-product로 계산\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        # print(torch.sum(hidden * encoder_output, dim=2).shape) : torch.Size([10, 64]) 10개 생성[max_length, batch_size]\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "    \n",
    "    # \n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        # print(energy.shape) : torch.Size([10, 64, 500]) 10개 생성[max_length, batch_size, hidden_size]\n",
    "        \n",
    "        # print(torch.sum(hidden * energy, dim=2).shape) : torch.Size([10, 64]) 10개 생성[max_length, batch_size]\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "    \n",
    "    \n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        # cat : 합칠 때 차원은 2차원으로 / expand : 확장\n",
    "        # Tanh 함수는 함수값을 [-1, 1]로 제한시킴\n",
    "        # print((hidden.expand(encoder_output.size(0), -1, -1).shape)) : torch.Size([10, 64, 500])\n",
    "        # print(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2).shape) : torch.Size([10, 64, 1000])\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        # print(energy.shape) : torch.Size([10, 64, 500]) 10개 생성[max_length, batch_size, hidden_size]\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "            \n",
    "        attn_energies = attn_energies.t() # t() : 행과 열을 바꿔서 저장[1, 2, 3], [4, 5, 6] -> [1, 4, 7], [2, 5, 8]\n",
    "        \n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # 참조를 보존해 둡니다\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 레이어를 정의합니다\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # 주의: 한 단위 시간에 대해 한 단계(단어)만을 수행합니다\n",
    "        # 현재의 입력 단어에 대한 임베딩을 구합니다   \n",
    "        #print(input_step)\n",
    "        embedded = self.embedding(input_step) # input_step : 입력 시퀀스 배치에 대한 한 단위 시간(한 단어). shape=(1, batch_size)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        #print(embedded.shape)\n",
    "        # print(embedded.shape) : torch.Size([1, 64, 500])\n",
    "        \n",
    "        # 양방향x\n",
    "        # last_hidden : GRU의 마지막 은닉 레이어. shape=(n_layers * num_directions, batch_size, hidden_size)\n",
    "        # print(last_hidden.shape) : torch.Size([2, 64, 500]) \n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden) \n",
    "        # print(rnn_output.shape) : torch.Size([1, 64, 500])\n",
    "        # print(hidden.shape) : torch.Size([2, 64, 500])\n",
    "\n",
    "        # attention 가중치\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs) # encoder_outputs : 인코더 모델 출력 shape=(max_length, batch_size, hidden_size)\n",
    "        # print(attn_weights.shape) : torch.Size([64, 1, 10]) \n",
    "\n",
    "        # 인코더 출력에 어텐션을 곱하여 새로운 context vector생성\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # print(context.shape) : torch.Size([64, 1, 500])\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0) # print(rnn_output.shape) : torch.Size([64, 500])\n",
    "        context = context.squeeze(1) # print(context.shape) : torch.Size([64, 500])\n",
    "        concat_input = torch.cat((rnn_output, context), 1) # print(concat_input.shape) : torch.Size([64, 1000])\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # print(concat_output.shape) : torch.Size([64, 500])\n",
    "\n",
    "        # output : 각 단어가 디코딩된 시퀀스에서 다음 단어로 사용되었을 때 적합할 확률을 나타내는 정규화된 softmax 텐서. \n",
    "        # shape=(batch_size, voc.num_words)\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer,\n",
    "         batch_size, clip, max_length = MAX_LENGTH):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    \n",
    "    # EncoderRNN의 forward부분 실행\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "    \n",
    "    # 초기 디코더 입력을 생성(각 문장을 SOS 토큰으로 시작)\n",
    "    decoder_input = torch.LongTensor([[CLS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    # 디코더의 초기 은닉 상태를 인코더의 마지막 은닉 상태로\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    \n",
    "    # teacher_forcing : Decoder부분에서 앞 단어가 잘못 추측되었을 경우 뒤에도 달라지니 정답을 입력해 주는 것\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            # LuongAttnDecoderRNN의 forward로 실행\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "             # Teacher forcing 사용: 다음 입력을 현재의 목표로 둡니다\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # Teacher forcing 미사용: 다음 입력을 디코더의 출력으로 둡니다\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "            \n",
    "    loss.backward()\n",
    "    \n",
    "    # clip_grad_norm_: 그라디언트를 제자리에서 수정합니다\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # 각 단계에 대한 배치 설정\n",
    "    # batch2TrainData : return inp, lengths, output, mask, max_target_len\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        \n",
    "        print_loss += loss\n",
    "\n",
    "\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Checkpoint를 저장\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탐욕적 디코딩(Greedy decoding) : 각 단계에 대해 단순히 decoder_output 에서 가장 높은 softmax값을 갖는 단어를 선택하는 방식\n",
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "\n",
    "        # EncoderRNN의 forward부분 실행\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "\n",
    "        # encoder의 마지막 hidden이 decoder의 처음 hidden\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        # decoder의 처음입력을 SOS로 초기화\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * CLS_token\n",
    "\n",
    "        # 디코더가 단어를 덧붙여 나갈 텐서를 초기화\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # LuongAttnDecoderRNN의 forward로 실행\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            # 가장 가능성 높은 단어 토큰과 그 softmax 점수를 구합니다\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "\n",
    "            # 토큰, 점수 기록\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "\n",
    "            # 현재의 토큰을 디코더의 다음 입력으로 준비시킵니다(차원을 증가시켜서)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    # indexes_batch : 문장을 단어집합에 저장된 수로 바꾼후 마지막에 EOS추가하는 함수\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    \n",
    "    # lengths 텐서를 만듭니다\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    # 배치의 차원을 뒤집어서 모델이 사용하는 형태로 만듭니다\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    # searcher를 이용하여 문장을 디코딩합니다\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    # 인덱스 -> 단어\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens] ###수정해야함\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # 입력 문장을 받아옵니다\n",
    "            input_sentence = input('> ')\n",
    "            # 종료 조건인지 검사합니다\n",
    "            if input_sentence == 'exit' or input_sentence == '종료': break\n",
    "            # 문장을 평가합니다\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # 응답 문장을 형식에 맞춰 출력합니다\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# loadFilename이 제공되는 경우에는 모델을 불러옵니다\n",
    "if loadFilename:\n",
    "    # 모델을 학습할 때와 같은 기기에서 불러오는 경우\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # GPU에서 학습한 모델을 CPU로 불러오는 경우\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "\n",
    "embedding = nn.Embedding(len(tokenizer.vocab), hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, len(tokenizer.vocab), decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-f319a975feb5>:4: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:20.)\n",
      "  loss = crossEntropy.masked_select(mask).mean()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 20.87 MiB free; 2.60 GiB reserved in total by PyTorch)\nException raised from malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:272 (most recent call first):\n00007FF9235775A200007FF923577540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FF926179C0600007FF926179B90 c10_cuda.dll!c10::CUDAOutOfMemoryError::CUDAOutOfMemoryError [<unknown file> @ <unknown line number>]\n00007FF92618069600007FF92617F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FF92618083A00007FF92617F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FF92617509900007FF926174EB0 c10_cuda.dll!c10::cuda::CUDAStream::unpack [<unknown file> @ <unknown line number>]\n00007FF8BF191FF100007FF8BF191EB0 torch_cuda.dll!at::native::empty_cuda [<unknown file> @ <unknown line number>]\n00007FF8BF2A8AFE00007FF8BF24E0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF8BF2A42A500007FF8BF24E0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF8E69E1A3A00007FF8E69CD9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FF8E69E000500007FF8E69CD9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FF8E6AB18A000007FF8E6AA8FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF8E6AC28DC00007FF8E6AC2850 torch_cpu.dll!at::empty [<unknown file> @ <unknown line number>]\n00007FF8E6885C6100007FF8E6885780 torch_cpu.dll!at::native::empty_like [<unknown file> @ <unknown line number>]\n00007FF8E6B871C200007FF8E6B0D060 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FF8E66421EE00007FF8E6636470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FF8E6AAF44F00007FF8E6AA8FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF8E6AC2C8200007FF8E6AC2C00 torch_cpu.dll!at::empty_like [<unknown file> @ <unknown line number>]\n00007FF8BEA007B600007FF8BE9F3E60 torch_cuda.dll!at::native::cat_out_cuda [<unknown file> @ <unknown line number>]\n00007FF8BEA1BEBE00007FF8BEA1BE20 torch_cuda.dll!at::native::softmax_backward_cuda [<unknown file> @ <unknown line number>]\n00007FF8BF29442D00007FF8BF24E0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF8BF2A1A6800007FF8BF24E0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF8E6AAEC6600007FF8E6AA8FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF8E6A99EEE00007FF8E6A99E80 torch_cpu.dll!at::_softmax_backward_data [<unknown file> @ <unknown line number>]\n00007FF8E7D93AD000007FF8E7D4E010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FF8E6640B2800007FF8E6636470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FF8E6AAEC6600007FF8E6AA8FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF8E6A99EEE00007FF8E6A99E80 torch_cpu.dll!at::_softmax_backward_data [<unknown file> @ <unknown line number>]\n00007FF8E7CC259C00007FF8E7CC2430 torch_cpu.dll!torch::autograd::generated::SoftmaxBackward::apply [<unknown file> @ <unknown line number>]\n00007FF8E7C87E9100007FF8E7C87B50 torch_cpu.dll!torch::autograd::Node::operator() [<unknown file> @ <unknown line number>]\n00007FF8E81EF9BA00007FF8E81EF300 torch_cpu.dll!torch::autograd::Engine::add_thread_pool_task [<unknown file> @ <unknown line number>]\n00007FF8E81F03AD00007FF8E81EFFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FF8E81F4FE200007FF8E81F4CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FF8E81F4C4100007FF8E81F4BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]\n00007FF9061408F700007FF906119F80 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FF8E81EBF1400007FF8E81EB780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]\n00007FF9426214C200007FF942621430 ucrtbase.dll!configthreadlocale [<unknown file> @ <unknown line number>]\n00007FF942FB703400007FF942FB7020 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]\n00007FF944CBD0D100007FF944CBD0B0 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c90ec7995b1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# 학습 단계를 수행합니다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting Training!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n\u001b[0m\u001b[0;32m     35\u001b[0m            \u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_n_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_n_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m            print_every, save_every, clip, corpus_name, loadFilename)\n",
      "\u001b[1;32m<ipython-input-14-8f42e1bed3d5>\u001b[0m in \u001b[0;36mtrainIters\u001b[1;34m(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n\u001b[0m\u001b[0;32m     24\u001b[0m                      decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-8dc59a95d86b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mn_totals\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnTotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# clip_grad_norm_: 그라디언트를 제자리에서 수정합니다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 4.00 GiB total capacity; 2.55 GiB already allocated; 20.87 MiB free; 2.60 GiB reserved in total by PyTorch)\nException raised from malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:272 (most recent call first):\n00007FF9235775A200007FF923577540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FF926179C0600007FF926179B90 c10_cuda.dll!c10::CUDAOutOfMemoryError::CUDAOutOfMemoryError [<unknown file> @ <unknown line number>]\n00007FF92618069600007FF92617F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FF92618083A00007FF92617F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FF92617509900007FF926174EB0 c10_cuda.dll!c10::cuda::CUDAStream::unpack [<unknown file> @ <unknown line number>]\n00007FF8BF191FF100007FF8BF191EB0 torch_cuda.dll!at::native::empty_cuda [<unknown file> @ <unknown line number>]\n00007FF8BF2A8AFE00007FF8BF24E0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF8BF2A42A500007FF8BF24E0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF8E69E1A3A00007FF8E69CD9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FF8E69E000500007FF8E69CD9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FF8E6AB18A000007FF8E6AA8FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF8E6AC28DC00007FF8E6AC2850 torch_cpu.dll!at::empty [<unknown file> @ <unknown line number>]\n00007FF8E6885C6100007FF8E6885780 torch_cpu.dll!at::native::empty_like [<unknown file> @ <unknown line number>]\n00007FF8E6B871C200007FF8E6B0D060 torch_cpu.dll!at::zeros_out [<unknown file> @ <unknown line number>]\n00007FF8E66421EE00007FF8E6636470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FF8E6AAF44F00007FF8E6AA8FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF8E6AC2C8200007FF8E6AC2C00 torch_cpu.dll!at::empty_like [<unknown file> @ <unknown line number>]\n00007FF8BEA007B600007FF8BE9F3E60 torch_cuda.dll!at::native::cat_out_cuda [<unknown file> @ <unknown line number>]\n00007FF8BEA1BEBE00007FF8BEA1BE20 torch_cuda.dll!at::native::softmax_backward_cuda [<unknown file> @ <unknown line number>]\n00007FF8BF29442D00007FF8BF24E0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF8BF2A1A6800007FF8BF24E0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FF8E6AAEC6600007FF8E6AA8FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF8E6A99EEE00007FF8E6A99E80 torch_cpu.dll!at::_softmax_backward_data [<unknown file> @ <unknown line number>]\n00007FF8E7D93AD000007FF8E7D4E010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FF8E6640B2800007FF8E6636470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FF8E6AAEC6600007FF8E6AA8FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FF8E6A99EEE00007FF8E6A99E80 torch_cpu.dll!at::_softmax_backward_data [<unknown file> @ <unknown line number>]\n00007FF8E7CC259C00007FF8E7CC2430 torch_cpu.dll!torch::autograd::generated::SoftmaxBackward::apply [<unknown file> @ <unknown line number>]\n00007FF8E7C87E9100007FF8E7C87B50 torch_cpu.dll!torch::autograd::Node::operator() [<unknown file> @ <unknown line number>]\n00007FF8E81EF9BA00007FF8E81EF300 torch_cpu.dll!torch::autograd::Engine::add_thread_pool_task [<unknown file> @ <unknown line number>]\n00007FF8E81F03AD00007FF8E81EFFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FF8E81F4FE200007FF8E81F4CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FF8E81F4C4100007FF8E81F4BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]\n00007FF9061408F700007FF906119F80 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FF8E81EBF1400007FF8E81EB780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]\n00007FF9426214C200007FF942621430 ucrtbase.dll!configthreadlocale [<unknown file> @ <unknown line number>]\n00007FF942FB703400007FF942FB7020 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]\n00007FF944CBD0D100007FF944CBD0B0 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]\n"
     ]
    }
   ],
   "source": [
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Dropout 레이어를 학습 모드로 둡니다\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Optimizer를 초기화합니다\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# 학습 단계를 수행합니다\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
